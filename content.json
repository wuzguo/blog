{"meta":{"title":"Zak's Home","subtitle":"分享个人学习心得","description":"一个追求进步的「十八线码农」","author":"Zàk (https://github.com/wuzguo)","url":"http://wuzguo.com/blog","root":"/blog/"},"pages":[{"title":"Categories","date":"2022-05-30T09:39:44.945Z","updated":"2022-05-30T09:39:44.945Z","comments":true,"path":"categories/index.html","permalink":"http://wuzguo.com/blog/categories/index.html","excerpt":"","text":""},{"title":"移动互联网？","date":"2017-03-18T11:56:20.000Z","updated":"2022-08-01T06:33:30.090Z","comments":false,"path":"app/index.html","permalink":"http://wuzguo.com/blog/app/index.html","excerpt":"","text":"本页面为尊敬的甲方爸爸留着，万一哪天有人找我投广告呢？","author":"Zak"},{"title":"为何建站 & 为何写作","date":"2017-02-07T05:23:07.000Z","updated":"2022-08-01T06:33:30.235Z","comments":false,"path":"about/index.html","permalink":"http://wuzguo.com/blog/about/index.html","excerpt":"","text":"为何建站纯属业余爱好，纯属闲暇时间的消遣，纯属对技术的热爱… 为何写写作读的书越多，你会发现知道的东西越多，不知道的东西就更多了，不知道远远大于知道，于是你在两者间不断碰撞、焦虑、欢喜、忧伤，产生各种思想、情感和故事，记录下来，就是文章。 显然，这个博客是带有某种程序员感情的，如果你也是开发，那就能共振了。哦说错了，是研发、共鸣。 写博客不代表不好好写代码、不好好工作，请不要纠结。本站主要作为技术笔记存档，内容主要涉及后端开发，大数据，物联网等内容。如果某一篇帮助了你，请记得点个赞。 作为个人博客或业余爱好的玩具，持续输出高质量的内容是件不太容易的事情，本站的任何文章，您可任意转载，毕竟很多内容我也是从别人那里转载的，但请保留原文链接。 关于作者一个追求进步的「十八线码农」，目前致力于Java大法，CRUD是目前的工作。 如果您遇到麻烦或对本站有疑问，请自行百度解决，本站不提供任何解决方案…","author":"Zak"},{"title":"QA - 帮助那个中心...","date":"2017-03-29T10:49:38.000Z","updated":"2022-08-01T06:33:30.323Z","comments":true,"path":"help/index.html","permalink":"http://wuzguo.com/blog/help/index.html","excerpt":"","text":"这个页面不贴配图了，说点实在的Q: 你这网站怎么弄的，还挺好看的啊？ A: 本站是静态博客，通过hexo生成，主题是我自己写的。免费使用，已开源至GitHub，本站底部有它们的链接 Q: 为什么博客有时候打开速度很慢，一直在加载呢？ A: 本站服务器在中国大陆，由于是静态网站不涉及数据库操作，理论上速度会很快。只是评论组件使用的Disqus服务器在墙外，物虽好但用户体验就得打折扣。不过不影响文章阅读。 Q: 我也想用你的主题搭个博客，不知道怎么弄？ A: 先去hexo官网看看，学会用这个写作并学会使用gitpages之类的部署博客。教程谷歌一下遍地皆是。主题是免费的，在本站底部可找到链接，根据文档自及配置就好了。遇到问题，在本页留言即可，我会很愿意帮助你的。需要注意的是主题有全站https开关，在主题_config.yml文件的fullHttps处配置。如配置不当r某些文件就无法正确引用，加载粗来的页面会很挫。 Q: 你这主题，我配置了怎么运行报错啊，跑不起来啊？ A: 关于使用主题出错，请先前往JSimple-issues寻找答案，说不定你遇到的问题，已经有人碰到而我也回答解决方式了。关于主题的任何问题，我都会在那里回复 Q: 上不了谷歌，翻不了墙怎么办？ A: 着急吃饭的朋友请走这边穿越火线。这类问题，有两种方案可选：改hosts和使用VPN代理。后者又有两种方案：掏钱和免费代理。这一点看个人需求。如果用量比较狠，还是掏钱吧。 免费的梯子像GreenVPN、Lantern、www.ishadowsocks.com都是很好的方案。关于hosts方案，请前往这个项目DeShell Q: 软件开发难么，怎么才能成为一个精英？ 软件开发看起来高大上外加一些神秘，但实际上和普通工种并无太大区别。我肤浅的认为兴趣至关重要，但实际上也发现了某些为了生计并不喜欢这行的，也做得风生水起，怎么说呢，只要有心，都能成才，难不难并不重要，看你想要什么。至于怎么成为一个软件精英，这个问题交给谷歌！ Q: 站长你哪里人、几岁了、在哪里工作、你们前台漂亮吗、你有我帅吗？ A: 首先，我没你帅；其次，我跟你熟吗 ：） 好了，暂时更新这些，需要什么底部开个腔……","author":"Zak"},{"title":"Cross Fire，Are You Ready ?","date":"2017-03-18T11:56:40.000Z","updated":"2022-08-01T06:33:29.918Z","comments":false,"path":"cross_fire/index.html","permalink":"http://wuzguo.com/blog/cross_fire/index.html","excerpt":"","text":"没有配图，请谅解。Top1Lantern学名，蓝灯，推荐指数，五星。每月500MB高速流量，用完也不收费，只是会有限速 Lantern Top2GreenVPN学名你说叫啥就是啥吧，推荐指数，五星。不限流量，注册个账号每天签到即可，pc、mobile均可使用，速度快、稳定 GreenVPN Top3Host Tool学名，猴斯特吐。推荐指数，三点五。对于谷歌、非死不可等网站有效，不是全网解决方案。效果也还不错 hosts","author":"Zak"},{"title":"重量级嘉宾推荐","date":"2017-02-07T05:22:22.000Z","updated":"2022-08-01T06:33:30.318Z","comments":false,"path":"links/index.html","permalink":"http://wuzguo.com/blog/links/index.html","excerpt":"","text":"以下摘录有趣、有意义、有影响力、有正能量的博客、资源这里并不打算作为网址大全，因此不接受以SEO为目的的友情链接。以下排序不分那啥，也不能覆盖所有方向，如果忽略了大师站，请原谅鄙人孤陋寡闻。 鸡爱死大法系阮一峰博客 娱乐系VIM CodeTank 扯淡系没有链接，如有资源请联系我","author":"Zak"},{"title":"Tags","date":"2022-05-30T09:39:45.439Z","updated":"2022-05-30T09:39:45.439Z","comments":true,"path":"tags/index.html","permalink":"http://wuzguo.com/blog/tags/index.html","excerpt":"","text":""},{"title":"时光轴","date":"2017-02-07T05:23:27.000Z","updated":"2022-08-01T06:33:30.279Z","comments":false,"path":"timeline/index.html","permalink":"http://wuzguo.com/blog/timeline/index.html","excerpt":"","text":"时光轴嘛，得先有时光，会更新的，别急……","author":"Zak"},{"title":"媒体资源？嗯，就叫这个吧...","date":"2017-03-18T11:56:26.000Z","updated":"2022-08-01T06:33:29.923Z","comments":false,"path":"video/index.html","permalink":"http://wuzguo.com/blog/video/index.html","excerpt":"","text":"片儿总会有的，我尽量快点补上你们喜欢的，不要急…","author":"Zak"}],"posts":[{"title":"Zookeeper 从入门到精通","slug":"server/zookeeper_tutorial","date":"2021-03-30T07:32:00.000Z","updated":"2022-08-01T06:35:23.763Z","comments":true,"path":"2021/03/30/server/zookeeper_tutorial.html","link":"","permalink":"http://wuzguo.com/blog/2021/03/30/server/zookeeper_tutorial.html","excerpt":"","text":"概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 从设计模式角度来理解它是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应。 特点 一个领导者（Leader），多个跟随者（Follower）组成的集群。 集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。 全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 更新请求顺序进行，来自同一个Client的更新请求按其发送顺序依次执行。 数据更新原子性，一次数据更新要么成功，要么失败。 实时性，在一定时间范围内，Client能读到最新数据。 数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。 应用场景提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。 统一命名服务 在分布式环境下，经常需要对应用&#x2F;服务进行统一命名，便于识别。例如：IP不容易记住，而域名容易记住。 统一配置管理 分布式环境下，配置文件同步非常常见。一般要求一个集群中，所有节点的配置信息是一致的，比如 Kafka 集群。对配置文件修改后，希望能够快速同步到各个节点上。 配置管理可交由ZooKeeper实现。可将配置信息写入ZooKeeper上的一个Znode。各个客户端服务器监听这个Znode。一旦Znode中的数据被修改，ZooKeeper将通知各个客户端服务器。 统一集群管理 分布式环境中，实时掌握每个节点的状态是必要的。ZooKeeper可以实现实时监控节点状态变化。 可将节点信息写入ZooKeeper上的一个ZNode。 监听这个ZNode可获取它的实时状态变化。 服务器节点动态上下线 客户端能实时洞察到服务器上下线的变化。 负载均衡 在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求。 内部原理节点类型持久（Persistent）：客户端和服务器端断开连接后，创建的节点不删除。 短暂（Ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除。 持久化目录节点：客户端与Zookeeper断开连接后，该节点依旧存在。 持久化顺序编号目录节点：客户端与Zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。 临时目录节点：客户端与Zookeeper断开连接后，该节点被删除。 临时顺序编号目录节点：客户端与Zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 选举机制集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。 以一个简单的例子来说明整个选举的过程。 假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么 server1 启动，发起一次选举。server1 投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，server1 状态保持为LOOKING； server2 启动，再发起一次选举。server1 和 server2 分别投自己一票并交换选票信息：此时 server1 发现 server2 的ID比自己目前投票推举的（server1）大，更改选票为推举 server2。此时 server1 票数0票，server2 票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING server3 启动，发起一次选举。此时 server1 和 server2 都会更改选票为 server3。此次投票结果：server1 为0票，server2 为0票，server3 为3票。此时server3 的票数已经超过半数，server3 当选Leader。server1，server2 更改状态为FOLLOWING，server3 更改状态为LEADING； server4 启动，发起一次选举。此时server1，server2，server3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：server3 为3票，server4 为1票。此时 server4 服从多数，更改选票信息为 server3，并更改状态为FOLLOWING； server5 启动，同 server4 一样当小弟。 写数据流程 Client 向 ZooKeeper 的 Server1 上写数据，发送一个写请求。 如果 Server1 不是Leader，那么 Server1 会把接受到的请求进一步转发给Leader，因为每个ZooKeeper的Server里面有一个是Leader。这个Leader 会将写请求广播给各个Server，比如 Server1 和 Server2，各个Server写成功后就会通知Leader。 当Leader收到大多数 Server 数据写成功了，那么就说明数据写成功了。如果这里三个节点的话，只要有两个节点数据写成功了，那么就认为数据写成功了。写成功之后，Leader会告诉 Server1 数据写成功了。 Server1 会进一步通知 Client 数据写成功了，这时就认为整个写操作成功。ZooKeeper 整个写数据流程就是这样的。","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://wuzguo.com/blog/tags/Zookeeper/"}],"author":"Zak"},{"title":"Java的四种引用学习（转载）","slug":"server/java_reference","date":"2021-03-12T08:26:00.000Z","updated":"2022-08-01T06:35:23.854Z","comments":true,"path":"2021/03/12/server/java_reference.html","link":"","permalink":"http://wuzguo.com/blog/2021/03/12/server/java_reference.html","excerpt":"","text":"从JDK1.2版本开始，把对象的引用分为四种级别，从而使程序更加灵活的控制对象的生命周期。这四种级别由高到低依次为：强引用、软引用、弱引用和虚引用。 1、强引用Object object =new Object(); 上述Object这类对象就具有强引用，属于不可回收的资源，垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠回收具有强引用的对象，来解决内存不足的问题。 值得注意的是：如果想中断或者回收强引用对象，可以显式地将引用赋值为 null，这样的话JVM就会在合适的时间，进行垃圾回收。 下图是堆区的内存示意图，分为新生代，老生代，而垃圾回收主要也是在这部分区域中进行。 2、软引用（SoftReference）如果一个对象只具有软引用，那么它的性质属于可有可无的那种。如果此时内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。 软引用可用来实现内存敏感的告诉缓存。软引用可以和一个引用队列联合使用，如果软件用所引用的对象被垃圾回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。 Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();SoftReference reference = new SoftReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 当内存不足时，软引用对象被回收时，reference.get()为null，此时软引用对象的作用已经发挥完毕，这时将其添加进ReferenceQueue 队列中 如果要判断哪些软引用对象已经被清理： SoftReference ref = null;while ((ref = (SoftReference) queue.poll()) != null) &#123; //清除软引用对象&#125; 3、弱引用(WeakReference)如果一个对象具有弱引用，那其的性质也是可有可无的状态。 而弱引用和软引用的区别在于：弱引用的对象拥有更短的生命周期，只要垃圾回收器扫描到它，不管内存空间充足与否，都会回收它的内存。 同样的弱引用也可以和引用队列一起使用。 Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();WeakReference reference = new WeakReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 4、虚引用（PhantomReference）虚引用和前面的软引用、弱引用不同，它并不影响对象的生命周期。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。 注意：虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之关联的引用队列中。 程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();PhantomReference reference = new PhantomReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 引用总结 对于强引用，平时在编写代码时会经常使用。 而其他三种类型的引用，使用得最多就是软引用和弱引用，这两种既有相似之处又有区别，他们都来描述非必须对象。 被软引用关联的对象只有在内存不足时才会被回收，而被弱引用关联的对象在JVM进行垃圾回收时总会被回收。 四种对象引用的差异对比Java中4种引用的级别由高到低依次为： 强引用 &gt; 软引用 &gt; 弱引用 &gt; 虚引用 垃圾回收时对比： 对象可及性的判断在很多的时候，一个对象并不是从根集直接引用的，而是一个对象被其他对象引用，甚至同时被几个对象所引用，从而构成一个以根集为顶的树形结构。 在这个树形的引用链中，箭头的方向代表了引用的方向，所指向的对象是被引用对象。由图可以看出，从根集到一个对象可以由很多条路径。 比如到达对象5的路径就有① -&gt; ⑤，③ -&gt;⑦两条路径。由此带来了一个问题，那就是某个对象的可及性如何判断： 单条引用路径可及性判断： 在这条路径中，最弱的一个引用决定对象的可及性。 多条引用路径可及性判断： 几条路径中，最强的一条的引用决定对象的可及性。 比如，我们假设图2中引用①和③为强引用，⑤为软引用，⑦为弱引用，对于对象5按照这两个判断原则，路径①-⑤取最弱的引用⑤，因此该路径对对象5的引用为软引用。同样，③-⑦为弱引用。在这两条路径之间取最强的引用，于是对象5是一个软可及对象。 比较容易理解的是Java垃圾回收器会优先清理可及强度低的对象 另外两个重要的点： 强可达的对象一定不会被清理 JVM保证抛出out of memory之前，清理所有的软引用对象 最后总结成一张表格： 引用类型 被垃圾回收时间 用途 生存时间 强引用 从来不会 对象的一般状态 JVM停止运行时终止 软引用 在内存不足时 对象缓存 内存不足时终止 弱引用 在垃圾回收时 对象缓存 垃圾回收时终止 虚引用 Unkonwn Unkonwn Unkonwn 引用队列ReferenceQueue的介绍引用队列配合Reference的子类等使用,当引用对象所指向的对象被垃圾回收后,该Reference则被追加到引用队列的末尾. ReferenceQueue源码分析(简要) ReferenceQueue是一个链表,这两个指针代表着头和尾 private Reference&lt;? extends T&gt; head = null;private Reference&lt;? extends T&gt; tail = null; 下面看下其共有的方法 取出元素: Reference&lt;? extends T&gt; ReferenceQueue#poll() 如果Reference指向的对象存在则返回null,否则返回这个Reference public Reference&lt;? extends T&gt; poll() &#123; synchronized (lock) &#123; if (head == null) return null; return reallyPollLocked(); &#125;&#125; 下面是具体将Reference取出的方法: private Reference&lt;? extends T&gt; reallyPollLocked() &#123; if (head != null) &#123; Reference&lt;? extends T&gt; r = head; if (head == tail) &#123; tail = null; head = null; &#125; else &#123; head = head.queueNext; &#125; //更新链表,将sQueueNextUnenqueued这个虚引用对象加入,并且已经表明该Reference已经被移除了,并且取出. r.queueNext = sQueueNextUnenqueued; return r; &#125; return null;&#125; 取出元素,如果队列属于空队列,那么久阻塞到其有元素为止 Reference&lt;? extends T&gt; ReferenceQueue#remove() 和remove()的区别是,设置一个阻塞时间 Reference&lt;? extends T&gt; ReferenceQueue#remove(long timeout) 具体实现 public Reference&lt;? extends T&gt; remove(long timeout) throws IllegalArgumentException, InterruptedException &#123; if (timeout &lt; 0) &#123; throw new IllegalArgumentException(&quot;Negative timeout value&quot;); &#125; synchronized (lock) &#123; Reference&lt;? extends T&gt; r = reallyPollLocked(); if (r != null) return r; long start = (timeout == 0) ? 0 : System.nanoTime(); //阻塞的具体实现过程,以及通过时间来控制的阻塞 for (;;) &#123; lock.wait(timeout); r = reallyPollLocked(); if (r != null) return r; if (timeout != 0) &#123; long end = System.nanoTime(); timeout -= (end - start) / 1000_000; if (timeout &lt;= 0) return null; start = end; &#125; &#125; &#125; &#125; WeakHashMap的相关介绍在Java集合中有一种特殊的Map类型即WeakHashMap,在这种Map中存放了键对象的弱引用,当一个键对象被垃圾回收器回收时,那么相应的值对象的引用会从Map中删除. WeakHashMap能够节约储存空间,可用来缓存那些非必须存在的数据. 而WeakHashMap是主要通过expungeStaleEntries()这个方法来实现的,而WeakHashMap也内置了一个ReferenceQueue,来获取键对象的引用情况. 这个方法,相当于遍历ReferenceQueue然后,将已经被回收的键对象,对应的值对象滞空. private void expungeStaleEntries() &#123; for (Object x; (x = queue.poll()) != null; ) &#123; synchronized (queue) &#123; @SuppressWarnings(&quot;unchecked&quot;) Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) x; int i = indexFor(e.hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; p = prev; while (p != null) &#123; Entry&lt;K,V&gt; next = p.next; if (p == e) &#123; if (prev == e) table[i] = next; else prev.next = next; // Must not null out e.next; // stale entries may be in use by a HashIterator //通过滞空,来帮助垃圾回收 e.value = null; size--; break; &#125; prev = p; p = next; &#125; &#125; &#125;&#125; 而且需要注意的是: expungeStaleEntries()并不是自动调用的,需要外部对WeakHashMap对象进行查询或者操作,才会进行自动释放的操作.如下我们看个例子: 下面例子是不断的增加1000*1000容量的WeakHashMap存入List中 public static void main(String[] args) throws Exception &#123; List&lt;WeakHashMap&lt;byte[][], byte[][]&gt;&gt; maps = new ArrayList&lt;WeakHashMap&lt;byte[][], byte[][]&gt;&gt;(); for (int i = 0; i &lt; 1000; i++) &#123; WeakHashMap&lt;byte[][], byte[][]&gt; d = new WeakHashMap&lt;byte[][], byte[][]&gt;(); d.put(new byte[1000][1000], new byte[1000][1000]); maps.add(d); System.gc(); System.err.println(i); &#125; &#125; 由于Java默认内存是64M，所以再不改变内存参数的情况下，该测试跑不了几步循环就内存溢出了。果不其然，WeakHashMap这个时候并没有自动帮我们释放不用的内存。 public static void main(String[] args) throws Exception &#123; List&lt;WeakHashMap&lt;byte[][], byte[][]&gt;&gt; maps = new ArrayList&lt;WeakHashMap&lt;byte[][], byte[][]&gt;&gt;(); for (int i = 0; i &lt; 1000; i++) &#123; WeakHashMap&lt;byte[][], byte[][]&gt; d = new WeakHashMap&lt;byte[][], byte[][]&gt;(); d.put(new byte[1000][1000], new byte[1000][1000]); maps.add(d); System.gc(); System.err.println(i); for (int j = 0; j &lt; i; j++) &#123; System.err.println(j+ &quot; size&quot; + maps.get(j).size()); &#125; &#125; &#125; 而通过访问WeakHashMap的size()方法,这些就可以跑通了. 这样就能够说明了WeakHashMap并不是自动进行键值的垃圾回收操作的,而需要做对WeakHashMap的访问操作这时候才进行对键对象的垃圾回收清理. WeakHashMap的神话 这篇帖子很棒,通过讨论WeakHashMap的回收问题,抛砖引玉. 来一张总结图: 由图可以看出,WeakHashMap中只要调用其操作方法,那么就会调用其expungeStaleEntries(). 参考文章博客原文地址：https://blog.csdn.net/l540675759/article/details/73733763","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"引用","slug":"引用","permalink":"http://wuzguo.com/blog/tags/%E5%BC%95%E7%94%A8/"},{"name":"强引用","slug":"强引用","permalink":"http://wuzguo.com/blog/tags/%E5%BC%BA%E5%BC%95%E7%94%A8/"},{"name":"软引用","slug":"软引用","permalink":"http://wuzguo.com/blog/tags/%E8%BD%AF%E5%BC%95%E7%94%A8/"},{"name":"弱引用","slug":"弱引用","permalink":"http://wuzguo.com/blog/tags/%E5%BC%B1%E5%BC%95%E7%94%A8/"},{"name":"虚引用","slug":"虚引用","permalink":"http://wuzguo.com/blog/tags/%E8%99%9A%E5%BC%95%E7%94%A8/"}],"author":"Zak"},{"title":"规则引擎 Drools","slug":"others/drools","date":"2021-03-12T07:40:00.000Z","updated":"2022-08-01T06:35:23.881Z","comments":true,"path":"2021/03/12/others/drools.html","link":"","permalink":"http://wuzguo.com/blog/2021/03/12/others/drools.html","excerpt":"","text":"1. 问题引出现有一个在线申请信用卡的业务场景，用户需要录入个人信息，如下图所示： 通过上图可以看到，用户录入的个人信息包括姓名、性别、年龄、学历、电话、所在公司、职位、月收入、是否有房、是否有车、是否有信用卡等。录入完成后点击申请按钮提交即可。 用户提交申请后，需要在系统的服务端进行用户信息合法性检查(是否有资格申请信用卡)，只有通过合法性检查的用户才可以成功申请到信用卡(注意：不同用户有可能申请到的信用卡额度不同)。 检查用户信息合法性的规则如下： 规则编号 名称 描述 1 检查学历与薪水1 如果申请人既没房也没车，同时学历为大专以下，并且月薪少于5000，那么不通过 2 检查学历与薪水2 如果申请人既没房也没车，同时学历为大专或本科，并且月薪少于3000，那么不通过 3 检查学历与薪水3 如果申请人既没房也没车，同时学历为本科以上，并且月薪少于2000，同时之前没有信用卡的，那么不通过 4 检查申请人已有的信用卡数量 如果申请人现有的信用卡数量大于10，那么不通过 用户信息合法性检查通过后，还需要根据如下信用卡发放规则确定用户所办信用卡的额度： 规则编号 名称 描述 1 规则1 如果申请人有房有车，或者月收入在20000以上，那么发放的信用卡额度为15000 2 规则2 如果申请人没房没车，但月收入在10000~20000之间，那么发放的信用卡额度为6000 3 规则3 如果申请人没房没车，月收入在10000以下，那么发放的信用卡额度为3000 4 规则4 如果申请人有房没车或者没房但有车，月收入在10000以下，那么发放的信用卡额度为5000 5 规则5 如果申请人有房没车或者是没房但有车，月收入在10000~20000之间，那么发放的信用卡额度为8000 思考：如何实现上面的业务逻辑呢？ 我们最容易想到的就是使用分支判断(if else)来实现，例如通过如下代码来检查用户信息合法性: //此处为伪代码//检查用户信息合法性，返回true表示检查通过，返回false表示检查不通过public boolean checkUser(User user)&#123; //如果申请人既没房也没车，同时学历为大专以下，并且月薪少于5000，那么不通过 if(user.getHouse() == null &amp;&amp; user.getcar() == null &amp;&amp; user.getEducation().equals(&quot;大专以下&quot;) &amp;&amp; user.getSalary &lt; 5000)&#123; return false; &#125; //如果申请人既没房也没车，同时学历为大专或本科，并且月薪少于3000，那么不通过 else if(user.getHouse() == null &amp;&amp; user.getcar() == null &amp;&amp; user.getEducation().equals(&quot;大专或本科&quot;) &amp;&amp; user.getSalary &lt; 3000)&#123; return false; &#125; //如果申请人既没房也没车，同时学历为本科以上，并且月薪少于2000，同时之前没有信用卡的，那么不通过 else if(user.getHouse() == null &amp;&amp; user.getcar() == null &amp;&amp; user.getEducation().equals(&quot;本科以上&quot;) &amp;&amp; user.getSalary &lt; 2000 &amp;&amp; user.getHasCreditCard() == false)&#123; return false; &#125; //如果申请人现有的信用卡数量大于10，那么不通过 else if(user.getCreditCardCount() &gt; 10)&#123; return false; &#125; return true;&#125; 如果用户信息合法性检查通过后，还需要通过如下代码确定用户所办信用卡的额度： //此处为伪代码//根据用户输入信息确定信用卡额度public Integer determineCreditCardLimit(User user)&#123; //如果申请人有房有车，或者月收入在20000以上，那么发放的信用卡额度为15000 if((user.getHouse() != null &amp;&amp; user.getcar() != null) || user.getSalary() &gt; 20000)&#123; return 15000; &#125; //如果申请人没房没车，并且月收入在10000到20000之间，那么发放的信用卡额度为6000 else if(user.getHouse() == null &amp;&amp; user.getcar() == null &amp;&amp; user.getSalary() &gt; 10000 &amp;&amp; user.getSalary() &lt; 20000)&#123; return 6000; &#125; //如果申请人没房没车，并且月收入在10000以下，那么发放的信用卡额度为3000 else if(user.getHouse() == null &amp;&amp; user.getcar() == null &amp;&amp; user.getSalary() &lt; 10000)&#123; return 3000; &#125; //如果申请人有房没车或者没房但有车，并且月收入在10000以下，那么发放的信用卡额度为5000 else if((((user.getHouse() != null &amp;&amp; user.getcar() == null) || (user.getHouse() == null &amp;&amp; user.getcar() != null)) &amp;&amp; user.getSalary() &lt; 10000)&#123; return 5000; &#125; //如果申请人有房没车或者没房但有车，并且月收入在10000到20000之间，那么发放的信用卡额度为8000 else if((((user.getHouse() != null &amp;&amp; user.getcar() == null) || (user.getHouse() == null &amp;&amp; user.getcar() != null)) &amp;&amp; (user.getSalary() &gt; 10000 &amp;&amp; user.getSalary() &lt; 20000))&#123; return 8000; &#125;&#125; 通过上面的伪代码我们可以看到，我们的业务规则是通过Java代码的方式实现的。这种实现方式存在如下问题： 1、硬编码实现业务规则难以维护 2、硬编码实现业务规则难以应对变化 3、业务规则发生变化需要修改代码，重启服务后才能生效 那么面对上面的业务场景，还有什么好的实现方式吗？ 答案是规则引擎。 2. 规则引擎概述2.1 什么是规则引擎规则引擎，全称为业务规则管理系统，英文名为BRMS(即Business Rule Management System)。规则引擎的主要思想是将应用程序中的业务决策部分分离出来，并使用预定义的语义模块编写业务决策（业务规则），由用户或开发者在需要时进行配置、管理。 需要注意的是规则引擎并不是一个具体的技术框架，而是指的一类系统，即业务规则管理系统。目前市面上具体的规则引擎产品有：drools、VisualRules、iLog等。 规则引擎实现了将业务决策从应用程序代码中分离出来，接收数据输入，解释业务规则，并根据业务规则做出业务决策。规则引擎其实就是一个输入输出平台。 上面的申请信用卡业务场景使用规则引擎后效果如下： 系统中引入规则引擎后，业务规则不再以程序代码的形式驻留在系统中，取而代之的是处理规则的规则引擎，业务规则存储在规则库中，完全独立于程序。业务人员可以像管理数据一样对业务规则进行管理，比如查询、添加、更新、统计、提交业务规则等。业务规则被加载到规则引擎中供应用系统调用。 2.2 使规则引擎的优势使用规则引擎的优势如下： 1、业务规则与系统代码分离，实现业务规则的集中管理 2、在不重启服务的情况下可随时对业务规则进行扩展和维护 3、可以动态修改业务规则，从而快速响应需求变更 4、规则引擎是相对独立的，只关心业务规则，使得业务分析人员也可以参与编辑、维护系统的业务规则 5、减少了硬编码业务规则的成本和风险 6、使用规则引擎提供的规则编辑工具，使复杂的业务规则实现变得的简单 2.3 规则引擎应用场景对于一些存在比较复杂的业务规则并且业务规则会频繁变动的系统比较适合使用规则引擎，如下： 1、风险控制系统—-风险贷款、风险评估 2、反欺诈项目—-银行贷款、征信验证 3、决策平台系统—-财务计算 4、促销平台系统—-满减、打折、加价购 2.4 Drools介绍drools是一款由JBoss组织提供的基于Java语言开发的开源规则引擎，可以将复杂且多变的业务规则从硬编码中解放出来，以规则脚本的形式存放在文件或特定的存储介质中(例如存放在数据库中)，使得业务规则的变更不需要修改项目代码、重启服务器就可以在线上环境立即生效。 drools官网地址：https://drools.org/ drools源码下载地址：https://github.com/kiegroup/drools 在项目中使用drools时，即可以单独使用也可以整合spring使用。如果单独使用只需要导入如下maven坐标即可： &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt;&lt;/dependency&gt; 如果我们使用IDEA开发drools应用，IDEA中已经集成了drools插件。如果使用eclipse开发drools应用还需要单独安装drools插件。 drools API开发步骤如下： 3. Drools入门案例本小节通过一个Drools入门案例来让大家初步了解Drools的使用方式、对Drools有一个整体概念。 3.1 业务场景说明业务场景：消费者在图书商城购买图书，下单后需要在支付页面显示订单优惠后的价格。具体优惠规则如下： 规则编号 规则名称 描述 1 规则一 所购图书总价在100元以下的没有优惠 2 规则二 所购图书总价在100到200元的优惠20元 3 规则三 所购图书总价在200到300元的优惠50元 4 规则四 所购图书总价在300元以上的优惠100元 现在需要根据上面的规则计算优惠后的价格。 3.2 开发实现第一步：创建maven工程drools_quickstart并导入drools相关maven坐标 &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;7.10.0.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt;&lt;/dependency&gt; 第二步：根据drools要求创建resources&#x2F;META-INF&#x2F;kmodule.xml配置文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;kmodule xmlns=&quot;http://www.drools.org/xsd/kmodule&quot;&gt; &lt;!-- name:指定kbase的名称，可以任意，但是需要唯一 packages:指定规则文件的目录，需要根据实际情况填写，否则无法加载到规则文件 default:指定当前kbase是否为默认 --&gt; &lt;kbase name=&quot;myKbase1&quot; packages=&quot;rules&quot; default=&quot;true&quot;&gt; &lt;!-- name:指定ksession名称，可以任意，但是需要唯一 default:指定当前session是否为默认 --&gt; &lt;ksession name=&quot;ksession-rule&quot; default=&quot;true&quot;/&gt; &lt;/kbase&gt;&lt;/kmodule&gt; 注意：上面配置文件的名称和位置都是固定写法，不能更改 第三步：创建实体类Order package com.itheima.drools.entity;/** * 订单 */public class Order &#123; private Double originalPrice;//订单原始价格，即优惠前价格 private Double realPrice;//订单真实价格，即优惠后价格 public String toString() &#123; return &quot;Order&#123;&quot; + &quot;originalPrice=&quot; + originalPrice + &quot;, realPrice=&quot; + realPrice + &#x27;&#125;&#x27;; &#125; public Double getOriginalPrice() &#123; return originalPrice; &#125; public void setOriginalPrice(Double originalPrice) &#123; this.originalPrice = originalPrice; &#125; public Double getRealPrice() &#123; return realPrice; &#125; public void setRealPrice(Double realPrice) &#123; this.realPrice = realPrice; &#125;&#125; 第四步：创建规则文件resources&#x2F;rules&#x2F;bookDiscount.drl //图书优惠规则package book.discountimport com.itheima.drools.entity.Order//规则一：所购图书总价在100元以下的没有优惠rule &quot;book_discount_1&quot; when $order:Order(originalPrice &lt; 100) then $order.setRealPrice($order.getOriginalPrice()); System.out.println(&quot;成功匹配到规则一：所购图书总价在100元以下的没有优惠&quot;);end//规则二：所购图书总价在100到200元的优惠20元rule &quot;book_discount_2&quot; when $order:Order(originalPrice &lt; 200 &amp;&amp; originalPrice &gt;= 100) then $order.setRealPrice($order.getOriginalPrice() - 20); System.out.println(&quot;成功匹配到规则二：所购图书总价在100到200元的优惠20元&quot;);end//规则三：所购图书总价在200到300元的优惠50元rule &quot;book_discount_3&quot; when $order:Order(originalPrice &lt;= 300 &amp;&amp; originalPrice &gt;= 200) then $order.setRealPrice($order.getOriginalPrice() - 50); System.out.println(&quot;成功匹配到规则三：所购图书总价在200到300元的优惠50元&quot;);end//规则四：所购图书总价在300元以上的优惠100元rule &quot;book_discount_4&quot; when $order:Order(originalPrice &gt;= 300) then $order.setRealPrice($order.getOriginalPrice() - 100); System.out.println(&quot;成功匹配到规则四：所购图书总价在300元以上的优惠100元&quot;);end 第五步：编写单元测试 @Testpublic void test1()&#123; KieServices kieServices = KieServices.Factory.get(); KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer(); //会话对象，用于和规则引擎交互 KieSession kieSession = kieClasspathContainer.newKieSession(); //构造订单对象，设置原始价格，由规则引擎根据优惠规则计算优惠后的价格 Order order = new Order(); order.setOriginalPrice(210D); //将数据提供给规则引擎，规则引擎会根据提供的数据进行规则匹配 kieSession.insert(order); //激活规则引擎，如果规则匹配成功则执行规则 kieSession.fireAllRules(); //关闭会话 kieSession.dispose(); System.out.println(&quot;优惠前原始价格：&quot; + order.getOriginalPrice() + &quot;，优惠后价格：&quot; + order.getRealPrice());&#125; 通过上面的入门案例我们可以发现，使用drools规则引擎主要工作就是编写规则文件，在规则文件中定义跟业务相关的业务规则，例如本案例定义的就是图书优惠规则。规则定义好后就需要调用drools提供的API将数据提供给规则引擎进行规则模式匹配，规则引擎会执行匹配成功的规则并将计算的结果返回给我们。 可能大家会有疑问，就是我们虽然没有在代码中编写规则的判断逻辑，但是我们还是在规则文件中编写了业务规则，这跟在代码中编写规则有什么本质的区别呢？ 我们前面其实已经提到，使用规则引擎时业务规则可以做到动态管理。业务人员可以像管理数据一样对业务规则进行管理，比如查询、添加、更新、统计、提交业务规则等。这样就可以做到在不重启服务的情况下调整业务规则。 3.3 小结3.3.1 规则引擎构成drools规则引擎由以下三部分构成： Working Memory（工作内存） Rule Base（规则库） Inference Engine（推理引擎） 其中Inference Engine（推理引擎）又包括： Pattern Matcher（匹配器） Agenda(议程) Execution Engine（执行引擎） 如下图所示： 3.3.2 相关概念说明Working Memory：工作内存，drools规则引擎会从Working Memory中获取数据并和规则文件中定义的规则进行模式匹配，所以我们开发的应用程序只需要将我们的数据插入到Working Memory中即可，例如本案例中我们调用kieSession.insert(order)就是将order对象插入到了工作内存中。 Fact：事实，是指在drools 规则应用当中，将一个普通的JavaBean插入到Working Memory后的对象就是Fact对象，例如本案例中的Order对象就属于Fact对象。Fact对象是我们的应用和规则引擎进行数据交互的桥梁或通道。 Rule Base：规则库，我们在规则文件中定义的规则都会被加载到规则库中。 Pattern Matcher：匹配器，将Rule Base中的所有规则与Working Memory中的Fact对象进行模式匹配，匹配成功的规则将被激活并放入Agenda中。 Agenda：议程，用于存放通过匹配器进行模式匹配后被激活的规则。 Execution Engine：执行引擎，执行Agenda中被激活的规则。 3.3.3 规则引擎执行过程 3.3.4 KIE介绍我们在操作Drools时经常使用的API以及它们之间的关系如下图： 通过上面的核心API可以发现，大部分类名都是以Kie开头。Kie全称为Knowledge Is Everything，即”知识就是一切”的缩写，是Jboss一系列项目的总称。如下图所示，Kie的主要模块有OptaPlanner、Drools、UberFire、jBPM。 通过上图可以看到，Drools是整个KIE项目中的一个组件，Drools中还包括一个Drools-WB的模块，它是一个可视化的规则编辑器。 4. Drools基础语法4.1 规则文件构成在使用Drools时非常重要的一个工作就是编写规则文件，通常规则文件的后缀为.drl。 drl是Drools Rule Language的缩写。在规则文件中编写具体的规则内容。 一套完整的规则文件内容构成如下： 关键字 描述 package 包名，只限于逻辑上的管理，同一个包名下的查询或者函数可以直接调用 import 用于导入类或者静态方法 global 全局变量 function 自定义函数 query 查询 rule end 规则体 Drools支持的规则文件，除了drl形式，还有Excel文件类型的。 4.2 规则体语法结构规则体是规则文件内容中的重要组成部分，是进行业务规则判断、处理业务结果的部分。 规则体语法结构如下： rule &quot;ruleName&quot; attributes when LHS then RHSend rule：关键字，表示规则开始，参数为规则的唯一名称。 attributes：规则属性，是rule与when之间的参数，为可选项。 when：关键字，后面跟规则的条件部分。 LHS(Left Hand Side)：是规则的条件部分的通用名称。它由零个或多个条件元素组成。如果LHS为空，则它将被视为始终为true的条件元素。 then：关键字，后面跟规则的结果部分。 RHS(Right Hand Side)：是规则的后果或行动部分的通用名称。 end：关键字，表示一个规则结束。 4.3 注释在drl形式的规则文件中使用注释和Java类中使用注释一致，分为单行注释和多行注释。 单行注释用”&#x2F;&#x2F;“进行标记，多行注释以”&#x2F;“开始，以”&#x2F;“结束。如下示例： //规则rule1的注释，这是一个单行注释rule &quot;rule1&quot; when then System.out.println(&quot;rule1触发&quot;);end/*规则rule2的注释，这是一个多行注释*/rule &quot;rule2&quot; when then System.out.println(&quot;rule2触发&quot;);end 4.4 Pattern模式匹配前面我们已经知道了Drools中的匹配器可以将Rule Base中的所有规则与Working Memory中的Fact对象进行模式匹配，那么我们就需要在规则体的LHS部分定义规则并进行模式匹配。LHS部分由一个或者多个条件组成，条件又称为pattern。 pattern的语法结构为：绑定变量名:Object(Field约束) 其中绑定变量名可以省略，通常绑定变量名的命名一般建议以$开始。如果定义了绑定变量名，就可以在规则体的RHS部分使用此绑定变量名来操作相应的Fact对象。Field约束部分是需要返回true或者false的0个或多个表达式。 例如我们的入门案例中： //规则二：所购图书总价在100到200元的优惠20元rule &quot;book_discount_2&quot; when //Order为类型约束，originalPrice为属性约束 $order:Order(originalPrice &lt; 200 &amp;&amp; originalPrice &gt;= 100) then $order.setRealPrice($order.getOriginalPrice() - 20); System.out.println(&quot;成功匹配到规则二：所购图书总价在100到200元的优惠20元&quot;);end 通过上面的例子我们可以知道，匹配的条件为： 1、工作内存中必须存在Order这种类型的Fact对象—–类型约束 2、Fact对象的originalPrice属性值必须小于200——属性约束 3、Fact对象的originalPrice属性值必须大于等于100——属性约束 以上条件必须同时满足当前规则才有可能被激活。 绑定变量既可以用在对象上，也可以用在对象的属性上。例如上面的例子可以改为： //规则二：所购图书总价在100到200元的优惠20元rule &quot;book_discount_2&quot; when $order:Order($op:originalPrice &lt; 200 &amp;&amp; originalPrice &gt;= 100) then System.out.println(&quot;$op=&quot; + $op); $order.setRealPrice($order.getOriginalPrice() - 20); System.out.println(&quot;成功匹配到规则二：所购图书总价在100到200元的优惠20元&quot;);end LHS部分还可以定义多个pattern，多个pattern之间可以使用and或者or进行连接，也可以不写，默认连接为and。 //规则二：所购图书总价在100到200元的优惠20元rule &quot;book_discount_2&quot; when $order:Order($op:originalPrice &lt; 200 &amp;&amp; originalPrice &gt;= 100) and $customer:Customer(age &gt; 20 &amp;&amp; gender==&#x27;male&#x27;) then System.out.println(&quot;$op=&quot; + $op); $order.setRealPrice($order.getOriginalPrice() - 20); System.out.println(&quot;成功匹配到规则二：所购图书总价在100到200元的优惠20元&quot;);end 4.5 比较操作符Drools提供的比较操作符，如下表： 符号 说明 &gt; 大于 &lt; 小于 &gt;&#x3D; 大于等于 &lt;&#x3D; 小于等于 &#x3D;&#x3D; 等于 !&#x3D; 不等于 contains 检查一个Fact对象的某个属性值是否包含一个指定的对象值 not contains 检查一个Fact对象的某个属性值是否不包含一个指定的对象值 memberOf 判断一个Fact对象的某个属性是否在一个或多个集合中 not memberOf 判断一个Fact对象的某个属性是否不在一个或多个集合中 matches 判断一个Fact对象的属性是否与提供的标准的Java正则表达式进行匹配 not matches 判断一个Fact对象的属性是否不与提供的标准的Java正则表达式进行匹配 前6个比较操作符和Java中的完全相同，下面我们重点学习后6个比较操作符。 4.5.1 语法 contains | not contains语法结构 Object(Field[Collection&#x2F;Array] contains value) Object(Field[Collection&#x2F;Array] not contains value) memberOf | not memberOf语法结构 Object(field memberOf value[Collection&#x2F;Array]) Object(field not memberOf value[Collection&#x2F;Array]) matches | not matches语法结构 Object(field matches “正则表达式”) Object(field not matches “正则表达式”) 4.5.2 操作步骤第一步：创建实体类，用于测试比较操作符 package com.itheima.drools.entity;import java.util.List;/** * 实体类 * 用于测试比较操作符 */public class ComparisonOperatorEntity &#123; private String names; private List&lt;String&gt; list; public String getNames() &#123; return names; &#125; public void setNames(String names) &#123; this.names = names; &#125; public List&lt;String&gt; getList() &#123; return list; &#125; public void setList(List&lt;String&gt; list) &#123; this.list = list; &#125;&#125; 第二步：在&#x2F;resources&#x2F;rules下创建规则文件comparisonOperator.drl package comparisonOperatorimport com.itheima.drools.entity.ComparisonOperatorEntity/* 当前规则文件用于测试Drools提供的比较操作符*///测试比较操作符containsrule &quot;rule_comparison_contains&quot; when ComparisonOperatorEntity(names contains &quot;张三&quot;) ComparisonOperatorEntity(list contains names) then System.out.println(&quot;规则rule_comparison_contains触发&quot;);end//测试比较操作符not containsrule &quot;rule_comparison_notContains&quot; when ComparisonOperatorEntity(names not contains &quot;张三&quot;) ComparisonOperatorEntity(list not contains names) then System.out.println(&quot;规则rule_comparison_notContains触发&quot;);end//测试比较操作符memberOfrule &quot;rule_comparison_memberOf&quot; when ComparisonOperatorEntity(names memberOf list) then System.out.println(&quot;规则rule_comparison_memberOf触发&quot;);end//测试比较操作符not memberOfrule &quot;rule_comparison_notMemberOf&quot; when ComparisonOperatorEntity(names not memberOf list) then System.out.println(&quot;规则rule_comparison_notMemberOf触发&quot;);end//测试比较操作符matchesrule &quot;rule_comparison_matches&quot; when ComparisonOperatorEntity(names matches &quot;张.*&quot;) then System.out.println(&quot;规则rule_comparison_matches触发&quot;);end//测试比较操作符not matchesrule &quot;rule_comparison_notMatches&quot; when ComparisonOperatorEntity(names not matches &quot;张.*&quot;) then System.out.println(&quot;规则rule_comparison_notMatches触发&quot;);end 第三步：编写单元测试 //测试比较操作符@Testpublic void test3()&#123; KieServices kieServices = KieServices.Factory.get(); KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer(); KieSession kieSession = kieClasspathContainer.newKieSession(); ComparisonOperatorEntity comparisonOperatorEntity = new ComparisonOperatorEntity(); comparisonOperatorEntity.setNames(&quot;张三&quot;); List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;张三&quot;); list.add(&quot;李四&quot;); comparisonOperatorEntity.setList(list); //将数据提供给规则引擎，规则引擎会根据提供的数据进行规则匹配，如果规则匹配成功则执行规则 kieSession.insert(comparisonOperatorEntity); kieSession.fireAllRules(); kieSession.dispose();&#125; 4.6 执行指定规则通过前面的案例可以看到，我们在调用规则代码时，满足条件的规则都会被执行。那么如果我们只想执行其中的某个规则如何实现呢？ Drools给我们提供的方式是通过规则过滤器来实现执行指定规则。对于规则文件不用做任何修改，只需要修改Java代码即可，如下： KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();ComparisonOperatorEntity comparisonOperatorEntity = new ComparisonOperatorEntity();comparisonOperatorEntity.setNames(&quot;张三&quot;);List&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(&quot;张三&quot;);list.add(&quot;李四&quot;);comparisonOperatorEntity.setList(list);kieSession.insert(comparisonOperatorEntity);//通过规则过滤器实现只执行指定规则kieSession.fireAllRules(new RuleNameEqualsAgendaFilter(&quot;rule_comparison_memberOf&quot;));kieSession.dispose(); 4.7 关键字Drools的关键字分为：硬关键字(Hard keywords)和软关键字(Soft keywords)。 硬关键字是我们在规则文件中定义包名或者规则名时明确不能使用的，否则程序会报错。软关键字虽然可以使用，但是不建议使用。 硬关键字包括：true false null 软关键字包括： lock-on-active date-effective date-expires no-loop auto-focus activation-group agenda-group ruleflow-group entry-point duration package import dialect salience enabled attributes rule extend when then template query declare function global eval not in or and exists forall accumulate collect from actio reverse result end over init 4.8 Drools内置方法规则文件的RHS部分的主要作用是通过插入，删除或修改工作内存中的Fact数据，来达到控制规则引擎执行的目的。Drools提供了一些方法可以用来操作工作内存中的数据，操作完成后规则引擎会重新进行相关规则的匹配，原来没有匹配成功的规则在我们修改数据完成后有可能就会匹配成功了。 创建如下实体类： package com.itheima.drools.entity;import java.util.List;/** * 学生 */public class Student &#123; private int id; private String name; private int age; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 4.8.1 update方法update方法的作用是更新工作内存中的数据，并让相关的规则重新匹配。 第一步：编写规则文件&#x2F;resources&#x2F;rules&#x2F;student.drl，文件内容如下 package studentimport com.itheima.drools.entity.Student/* 当前规则文件用于测试Drools提供的内置方法*/rule &quot;rule_student_age小于10岁&quot; when $s:Student(age &lt; 10) then $s.setAge(15); update($s);//更新数据，导致相关的规则会重新匹配 System.out.println(&quot;规则rule_student_age小于10岁触发&quot;);endrule &quot;rule_student_age小于20岁同时大于10岁&quot; when $s:Student(age &lt; 20 &amp;&amp; age &gt; 10) then $s.setAge(25); update($s);//更新数据，导致相关的规则会重新匹配 System.out.println(&quot;规则rule_student_age小于20岁同时大于10岁触发&quot;);endrule &quot;rule_student_age大于20岁&quot; when $s:Student(age &gt; 20) then System.out.println(&quot;规则rule_student_age大于20岁触发&quot;);end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();Student student = new Student();student.setAge(5);//将数据提供给规则引擎，规则引擎会根据提供的数据进行规则匹配，如果规则匹配成功则执行规则kieSession.insert(student);kieSession.fireAllRules();kieSession.dispose(); 通过控制台的输出可以看到规则文件中定义的三个规则都触发了。 在更新数据时需要注意防止发生死循环。 4.8.2 insert方法insert方法的作用是向工作内存中插入数据，并让相关的规则重新匹配。 第一步：修改student.drl文件内容如下 package studentimport com.itheima.drools.entity.Student/* 当前规则文件用于测试Drools提供的内置方法*/rule &quot;rule_student_age等于10岁&quot; when $s:Student(age == 10) then Student student = new Student(); student.setAge(5); insert(student);//插入数据，导致相关的规则会重新匹配 System.out.println(&quot;规则rule_student_age等于10岁触发&quot;);endrule &quot;rule_student_age小于10岁&quot; when $s:Student(age &lt; 10) then $s.setAge(15); update($s); System.out.println(&quot;规则rule_student_age小于10岁触发&quot;);endrule &quot;rule_student_age小于20岁同时大于10岁&quot; when $s:Student(age &lt; 20 &amp;&amp; age &gt; 10) then $s.setAge(25); update($s); System.out.println(&quot;规则rule_student_age小于20岁同时大于10岁触发&quot;);endrule &quot;rule_student_age大于20岁&quot; when $s:Student(age &gt; 20) then System.out.println(&quot;规则rule_student_age大于20岁触发&quot;);end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();Student student = new Student();student.setAge(10);//将数据提供给规则引擎，规则引擎会根据提供的数据进行规则匹配，如果规则匹配成功则执行规则kieSession.insert(student);kieSession.fireAllRules();kieSession.dispose(); 通过控制台输出可以发现，四个规则都触发了，这是因为首先进行规则匹配时只有第一个规则可以匹配成功，但是在第一个规则中向工作内存中插入了一个数据导致重新进行规则匹配，此时第二个规则可以匹配成功。在第二个规则中进行了数据修改导致第三个规则也可以匹配成功，以此类推最终四个规则都匹配成功并执行了。 4.8.3 retract方法retract方法的作用是删除工作内存中的数据，并让相关的规则重新匹配。 第一步：修改student.drl文件内容如下 package studentimport com.itheima.drools.entity.Student/* 当前规则文件用于测试Drools提供的内置方法*/rule &quot;rule_student_age等于10岁时删除数据&quot; /* salience：设置当前规则的执行优先级，数值越大越优先执行，默认值为0. 因为当前规则的匹配条件和下面规则的匹配条件相同，为了保证先执行当前规则，需要设置优先级 */ salience 100 when $s:Student(age == 10) then retract($s);//retract方法的作用是删除工作内存中的数据，并让相关的规则重新匹配。 System.out.println(&quot;规则rule_student_age等于10岁时删除数据触发&quot;);endrule &quot;rule_student_age等于10岁&quot; when $s:Student(age == 10) then Student student = new Student(); student.setAge(5); insert(student); System.out.println(&quot;规则rule_student_age等于10岁触发&quot;);endrule &quot;rule_student_age小于10岁&quot; when $s:Student(age &lt; 10) then $s.setAge(15); update($s); System.out.println(&quot;规则rule_student_age小于10岁触发&quot;);endrule &quot;rule_student_age小于20岁同时大于10岁&quot; when $s:Student(age &lt; 20 &amp;&amp; age &gt; 10) then $s.setAge(25); update($s); System.out.println(&quot;规则rule_student_age小于20岁同时大于10岁触发&quot;);endrule &quot;rule_student_age大于20岁&quot; when $s:Student(age &gt; 20) then System.out.println(&quot;规则rule_student_age大于20岁触发&quot;);end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();Student student = new Student();student.setAge(10);//将数据提供给规则引擎，规则引擎会根据提供的数据进行规则匹配，如果规则匹配成功则执行规则kieSession.insert(student);kieSession.fireAllRules();kieSession.dispose(); 通过控制台输出可以发现，只有第一个规则触发了，因为在第一个规则中将工作内存中的数据删除了导致第二个规则并没有匹配成功。 5. 规则属性前面我们已经知道了规则体的构成如下： rule &quot;ruleName&quot; attributes when LHS then RHSend 本章节就是针对规则体的attributes属性部分进行讲解。Drools中提供的属性如下表(部分属性)： 属性名 说明 salience 指定规则执行优先级 dialect 指定规则使用的语言类型，取值为java和mvel enabled 指定规则是否启用 date-effective 指定规则生效时间 date-expires 指定规则失效时间 activation-group 激活分组，具有相同分组名称的规则只能有一个规则触发 agenda-group 议程分组，只有获取焦点的组中的规则才有可能触发 timer 定时器，指定规则触发的时间 auto-focus 自动获取焦点，一般结合agenda-group一起使用 no-loop 防止死循环 5.1 enabled属性enabled属性对应的取值为true和false，默认值为true。 用于指定当前规则是否启用，如果设置的值为false则当前规则无论是否匹配成功都不会触发。 rule &quot;rule_comparison_notMemberOf&quot; //指定当前规则不可用，当前规则无论是否匹配成功都不会执行 enabled false when ComparisonOperatorEntity(names not memberOf list) then System.out.println(&quot;规则rule_comparison_notMemberOf触发&quot;);end 5.2 dialect属性dialect属性用于指定当前规则使用的语言类型，取值为java和mvel，默认值为java。 注：mvel是一种基于java语法的表达式语言。 mvel像正则表达式一样，有直接支持集合、数组和字符串匹配的操作符。 mvel还提供了用来配置和构造字符串的模板语言。 mvel表达式内容包括属性表达式，布尔表达式，方法调用，变量赋值，函数定义等。 5.3 salience属性salience属性用于指定规则的执行优先级，取值类型为Integer。数值越大越优先执行。每个规则都有一个默认的执行顺序，如果不设置salience属性，规则体的执行顺序为由上到下。 可以通过创建规则文件salience.drl来测试salience属性，内容如下： package test.saliencerule &quot;rule_1&quot; when eval(true) then System.out.println(&quot;规则rule_1触发&quot;);end rule &quot;rule_2&quot; when eval(true) then System.out.println(&quot;规则rule_2触发&quot;);endrule &quot;rule_3&quot; when eval(true) then System.out.println(&quot;规则rule_3触发&quot;);end 通过控制台可以看到，由于以上三个规则没有设置salience属性，所以执行的顺序是按照规则文件中规则的顺序由上到下执行的。接下来我们修改一下文件内容： package testsaliencerule &quot;rule_1&quot; salience 9 when eval(true) then System.out.println(&quot;规则rule_1触发&quot;);endrule &quot;rule_2&quot; salience 10 when eval(true) then System.out.println(&quot;规则rule_2触发&quot;);endrule &quot;rule_3&quot; salience 8 when eval(true) then System.out.println(&quot;规则rule_3触发&quot;);end 通过控制台可以看到，规则文件执行的顺序是按照我们设置的salience值由大到小顺序执行的。 建议在编写规则时使用salience属性明确指定执行优先级。 5.4 no-loop属性no-loop属性用于防止死循环，当规则通过update之类的函数修改了Fact对象时，可能使当前规则再次被激活从而导致死循环。取值类型为Boolean，默认值为false。测试步骤如下： 第一步：编写规则文件&#x2F;resource&#x2F;rules&#x2F;noloop.drl package testnoloopimport com.itheima.drools.entity.Student/* 此规则文件用于测试no-loop属性*/rule &quot;rule_noloop&quot; when // no-loop true $student:Student(age == 25) then update($student);//注意此处执行update会导致当前规则重新被激活 System.out.println(&quot;规则rule_noloop触发&quot;);end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();Student student = new Student();student.setAge(25);//将数据提供给规则引擎，规则引擎会根据提供的数据进行规则匹配，如果规则匹配成功则执行规则kieSession.insert(student);kieSession.fireAllRules();kieSession.dispose(); 通过控制台可以看到，由于我们没有设置no-loop属性的值，所以发生了死循环。接下来设置no-loop的值为true再次测试则不会发生死循环。 5.5 activation-group属性activation-group属性是指激活分组，取值为String类型。具有相同分组名称的规则只能有一个规则被触发。 第一步：编写规则文件&#x2F;resources&#x2F;rules&#x2F;activationgroup.drl package testactivationgroup/* 此规则文件用于测试activation-group属性*/ rule &quot;rule_activationgroup_1&quot; activation-group &quot;mygroup&quot; when then System.out.println(&quot;规则rule_activationgroup_1触发&quot;);endrule &quot;rule_activationgroup_2&quot; activation-group &quot;mygroup&quot; when then System.out.println(&quot;规则rule_activationgroup_2触发&quot;);end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();kieSession.fireAllRules();kieSession.dispose(); 通过控制台可以发现，上面的两个规则因为属于同一个分组，所以只有一个触发了。同一个分组中的多个规则如果都能够匹配成功，具体哪一个最终能够被触发可以通过salience属性确定。 5.6 agenda-group属性agenda-group属性为议程分组，属于另一种可控的规则执行方式。用户可以通过设置agenda-group来控制规则的执行，只有获取焦点的组中的规则才会被触发。 第一步：创建规则文件&#x2F;resources&#x2F;rules&#x2F;agendagroup.drl package testagendagroup/* 此规则文件用于测试agenda-group属性*/rule &quot;rule_agendagroup_1&quot; agenda-group &quot;myagendagroup_1&quot; when then System.out.println(&quot;规则rule_agendagroup_1触发&quot;);endrule &quot;rule_agendagroup_2&quot; agenda-group &quot;myagendagroup_1&quot; when then System.out.println(&quot;规则rule_agendagroup_2触发&quot;);endrule &quot;rule_agendagroup_3&quot; agenda-group &quot;myagendagroup_2&quot; when then System.out.println(&quot;规则rule_agendagroup_3触发&quot;);endrule &quot;rule_agendagroup_4&quot; agenda-group &quot;myagendagroup_2&quot; when then System.out.println(&quot;规则rule_agendagroup_4触发&quot;);end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();//设置焦点，对应agenda-group分组中的规则才可能被触发kieSession.getAgenda().getAgendaGroup(&quot;myagendagroup_1&quot;).setFocus();kieSession.fireAllRules();kieSession.dispose(); 通过控制台可以看到，只有获取焦点的分组中的规则才会触发。与activation-group不同的是，activation-group定义的分组中只能够有一个规则可以被触发，而agenda-group分组中的多个规则都可以被触发。 5.7 auto-focus属性auto-focus属性为自动获取焦点，取值类型为Boolean，默认值为false。一般结合agenda-group属性使用，当一个议程分组未获取焦点时，可以设置auto-focus属性来控制。 第一步：修改&#x2F;resources&#x2F;rules&#x2F;agendagroup.drl文件内容如下 package testagendagrouprule &quot;rule_agendagroup_1&quot; agenda-group &quot;myagendagroup_1&quot; when then System.out.println(&quot;规则rule_agendagroup_1触发&quot;);endrule &quot;rule_agendagroup_2&quot; agenda-group &quot;myagendagroup_1&quot; when then System.out.println(&quot;规则rule_agendagroup_2触发&quot;);endrule &quot;rule_agendagroup_3&quot; agenda-group &quot;myagendagroup_2&quot; auto-focus true //自动获取焦点 when then System.out.println(&quot;规则rule_agendagroup_3触发&quot;);endrule &quot;rule_agendagroup_4&quot; agenda-group &quot;myagendagroup_2&quot; auto-focus true //自动获取焦点 when then System.out.println(&quot;规则rule_agendagroup_4触发&quot;);end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();kieSession.fireAllRules();kieSession.dispose(); 通过控制台可以看到，设置auto-focus属性为true的规则都触发了。 5.8 timer属性timer属性可以通过定时器的方式指定规则执行的时间，使用方式有两种： 方式一：timer (int: ?) 此种方式遵循java.util.Timer对象的使用方式，第一个参数表示几秒后执行，第二个参数表示每隔几秒执行一次，第二个参数为可选。 方式二：timer(cron: ) 此种方式使用标准的unix cron表达式的使用方式来定义规则执行的时间。 第一步：创建规则文件&#x2F;resources&#x2F;rules&#x2F;timer.drl package testtimerimport java.text.SimpleDateFormatimport java.util.Date/* 此规则文件用于测试timer属性*/rule &quot;rule_timer_1&quot; timer (5s 2s) //含义：5秒后触发，然后每隔2秒触发一次 when then System.out.println(&quot;规则rule_timer_1触发，触发时间为：&quot; + new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(new Date()));endrule &quot;rule_timer_2&quot; timer (cron:0/1 * * * * ?) //含义：每隔1秒触发一次 when then System.out.println(&quot;规则rule_timer_2触发，触发时间为：&quot; + new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(new Date()));end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();final KieSession kieSession = kieClasspathContainer.newKieSession();new Thread(new Runnable() &#123; public void run() &#123; //启动规则引擎进行规则匹配，直到调用halt方法才结束规则引擎 kieSession.fireUntilHalt(); &#125;&#125;).start();Thread.sleep(10000);//结束规则引擎kieSession.halt();kieSession.dispose(); 注意：单元测试的代码和以前的有所不同，因为我们规则文件中使用到了timer进行定时执行，需要程序能够持续一段时间才能够看到定时器触发的效果。 5.9 date-effective属性date-effective属性用于指定规则的生效时间，即只有当前系统时间大于等于设置的时间或者日期规则才有可能触发。默认日期格式为：dd-MMM-yyyy。用户也可以自定义日期格式。 第一步：编写规则文件&#x2F;resources&#x2F;rules&#x2F;dateeffective.drl package testdateeffective/* 此规则文件用于测试date-effective属性*/rule &quot;rule_dateeffective_1&quot; date-effective &quot;2020-10-01 10:00&quot; when then System.out.println(&quot;规则rule_dateeffective_1触发&quot;);end 第二步：编写单元测试 //设置日期格式System.setProperty(&quot;drools.dateformat&quot;,&quot;yyyy-MM-dd HH:mm&quot;);KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();kieSession.fireAllRules();kieSession.dispose(); 注意：上面的代码需要设置日期格式，否则我们在规则文件中写的日期格式和默认的日期格式不匹配程序会报错。 5.10 date-expires属性date-expires属性用于指定规则的失效时间，即只有当前系统时间小于设置的时间或者日期规则才有可能触发。默认日期格式为：dd-MMM-yyyy。用户也可以自定义日期格式。 第一步：编写规则文件&#x2F;resource&#x2F;rules&#x2F;dateexpires.drl package testdateexpires/* 此规则文件用于测试date-expires属性*/rule &quot;rule_dateexpires_1&quot; date-expires &quot;2019-10-01 10:00&quot; when then System.out.println(&quot;规则rule_dateexpires_1触发&quot;);end 第二步：编写单元测试 //设置日期格式System.setProperty(&quot;drools.dateformat&quot;,&quot;yyyy-MM-dd HH:mm&quot;);KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();kieSession.fireAllRules();kieSession.dispose(); 注意：上面的代码需要设置日期格式，否则我们在规则文件中写的日期格式和默认的日期格式不匹配程序会报错。 6. Drools高级语法前面章节我们已经知道了一套完整的规则文件内容构成如下： 关键字 描述 package 包名，只限于逻辑上的管理，同一个包名下的查询或者函数可以直接调用 import 用于导入类或者静态方法 global 全局变量 function 自定义函数 query 查询 rule end 规则体 本章节我们就来学习其中的几个关键字。 6.1 global全局变量global关键字用于在规则文件中定义全局变量，它可以让应用程序的对象在规则文件中能够被访问。可以用来为规则文件提供数据或服务。 语法结构为：global 对象类型 对象名称 在使用global定义的全局变量时有两点需要注意： 1、如果对象类型为包装类型时，在一个规则中改变了global的值，那么只针对当前规则有效，对其他规则中的global不会有影响。可以理解为它是当前规则代码中的global副本，规则内部修改不会影响全局的使用。 2、如果对象类型为集合类型或JavaBean时，在一个规则中改变了global的值，对java代码和所有规则都有效。 下面我们通过代码进行验证： 第一步：创建UserService类 package com.itheima.drools.service;public class UserService &#123; public void save()&#123; System.out.println(&quot;UserService.save()...&quot;); &#125;&#125; 第二步：编写规则文件&#x2F;resources&#x2F;rules&#x2F;global.drl package testglobal/* 此规则文件用于测试global全局变量*/global java.lang.Integer count //定义一个包装类型的全局变量global com.itheima.drools.service.UserService userService //定义一个JavaBean类型的全局变量global java.util.List gList //定义一个集合类型的全局变量rule &quot;rule_global_1&quot; when then count += 10; //全局变量计算，只对当前规则有效，其他规则不受影响 userService.save();//调用全局变量的方法 gList.add(&quot;itcast&quot;);//向集合类型的全局变量中添加元素，Java代码和所有规则都受影响 gList.add(&quot;itheima&quot;); System.out.println(&quot;count=&quot; + count); System.out.println(&quot;gList.size=&quot; + gList.size());endrule &quot;rule_global_2&quot; when then userService.save(); System.out.println(&quot;count=&quot; + count); System.out.println(&quot;gList.size=&quot; + gList.size());end 第三步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();//设置全局变量，名称和类型必须和规则文件中定义的全局变量名称对应kieSession.setGlobal(&quot;userService&quot;,new UserService());kieSession.setGlobal(&quot;count&quot;,5);List list = new ArrayList();//size为0kieSession.setGlobal(&quot;gList&quot;,list);kieSession.fireAllRules();kieSession.dispose();//因为在规则中为全局变量添加了两个元素，所以现在的size为2System.out.println(list.size()); 6.2 query查询query查询提供了一种查询working memory中符合约束条件的Fact对象的简单方法。它仅包含规则文件中的LHS部分，不用指定“when”和“then”部分并且以end结束。具体语法结构如下： query 查询的名称(可选参数) LHSend 具体操作步骤： 第一步：编写规则文件&#x2F;resources&#x2F;rules&#x2F;query.drl package testqueryimport com.itheima.drools.entity.Student/* 此规则文件用于测试query查询*///不带参数的查询//当前query用于查询Working Memory中age&gt;10的Student对象query &quot;query_1&quot; $student:Student(age &gt; 10)end//带有参数的查询//当前query用于查询Working Memory中age&gt;10同时name需要和传递的参数name相同的Student对象query &quot;query_2&quot;(String sname) $student:Student(age &gt; 20 &amp;&amp; name == sname)end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();Student student1 = new Student();student1.setName(&quot;张三&quot;);student1.setAge(12);Student student2 = new Student();student2.setName(&quot;李四&quot;);student2.setAge(8);Student student3 = new Student();student3.setName(&quot;王五&quot;);student3.setAge(22);//将对象插入Working Memory中kieSession.insert(student1);kieSession.insert(student2);kieSession.insert(student3);//调用规则文件中的查询QueryResults results1 = kieSession.getQueryResults(&quot;query_1&quot;);int size = results1.size();System.out.println(&quot;size=&quot; + size);for (QueryResultsRow row : results1) &#123; Student student = (Student) row.get(&quot;$student&quot;); System.out.println(student);&#125;//调用规则文件中的查询QueryResults results2 = kieSession.getQueryResults(&quot;query_2&quot;,&quot;王五&quot;);size = results2.size();System.out.println(&quot;size=&quot; + size);for (QueryResultsRow row : results2) &#123; Student student = (Student) row.get(&quot;$student&quot;); System.out.println(student);&#125;//kieSession.fireAllRules();kieSession.dispose(); 6.3 function函数function关键字用于在规则文件中定义函数，就相当于java类中的方法一样。可以在规则体中调用定义的函数。使用函数的好处是可以将业务逻辑集中放置在一个地方，根据需要可以对函数进行修改。 函数定义的语法结构如下： function 返回值类型 函数名(可选参数)&#123; //逻辑代码&#125; 具体操作步骤： 第一步：编写规则文件&#x2F;resources&#x2F;rules&#x2F;function.drl package testfunctionimport com.itheima.drools.entity.Student/* 此规则文件用于测试function函数*///定义一个函数function String sayHello(String name)&#123; return &quot;hello &quot; + name;&#125;rule &quot;rule_function_1&quot; when $student:Student(name != null) then //调用上面定义的函数 String ret = sayHello($student.getName()); System.out.println(ret);end 第二步：编写单元测试 KieServices kieServices = KieServices.Factory.get();KieContainer kieClasspathContainer = kieServices.getKieClasspathContainer();KieSession kieSession = kieClasspathContainer.newKieSession();Student student = new Student();student.setName(&quot;小明&quot;);kieSession.insert(student);kieSession.fireAllRules();kieSession.dispose(); 6.4 LHS加强前面我们已经知道了在规则体中的LHS部分是介于when和then之间的部分，主要用于模式匹配，只有匹配结果为true时，才会触发RHS部分的执行。本章节我们会针对LHS部分学习几个新的用法。 6.4.1 复合值限制in&#x2F;not in复合值限制是指超过一种匹配值的限制条件，类似于SQL语句中的in关键字。Drools规则体中的LHS部分可以使用in或者not in进行复合值的匹配。具体语法结构如下： Object(field in (比较值1,比较值2…)) 举例： $s:Student(name in (&quot;张三&quot;,&quot;李四&quot;,&quot;王五&quot;))$s:Student(name not in (&quot;张三&quot;,&quot;李四&quot;,&quot;王五&quot;)) 6.4.2 条件元素evaleval用于规则体的LHS部分，并返回一个Boolean类型的值。语法结构如下： eval(表达式) 举例： eval(true)eval(false)eval(1 == 1) 6.4.3 条件元素notnot用于判断Working Memory中是否存在某个Fact对象，如果不存在则返回true，如果存在则返回false。语法结构如下： not Object(可选属性约束) 举例： not Student()not Student(age &lt; 10) 6.4.4 条件元素existsexists的作用与not相反，用于判断Working Memory中是否存在某个Fact对象，如果存在则返回true，不存在则返回false。语法结构如下： exists Object(可选属性约束) 举例： exists Student()exists Student(age &lt; 10 &amp;&amp; name != null) 可能有人会有疑问，我们前面在LHS部分进行条件编写时并没有使用exists也可以达到判断Working Memory中是否存在某个符合条件的Fact元素的目的，那么我们使用exists还有什么意义？ 两者的区别：当向Working Memory中加入多个满足条件的Fact对象时，使用了exists的规则执行一次，不使用exists的规则会执行多次。 例如： 规则文件(只有规则体)： rule &quot;使用exists的规则&quot; when exists Student() then System.out.println(&quot;规则：使用exists的规则触发&quot;);endrule &quot;没有使用exists的规则&quot; when Student() then System.out.println(&quot;规则：没有使用exists的规则触发&quot;);end Java代码： kieSession.insert(new Student());kieSession.insert(new Student());kieSession.fireAllRules(); 上面第一个规则只会执行一次，因为Working Memory中存在两个满足条件的Fact对象，第二个规则会执行两次。 6.4.5 规则继承规则之间可以使用extends关键字进行规则条件部分的继承，类似于java类之间的继承。 例如： rule &quot;rule_1&quot; when Student(age &gt; 10) then System.out.println(&quot;规则：rule_1触发&quot;);endrule &quot;rule_2&quot; extends &quot;rule_1&quot; //继承上面的规则 when /* 此处的条件虽然只写了一个，但是从上面的规则继承了一个条件， 所以当前规则存在两个条件，即Student(age &lt; 20)和Student(age &gt; 10) */ Student(age &lt; 20) then System.out.println(&quot;规则：rule_2触发&quot;);end 6.5 RHS加强RHS部分是规则体的重要组成部分，当LHS部分的条件匹配成功后，对应的RHS部分就会触发执行。一般在RHS部分中需要进行业务处理。 在RHS部分Drools为我们提供了一个内置对象，名称就是drools。本小节我们来介绍几个drools对象提供的方法。 6.5.1 halthalt方法的作用是立即终止后面所有规则的执行。 package testhaltrule &quot;rule_halt_1&quot; when then System.out.println(&quot;规则：rule_halt_1触发&quot;); drools.halt();//立即终止后面所有规则执行end//当前规则并不会触发，因为上面的规则调用了halt方法导致后面所有规则都不会执行rule &quot;rule_halt_2&quot; when then System.out.println(&quot;规则：rule_halt_2触发&quot;);end 6.5.2 getWorkingMemorygetWorkingMemory方法的作用是返回工作内存对象。 package testgetWorkingMemoryrule &quot;rule_getWorkingMemory&quot; when then System.out.println(drools.getWorkingMemory());end 6.5.3 getRulegetRule方法的作用是返回规则对象。 package testgetRulerule &quot;rule_getRule&quot; when then System.out.println(drools.getRule());end 6.6 规则文件编码规范我们在进行drl类型的规则文件编写时尽量遵循如下规范： 所有的规则文件(.drl)应统一放在一个规定的文件夹中，如：&#x2F;rules文件夹 书写的每个规则应尽量加上注释。注释要清晰明了，言简意赅 同一类型的对象尽量放在一个规则文件中，如所有Student类型的对象尽量放在一个规则文件中 规则结果部分(RHS)尽量不要有条件语句，如if(…)，尽量不要有复杂的逻辑和深层次的嵌套语句 每个规则最好都加上salience属性，明确执行顺序 Drools默认dialect为”Java”，尽量避免使用dialect “mvel” 7. Spring整合Drools7.1 Spring简单整合Drools在项目中使用Drools时往往会跟Spring整合来使用。具体整合步骤如下： 第一步：创建maven工程drools_spring并配置pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;drools_spring&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;drools.version&gt;7.10.0.Final&lt;/drools.version&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;$&#123;drools.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-spring&lt;/artifactId&gt; &lt;version&gt;$&#123;drools.version&#125;&lt;/version&gt; &lt;!--注意：此处必须排除传递过来的依赖，否则会跟我们自己导入的Spring jar包产生冲突--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 第二步：创建规则目录&#x2F;resources&#x2F;rules，中rules目录中创建规则文件helloworld.drl package helloworldrule &quot;rule_helloworld&quot; when eval(true) then System.out.println(&quot;规则：rule_helloworld触发...&quot;);end 第三步：创建Spring配置文件&#x2F;resources&#x2F;spring.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:kie=&quot;http://drools.org/schema/kie-spring&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://drools.org/schema/kie-spring http://drools.org/schema/kie-spring.xsd&quot;&gt; &lt;kie:kmodule id=&quot;kmodule&quot;&gt; &lt;kie:kbase name=&quot;kbase&quot; packages=&quot;rules&quot;&gt; &lt;kie:ksession name=&quot;ksession&quot;&gt;&lt;/kie:ksession&gt; &lt;/kie:kbase&gt; &lt;/kie:kmodule&gt; &lt;bean class=&quot;org.kie.spring.annotations.KModuleAnnotationPostProcessor&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; 第四步：编写单元测试类 package com.itheima.test;import org.junit.Test;import org.junit.runner.RunWith;import org.kie.api.KieBase;import org.kie.api.cdi.KBase;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:spring.xml&quot;)public class DroolsSpringTest &#123; @KBase(&quot;kbase&quot;) private KieBase kieBase;//注入KieBase对象 @Test public void test1()&#123; KieSession kieSession = kieBase.newKieSession(); kieSession.fireAllRules(); kieSession.dispose(); &#125;&#125; 7.2 Spring整合Drools+web本小节我们来进行Drools和Spring Web的整合。具体操作步骤如下： 第一步：创建maven的war工程drools_springweb并在pom.xml文件中导入相关maven坐标 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;drools_springweb&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;drools.version&gt;7.10.0.Final&lt;/drools.version&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;$&#123;drools.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-spring&lt;/artifactId&gt; &lt;version&gt;$&#123;drools.version&#125;&lt;/version&gt; &lt;!--注意：此处必须排除传递过来的依赖，否则会跟我们自己导入的Spring jar包产生冲突--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!-- 指定端口 --&gt; &lt;port&gt;80&lt;/port&gt; &lt;!-- 请求路径 --&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 第二步：配置web.xml &lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 指定加载的配置文件 ，通过参数contextConfigLocation加载 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 第三步：创建&#x2F;resources&#x2F;springmvc.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:kie=&quot;http://drools.org/schema/kie-spring&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://drools.org/schema/kie-spring http://drools.org/schema/kie-spring.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;kie:kmodule id=&quot;kmodule&quot;&gt; &lt;kie:kbase name=&quot;kbase&quot; packages=&quot;rules&quot;&gt; &lt;kie:ksession name=&quot;ksession&quot;&gt;&lt;/kie:ksession&gt; &lt;/kie:kbase&gt; &lt;/kie:kmodule&gt; &lt;bean class=&quot;org.kie.spring.annotations.KModuleAnnotationPostProcessor&quot;/&gt; &lt;!--spring批量扫描--&gt; &lt;context:component-scan base-package=&quot;com.itheima&quot; /&gt; &lt;context:annotation-config/&gt; &lt;!--springMVC注解驱动--&gt; &lt;mvc:annotation-driven/&gt;&lt;/beans&gt; 第四步：创建规则文件&#x2F;resources&#x2F;rules&#x2F;helloworld.drl package helloworldrule &quot;rule_helloworld&quot; when eval(true) then System.out.println(&quot;规则：rule_helloworld触发...&quot;);end 第五步：创建RuleService package com.itheima.service;import org.kie.api.KieBase;import org.kie.api.cdi.KBase;import org.kie.api.runtime.KieSession;import org.springframework.stereotype.Service;@Servicepublic class RuleService &#123; @KBase(&quot;kbase&quot;) private KieBase kieBase; public void rule()&#123; KieSession kieSession = kieBase.newKieSession(); kieSession.fireAllRules(); kieSession.dispose(); &#125;&#125; 第六步：创建HelloController package com.itheima.controller;import com.itheima.service.RuleService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/hello&quot;)public class HelloController &#123; @Autowired private RuleService ruleService; @RequestMapping(&quot;/rule&quot;) public String rule()&#123; ruleService.rule(); return &quot;OK&quot;; &#125;&#125; 7.3 Spring Boot整合Drools目前在企业开发中Spring Boot已经成为主流，本小节我们来进行Spring Boot整合Drools。具体操作步骤： 第一步：创建maven工程drools_springboot并配置pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starters&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;drools_springboot&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;!--drools规则引擎--&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-core&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-templates&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-api&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-spring&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.*&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 第二步：创建&#x2F;resources&#x2F;application.yml文件 server: port: 8080spring: application: name: drools_springboot 第三步：创建规则文件&#x2F;resources&#x2F;rules&#x2F;helloworld.drl package helloworldrule &quot;rule_helloworld&quot; when eval(true) then System.out.println(&quot;规则：rule_helloworld触发...&quot;);end 第四步：编写配置类DroolsConfig package com.itheima.drools.config;import org.kie.api.KieBase;import org.kie.api.KieServices;import org.kie.api.builder.KieBuilder;import org.kie.api.builder.KieFileSystem;import org.kie.api.builder.KieRepository;import org.kie.api.runtime.KieContainer;import org.kie.api.runtime.KieSession;import org.kie.internal.io.ResourceFactory;import org.kie.spring.KModuleBeanFactoryPostProcessor;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.io.support.PathMatchingResourcePatternResolver;import org.springframework.core.io.support.ResourcePatternResolver;import org.springframework.core.io.Resource;import java.io.IOException;/** * 规则引擎配置类 */@Configurationpublic class DroolsConfig &#123; //指定规则文件存放的目录 private static final String RULES_PATH = &quot;rules/&quot;; private final KieServices kieServices = KieServices.Factory.get(); @Bean @ConditionalOnMissingBean public KieFileSystem kieFileSystem() throws IOException &#123; KieFileSystem kieFileSystem = kieServices.newKieFileSystem(); ResourcePatternResolver resourcePatternResolver = new PathMatchingResourcePatternResolver(); Resource[] files = resourcePatternResolver.getResources(&quot;classpath*:&quot; + RULES_PATH + &quot;*.*&quot;); String path = null; for (Resource file : files) &#123; path = RULES_PATH + file.getFilename(); kieFileSystem.write(ResourceFactory.newClassPathResource(path, &quot;UTF-8&quot;)); &#125; return kieFileSystem; &#125; @Bean @ConditionalOnMissingBean public KieContainer kieContainer() throws IOException &#123; KieRepository kieRepository = kieServices.getRepository(); kieRepository.addKieModule(kieRepository::getDefaultReleaseId); KieBuilder kieBuilder = kieServices.newKieBuilder(kieFileSystem()); kieBuilder.buildAll(); return kieServices.newKieContainer(kieRepository.getDefaultReleaseId()); &#125; @Bean @ConditionalOnMissingBean public KieBase kieBase() throws IOException &#123; return kieContainer().getKieBase(); &#125; @Bean @ConditionalOnMissingBean public KModuleBeanFactoryPostProcessor kiePostProcessor() &#123; return new KModuleBeanFactoryPostProcessor(); &#125;&#125; 第五步：创建RuleService类 package com.itheima.drools.service;import org.kie.api.KieBase;import org.kie.api.runtime.KieSession;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class RuleService &#123; @Autowired private KieBase kieBase; public void rule()&#123; KieSession kieSession = kieBase.newKieSession(); kieSession.fireAllRules(); kieSession.dispose(); &#125;&#125; 第六步：创建HelloController类 package com.itheima.drools.controller;import com.itheima.drools.service.RuleService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/hello&quot;)public class HelloController &#123; @Autowired private RuleService ruleService; @RequestMapping(&quot;/rule&quot;) public String rule()&#123; ruleService.rule(); return &quot;OK&quot;; &#125;&#125; 第七步：创建启动类DroolsApplication package com.itheima.drools;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class DroolsApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DroolsApplication.class,args); &#125;&#125; 第八步：启动服务，访问http://localhost:8080/hello/rule 8. WorkBench8.1 WorkBench简介WorkBench是KIE组件中的元素，也称为KIE-WB，是Drools-WB与JBPM-WB的结合体。它是一个可视化的规则编辑器。WorkBench其实就是一个war包，安装到tomcat中就可以运行。使用WorkBench可以在浏览器中创建数据对象、创建规则文件、创建测试场景并将规则部署到maven仓库供其他应用使用。 下载地址：https://download.jboss.org/drools/release/7.6.0.Final/kie-drools-wb-7.6.0.Final-tomcat8.war 注意：下载的war包需要安装到tomcat8中。 8.2 安装方式软件安装时经常会涉及到软件版本兼容性的问题，所以需要明确各个软件的使用版本。 本课程使用的软件环境如下： 操作系统：Windows 10 64位 JDK版本：1.8 maven版本：3.5.4 Tomcat版本：8.5 具体安装步骤： 第一步：配置Tomcat的环境变量CATALINA_HOME，对应的值为Tomcat安装目录 第二步：在Tomcat的bin目录下创建setenv.bat文件，内容如下： CATALINA_OPTS=&quot;-Xmx512M \\ -Djava.security.auth.login.config=$CATALINA_HOME/webapps/kie-drools-wb/WEB-INF/classes/login.config \\ -Dorg.jboss.logging.provider=jdk&quot; 第三步：将下载的WorkBench的war包改名为kie-drools-wb.war并复制到Tomcat的webapps目录下 第四步：修改Tomcat下conf&#x2F;tomcat-users.xml文件 &lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;utf-8&#x27;?&gt;&lt;tomcat-users xmlns=&quot;http://tomcat.apache.org/xml&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://tomcat.apache.org/xml tomcat-users.xsd&quot; version=&quot;1.0&quot;&gt; &lt;!--定义admin角色--&gt; &lt;role rolename=&quot;admin&quot;/&gt; &lt;!--定义一个用户，用户名为kie，密码为kie，对应的角色为admin角色--&gt; &lt;user username=&quot;kie&quot; password=&quot;kie&quot; roles=&quot;admin&quot;/&gt;&lt;/tomcat-users&gt; 第五步：下载以下三个jar包并复制到Tomcat的lib目录下 kie-tomcat-integration-7.10.0.Final.jarjavax.security.jacc-api-1.5.jarslf4j-api-1.7.25.jar 第六步：修改Tomcat的conf&#x2F;server.xml文件，添加Valve标签，内容为： &lt;Valve className=&quot;org.kie.integration.tomcat.JACCValve&quot;/&gt; 第七步：启动Tomcat并访问http://localhost:8080/kie-drools-wb，可以看到WorkBench的登录页面。使用前面在tomcat-users.xml文件中定义的用户进行登录即可 登录成功后进入系统首页： 8.3 使用方式8.3.1 创建空间、项目WorkBench中存在空间和项目的概念。我们在使用WorkBench时首先需要创建空间（Space），在空间中创建项目，在项目中创建数据对象、规则文件等。 创建空间 第一步：登录WorkBench后进行系统首页，点击首页中的Design区域进入项目列表页面： 如果是第一次登录还没有创建项目则无法看到项目 第二步：点击左上角Spaces导航链接进入空间列表页面 第三步：点击右上角Add Space按钮弹出创建添加空间窗口 录入空间名称，点击Save按钮则完成空间的创建，如下图： 创建项目 前面已经提到，我们在WorkBench中需要先创建空间，在空间中才能创建项目。上面我们已经创建了一个空间itheima，现在需要住此空间中创建项目。 第一步：点击itheima空间，进入此空间 可以看到当前空间中还没有项目 第二步：点击Add Project按钮弹出添加项目窗口 第三步：在添加项目窗口中录入项目名称（例如项目名称为pro1），点击Add按钮完成操作 可以看到在完成项目创建后，系统直接跳转到了项目页面。要查看当前itheima空间中的所有项目，可以点击左上角itheima链接： 8.3.2 创建数据对象数据对象其实就是JavaBean，一般都是在drl规则文件中使用进行规则匹配。 第一步：在itheima空间中点击pro1项目，进入此项目页面 第二步：点击Create New Asset按钮选择“数据对象” 第三步：在弹出的创建数据对象窗口中输入数据对象的名称，点击确定按钮完成操作 操作完成后可以看到如下： 第四步：点击“添加字段”按钮弹出新建字段窗口 第五步：在新建字段窗口中录入字段Id（其实就是属性名），选择类型，点击创建按钮完成操作 完成操作后可以看到刚才创建的字段： 可以点击添加字段按钮继续创建其他字段： 注意添加完字段后需要点击右上角保存按钮完成保存操作： 点击源代码按钮可以查看刚才创建的Person对象源码： 点击左上角pro1项目链接，可以看到当前pro1项目中已经创建的各种类型的对象： 8.3.3 创建DRL规则文件第一步：在pro1项目页面点击右上角Create New Asset按钮，选择“DRL文件”，弹出创建DRL文件窗口 第二步：在添加DRL文件窗口录入DRL文件名称，点击确定按钮完成操作 第三步：上面点击确定按钮完成创建DRL文件后，页面会跳转到编辑DRL文件页面 可以看到DRL规则文件页面分为两个部分：左侧为项目浏览视图、右侧为编辑区域，需要注意的是左侧默认展示的不是项目浏览视图，需要点击上面设置按钮，选择“资料库视图”和“显示为文件夹”，如下图所示： 第四步：在编辑DRL文件页面右侧区域进行DRL文件的编写，点击右上角保存按钮完成保存操作，点击检验按钮进行规则文件语法检查 点击左上角pro1项目回到项目页面，可以看到此项目下已经存在两个对象，即person.drl规则文件和Person类： 8.3.4 创建测试场景前面我们已经创建了Person数据对象和person规则文件，现在我们需要测试一下规则文件中的规则，可以通过创建测试场景来进行测试。 第一步：在项目页面点击Create New Asset按钮选择“测试场景”，弹出创建测试场景窗口 第二步：在弹出的创建测试场景窗口中录入测试场景的名称，点击确定完成操作 完成测试场景的创建后，页面会跳转到测试场景编辑页面，如下图： 第三步：因为我们编写的规则文件中需要从工作内存中获取Person对象进行规则匹配，所以在测试场景中需要准备Person对象给工作内存，点击“GIVEN”按钮弹出新建数据录入窗口，选择Person类，输入框中输入事实名称（名称任意），如下图 第四步：录入事实名称后点击后面的添加按钮，可以看到Person对象已经添加成功 第五步：我们给工作内存提供的Person对象还需要设置age属性的值，点击“添加字段”按钮弹出窗口，选择age属性 点击确定按钮后可以看到字段已经添加成功： 第六步：点击age属性后面的编辑按钮，弹出字段值窗口 第七步：在弹出的窗口中点击字面值按钮，重新回到测试场景页面，可以看到age后面出现输入框，可以为age属性设置值 设置好age属性的值后点击保存按钮保存测试场景 第八步：点击右上角“运行测试场景”按钮进行测试 测试成功后可以查看WorkBench部署的Tomcat控制台： 8.3.5 设置 KieBase和KieSession第一步：在pro1项目页面点击右上角Settings按钮进入设置页面 第二步：在设置页面选择“知识库和会话”选项 第三步：在弹出的知识库和会话页面点击“添加”按钮进行设置 第四步：设置完成后点击右上角保存按钮完成设置操作，可以通过左侧浏览视图点击kmodule.xml，查看文件内容 8.3.6 编译、构建、部署前面我们已经在WorkBench中创建了一个空间itheima，并且在此空间中创建了一个项目pro1，在此项目中创建了数据文件、规则文件和测试场景，如下图： 点击右上角“Compile”按钮可以对项目进行编译，点击“Bulid&amp;Deploy”按钮进行构建和部署。 部署成功后可以在本地maven仓库中看到当前项目已经被打成jar包： 将上面的jar包进行解压，可以看到我们创建的数据对象Person和规则文件person以及kmodule.xml都已经打到jar包中了。 8.3.7 在项目中使用部署的规则前面我们已经在WorkBench中创建了pro1项目，并且在pro1项目中创建了数据文件、规则文件等。最后我们将此项目打成jar包部署到了maven仓库中。本小节就需要在外部项目中使用我们定义的规则。 第一步：在IDEA中创建一个maven项目并在pom.xml文件中导入相关坐标 &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;7.10.0.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt;&lt;/dependency&gt; 第二步：在项目中创建一个数据对象Person，需要和WorkBench中创建的Person包名、类名完全相同，属性也需要对应 package com.itheima.pro1;public class Person implements java.io.Serializable &#123; static final long serialVersionUID = 1L; private java.lang.String id; private java.lang.String name; private int age; public Person() &#123; &#125; public java.lang.String getId() &#123; return this.id; &#125; public void setId(java.lang.String id) &#123; this.id = id; &#125; public java.lang.String getName() &#123; return this.name; &#125; public void setName(java.lang.String name) &#123; this.name = name; &#125; public int getAge() &#123; return this.age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public Person(java.lang.String id, java.lang.String name, int age) &#123; this.id = id; this.name = name; this.age = age; &#125;&#125; 第三步：编写单元测试，远程加载maven仓库中的jar包最终完成规则调用 @Testpublic void test1() throws Exception&#123; //通过此URL可以访问到maven仓库中的jar包 //URL地址构成：http://ip地址:Tomcat端口号/WorkBench工程名/maven2/坐标/版本号/xxx.jar String url = &quot;http://localhost:8080/kie-drools-wb/maven2/com/itheima/pro1/1.0.0/pro1-1.0.0.jar&quot;; KieServices kieServices = KieServices.Factory.get(); //通过Resource资源对象加载jar包 UrlResource resource = (UrlResource) kieServices.getResources().newUrlResource(url); //通过Workbench提供的服务来访问maven仓库中的jar包资源，需要先进行Workbench的认证 resource.setUsername(&quot;kie&quot;); resource.setPassword(&quot;kie&quot;); resource.setBasicAuthentication(&quot;enabled&quot;); //将资源转换为输入流，通过此输入流可以读取jar包数据 InputStream inputStream = resource.getInputStream(); //创建仓库对象，仓库对象中保存Drools的规则信息 KieRepository repository = kieServices.getRepository(); //通过输入流读取maven仓库中的jar包数据，包装成KieModule模块添加到仓库中 KieModule kieModule = repository. addKieModule(kieServices.getResources().newInputStreamResource(inputStream)); //基于KieModule模块创建容器对象，从容器中可以获取session会话 KieContainer kieContainer = kieServices.newKieContainer(kieModule.getReleaseId()); KieSession session = kieContainer.newKieSession(); Person person = new Person(); person.setAge(10); session.insert(person); session.fireAllRules(); session.dispose();&#125; 执行单元测试可以发现控制台已经输出了相关内容。通过WorkBench修改规则输出内容并发布，再次执行单元测试可以发现控制台输出的内容也发生了变化。 通过上面的案例可以发现，我们在IEDA中开发的项目中并没有编写规则文件，规则文件是我们通过WorkBench开发并安装部署到maven仓库中，我们自己开发的项目只需要远程加载maven仓库中的jar包就可以完成规则的调用。这种开发方式的好处是我们的应用可以和业务规则完全分离，同时通过WorkBench修改规则后我们的应用不需要任何修改就可以加载到最新的规则从而实现规则的动态变更。 9. Drools实战9.1 个人所得税计算器本小节我们需要通过Drools规则引擎来根据规则计算个人所得税，最终页面效果如下： 9.1.1 名词解释税前月收入：即税前工资，指交纳个人所得税之前的总工资 应纳税所得额：指按照税法规定确定纳税人在一定期间所获得的所有应税收入减除在该纳税期间依法允许减除的各种支出后的余额 税率：是对征税对象的征收比例或征收额度 速算扣除数：指为解决超额累进税率分级计算税额的复杂技术问题，而预先计算出的一个数据，可以简化计算过程 扣税额：是指实际缴纳的税额 税后工资：是指扣完税后实际到手的工资收入 9.1.2 计算规则要实现个人所得税计算器，需要了解如下计算规则： 规则编号 名称 描述 1 计算应纳税所得额 应纳税所得额为税前工资减去3500 2 设置税率，应纳税所得额&lt;&#x3D;1500 税率为0.03，速算扣除数为0 3 设置税率，应纳税所得额在1500至4500之间 税率为0.1，速算扣除数为105 4 设置税率，应纳税所得额在4500志9000之间 税率为0.2，速算扣除数为555 5 设置税率，应纳税所得额在9000志35000之间 税率为0.25，速算扣除数为1005 6 设置税率，应纳税所得额在35000至55000之间 税率为0.3，速算扣除数为2755 7 设置税率，应纳税所得额在55000至80000之间 税率为0.35，速算扣除数为5505 8 设置税率，应纳税所得额在80000以上 税率为0.45，速算扣除数为13505 9 计算税后工资 扣税额&#x3D;应纳税所得额*税率-速算扣除数 税后工资&#x3D;税前工资-扣税额 9.1.3 实现步骤本实战案例我们基于Spring Boot整合Drools的方式来实现。 第一步：创建maven工程calculation并配置pom.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starters&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;cn.itcast&lt;/groupId&gt; &lt;artifactId&gt;calculation&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;!--drools规则引擎--&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-core&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-templates&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-api&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-spring&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.*&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 第二步：创建&#x2F;resources&#x2F;application.yml文件 server: port: 8080spring: application: name: calculation 第三步：编写配置类DroolsConfig package com.itheima.drools.config;import org.kie.api.KieBase;import org.kie.api.KieServices;import org.kie.api.builder.KieBuilder;import org.kie.api.builder.KieFileSystem;import org.kie.api.builder.KieRepository;import org.kie.api.runtime.KieContainer;import org.kie.api.runtime.KieSession;import org.kie.internal.io.ResourceFactory;import org.kie.spring.KModuleBeanFactoryPostProcessor;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.io.support.PathMatchingResourcePatternResolver;import org.springframework.core.io.support.ResourcePatternResolver;import org.springframework.core.io.Resource;import java.io.IOException;/** * 规则引擎配置类 */@Configurationpublic class DroolsConfig &#123; //指定规则文件存放的目录 private static final String RULES_PATH = &quot;rules/&quot;; private final KieServices kieServices = KieServices.Factory.get(); @Bean @ConditionalOnMissingBean public KieFileSystem kieFileSystem() throws IOException &#123; System.setProperty(&quot;drools.dateformat&quot;,&quot;yyyy-MM-dd&quot;); KieFileSystem kieFileSystem = kieServices.newKieFileSystem(); ResourcePatternResolver resourcePatternResolver = new PathMatchingResourcePatternResolver(); Resource[] files = resourcePatternResolver.getResources(&quot;classpath*:&quot; + RULES_PATH + &quot;*.*&quot;); String path = null; for (Resource file : files) &#123; path = RULES_PATH + file.getFilename(); kieFileSystem.write(ResourceFactory.newClassPathResource(path, &quot;UTF-8&quot;)); &#125; return kieFileSystem; &#125; @Bean @ConditionalOnMissingBean public KieContainer kieContainer() throws IOException &#123; KieRepository kieRepository = kieServices.getRepository(); kieRepository.addKieModule(kieRepository::getDefaultReleaseId); KieBuilder kieBuilder = kieServices.newKieBuilder(kieFileSystem()); kieBuilder.buildAll(); return kieServices.newKieContainer(kieRepository.getDefaultReleaseId()); &#125; @Bean @ConditionalOnMissingBean public KieBase kieBase() throws IOException &#123; return kieContainer().getKieBase(); &#125; @Bean @ConditionalOnMissingBean public KModuleBeanFactoryPostProcessor kiePostProcessor() &#123; return new KModuleBeanFactoryPostProcessor(); &#125;&#125; 第四步：编写实体类Calculation package com.itheima.drools.entity;public class Calculation &#123; private double wage;//税前工资 private double wagemore;//应纳税所得额 private double cess;//税率 private double preminus;//速算扣除数 private double wageminus;//扣税额 private double actualwage;//税后工资 public double getWage() &#123; return wage; &#125; public void setWage(double wage) &#123; this.wage = wage; &#125; public double getActualwage() &#123; return actualwage; &#125; public void setActualwage(double actualwage) &#123; this.actualwage = actualwage; &#125; public double getWagemore() &#123; return wagemore; &#125; public void setWagemore(double wagemore) &#123; this.wagemore = wagemore; &#125; public double getCess() &#123; return cess; &#125; public void setCess(double cess) &#123; this.cess = cess; &#125; public double getPreminus() &#123; return preminus; &#125; public void setPreminus(double preminus) &#123; this.preminus = preminus; &#125; public double getWageminus() &#123; return wageminus; &#125; public void setWageminus(double wageminus) &#123; this.wageminus = wageminus; &#125; @Override public String toString() &#123; return &quot;Calculation&#123;&quot; + &quot;wage=&quot; + wage + &quot;, actualwage=&quot; + actualwage + &quot;, wagemore=&quot; + wagemore + &quot;, cess=&quot; + cess + &quot;, preminus=&quot; + preminus + &quot;, wageminus=&quot; + wageminus + &#x27;&#125;&#x27;; &#125;&#125; 第五步：在resources&#x2F;rules下创建规则文件calculation.drl文件 package calculationimport com.itheima.drools.entity.Calculationrule &quot;个人所得税：计算应纳税所得额&quot; enabled true salience 3 no-loop true date-effective &quot;2011-09-01&quot; //生效日期 when $cal : Calculation(wage&gt;0) then $cal.setWagemore($cal.getWage()-3500); update($cal);endrule &quot;个人所得税：设置税率--&gt;&gt;应纳税所得额&lt;=1500&quot; salience 2 no-loop true activation-group &quot;SETCess_Group&quot; when $cal : Calculation(wagemore &lt;= 1500) then $cal.setCess(0.03); $cal.setPreminus(0); update($cal);endrule &quot;个人所得税：设置税率--&gt;&gt;应纳税所得额在1500至4500之间&quot; salience 2 no-loop true activation-group &quot;SETCess_Group&quot; when $cal : Calculation(wagemore &gt; 1500 &amp;&amp; wagemore &lt;= 4500) then $cal.setCess(0.1); $cal.setPreminus(105); update($cal);endrule &quot;个人所得税：设置税率--&gt;&gt;应纳税所得额在4500志9000之间&quot; salience 2 no-loop true activation-group &quot;SETCess_Group&quot; when $cal : Calculation(wagemore &gt; 4500 &amp;&amp; wagemore &lt;= 9000) then $cal.setCess(0.2); $cal.setPreminus(555); update($cal);endrule &quot;个人所得税：设置税率--&gt;&gt;应纳税所得额在9000志35000之间&quot; salience 2 no-loop true activation-group &quot;SETCess_Group&quot; when $cal : Calculation(wagemore &gt; 9000 &amp;&amp; wagemore &lt;= 35000) then $cal.setCess(0.25); $cal.setPreminus(1005); update($cal);endrule &quot;个人所得税：设置税率--&gt;&gt;应纳税所得额在35000至55000之间&quot; salience 2 no-loop true activation-group &quot;SETCess_Group&quot; when $cal : Calculation(wagemore &gt; 35000 &amp;&amp; wagemore &lt;= 55000) then $cal.setCess(0.3); $cal.setPreminus(2755); update($cal);endrule &quot;个人所得税：设置税率--&gt;&gt;应纳税所得额在55000至80000之间&quot; salience 2 no-loop true activation-group &quot;SETCess_Group&quot; when $cal : Calculation(wagemore &gt; 55000 &amp;&amp; wagemore &lt;= 80000) then $cal.setCess(0.35); $cal.setPreminus(5505); update($cal);endrule &quot;个人所得税：设置税率--&gt;&gt;应纳税所得额在80000以上&quot; salience 2 no-loop true activation-group &quot;SETCess_Group&quot; when $cal : Calculation(wagemore &gt; 80000) then $cal.setCess(0.45); $cal.setPreminus(13505); update($cal);endrule &quot;个人所得税：计算税后工资&quot; salience 1 when $cal : Calculation(wage &gt; 0 &amp;&amp; wagemore &gt; 0 &amp;&amp; wagemore &gt; 0 &amp;&amp; cess &gt; 0) then $cal.setWageminus($cal.getWagemore()*$cal.getCess()-$cal.getPreminus()); $cal.setActualwage($cal.getWage()-$cal.getWageminus()); System.out.println(&quot;-----税前工资：&quot;+$cal.getWage()); System.out.println(&quot;-----应纳税所得额：&quot;+$cal.getWagemore()); System.out.println(&quot;-----税率：&quot; + $cal.getCess()); System.out.println(&quot;-----速算扣除数：&quot; + $cal.getPreminus()); System.out.println(&quot;-----扣税额：&quot; + $cal.getWageminus()); System.out.println(&quot;-----税后工资：&quot; + $cal.getActualwage());end 第六步：创建RuleService package com.itheima.drools.service;import com.itheima.drools.entity.Calculation;import org.kie.api.KieBase;import org.kie.api.runtime.KieSession;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;/** * 调用规则引擎，执行规则 */@Servicepublic class RuleService &#123; @Autowired private KieBase kieBase; //个人所得税计算 public Calculation calculate(Calculation calculation)&#123; KieSession kieSession = kieBase.newKieSession(); kieSession.insert(calculation); kieSession.fireAllRules(); kieSession.dispose(); return calculation; &#125;&#125; 第七步：创建RuleController package com.itheima.drools.controller;import com.itheima.drools.entity.Calculation;import com.itheima.drools.service.RuleService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/rule&quot;)public class RuleController &#123; @Autowired private RuleService ruleService; @RequestMapping(&quot;/calculate&quot;) public Calculation calculate(double wage)&#123; Calculation calculation = new Calculation(); calculation.setWage(wage); calculation = ruleService.calculate(calculation); System.out.println(calculation); return calculation; &#125;&#125; 第八步：创建启动类DroolsApplication package com.itheima.drools;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class DroolsApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DroolsApplication.class); &#125;&#125; 第九步：导入静态资源文件到resources&#x2F;static目录下 9.2 信用卡申请本小节我们需要通过Drools规则引擎来根据规则进行申请人的合法性检查，检查通过后再根据规则确定信用卡额度，最终页面效果如下： 9.2.1 计算规则合法性检查规则如下： 规则编号 名称 描述 1 检查学历与薪水1 如果申请人既没房也没车，同时学历为大专以下，并且月薪少于5000，那么不通过 2 检查学历与薪水2 如果申请人既没房也没车，同时学历为大专或本科，并且月薪少于3000，那么不通过 3 检查学历与薪水3 如果申请人既没房也没车，同时学历为本科以上，并且月薪少于2000，同时之前没有信用卡的，那么不通过 4 检查申请人已有的信用卡数量 如果申请人现有的信用卡数量大于10，那么不通过 信用卡额度确定规则： 规则编号 名称 描述 1 规则1 如果申请人有房有车，或者月收入在20000以上，那么发放的信用卡额度为15000 2 规则2 如果申请人没房没车，但月收入在10000~20000之间，那么发放的信用卡额度为6000 3 规则3 如果申请人没房没车，月收入在10000以下，那么发放的信用卡额度为3000 4 规则4 如果申请人有房没车或者没房但有车，月收入在10000以下，那么发放的信用卡额度为5000 5 规则5 如果申请人有房没车或者是没房但有车，月收入在10000~20000之间，那么发放的信用卡额度为8000 9.2.2 实现步骤第一步：创建maven工程creditCardApply并配置pom.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starters&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;creditCardApply&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;!--drools规则引擎--&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-core&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-templates&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-api&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-spring&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.*&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 第二步：创建&#x2F;resources&#x2F;application.yml文件 server: port: 8080spring: application: name: creditCardApply 第三步：编写配置类DroolsConfig package com.itheima.drools.config;import org.kie.api.KieBase;import org.kie.api.KieServices;import org.kie.api.builder.KieBuilder;import org.kie.api.builder.KieFileSystem;import org.kie.api.builder.KieRepository;import org.kie.api.runtime.KieContainer;import org.kie.api.runtime.KieSession;import org.kie.internal.io.ResourceFactory;import org.kie.spring.KModuleBeanFactoryPostProcessor;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.io.support.PathMatchingResourcePatternResolver;import org.springframework.core.io.support.ResourcePatternResolver;import org.springframework.core.io.Resource;import java.io.IOException;/** * 规则引擎配置类 */@Configurationpublic class DroolsConfig &#123; //指定规则文件存放的目录 private static final String RULES_PATH = &quot;rules/&quot;; private final KieServices kieServices = KieServices.Factory.get(); @Bean @ConditionalOnMissingBean public KieFileSystem kieFileSystem() throws IOException &#123; KieFileSystem kieFileSystem = kieServices.newKieFileSystem(); ResourcePatternResolver resourcePatternResolver = new PathMatchingResourcePatternResolver(); Resource[] files = resourcePatternResolver.getResources(&quot;classpath*:&quot; + RULES_PATH + &quot;*.*&quot;); String path = null; for (Resource file : files) &#123; path = RULES_PATH + file.getFilename(); kieFileSystem.write(ResourceFactory.newClassPathResource(path, &quot;UTF-8&quot;)); &#125; return kieFileSystem; &#125; @Bean @ConditionalOnMissingBean public KieContainer kieContainer() throws IOException &#123; KieRepository kieRepository = kieServices.getRepository(); kieRepository.addKieModule(kieRepository::getDefaultReleaseId); KieBuilder kieBuilder = kieServices.newKieBuilder(kieFileSystem()); kieBuilder.buildAll(); return kieServices.newKieContainer(kieRepository.getDefaultReleaseId()); &#125; @Bean @ConditionalOnMissingBean public KieBase kieBase() throws IOException &#123; return kieContainer().getKieBase(); &#125; @Bean @ConditionalOnMissingBean public KModuleBeanFactoryPostProcessor kiePostProcessor() &#123; return new KModuleBeanFactoryPostProcessor(); &#125;&#125; 第四步：编写实体类CreditCardApplyInfo package com.itheima.drools.entity;/** * 信用卡申请信息 */public class CreditCardApplyInfo &#123; public static final String EDUCATION_1 = &quot;专科以下&quot;; public static final String EDUCATION_2 = &quot;专科&quot;; public static final String EDUCATION_3 = &quot;本科&quot;; public static final String EDUCATION_4 = &quot;本科以上&quot;; private String name; private String sex; private int age; private String education; private String telephone; private double monthlyIncome = 0;//月收入 private String address; private boolean hasHouse = false;//是否有房 private boolean hasCar = false;//是否有车 private int hasCreditCardCount = 0;//现持有信用卡数量 private boolean checkResult = true;//审核是否通过 private double quota = 0;//额度 public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getEducation() &#123; return education; &#125; public void setEducation(String education) &#123; this.education = education; &#125; public String getTelephone() &#123; return telephone; &#125; public void setTelephone(String telephone) &#123; this.telephone = telephone; &#125; public double getMonthlyIncome() &#123; return monthlyIncome; &#125; public void setMonthlyIncome(double monthlyIncome) &#123; this.monthlyIncome = monthlyIncome; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public boolean isHasHouse() &#123; return hasHouse; &#125; public void setHasHouse(boolean hasHouse) &#123; this.hasHouse = hasHouse; &#125; public boolean isHasCar() &#123; return hasCar; &#125; public void setHasCar(boolean hasCar) &#123; this.hasCar = hasCar; &#125; public int getHasCreditCardCount() &#123; return hasCreditCardCount; &#125; public void setHasCreditCardCount(int hasCreditCardCount) &#123; this.hasCreditCardCount = hasCreditCardCount; &#125; public boolean isCheckResult() &#123; return checkResult; &#125; public void setCheckResult(boolean checkResult) &#123; this.checkResult = checkResult; &#125; public double getQuota() &#123; return quota; &#125; public void setQuota(double quota) &#123; this.quota = quota; &#125; public String toString() &#123; if(checkResult)&#123; return &quot;审核通过，信用卡额度为：&quot; + quota; &#125;else &#123; return &quot;审核不通过&quot;; &#125; &#125;&#125; 第五步：在resources&#x2F;rules下创建规则文件creditCardApply.drl文件 package com.itheima.creditCardApplyimport com.itheima.drools.entity.CreditCardApplyInfo//合法性检查rule &quot;如果申请人既没房也没车，同时学历为大专以下，并且月薪少于5000，那么不通过&quot; salience 10 no-loop true when $c:CreditCardApplyInfo(hasCar == false &amp;&amp; hasHouse == false &amp;&amp; education == CreditCardApplyInfo.EDUCATION_1 &amp;&amp; monthlyIncome &lt; 5000) then $c.setCheckResult(false); drools.halt();endrule &quot;如果申请人既没房也没车，同时学历为大专或本科，并且月薪少于3000，那么不通过&quot; salience 10 no-loop true when $c:CreditCardApplyInfo(hasCar == false &amp;&amp; hasHouse == false &amp;&amp; (education == CreditCardApplyInfo.EDUCATION_2 || education == CreditCardApplyInfo.EDUCATION_3) &amp;&amp; monthlyIncome &lt; 3000) then $c.setCheckResult(false); drools.halt();endrule &quot;如果申请人既没房也没车，同时学历为本科以上，并且月薪少于2000，同时之前没有信用卡的，那么不通过&quot; salience 10 no-loop true when $c:CreditCardApplyInfo(hasCar == false &amp;&amp; hasHouse == false &amp;&amp; education == CreditCardApplyInfo.EDUCATION_4 &amp;&amp; monthlyIncome &lt; 2000 &amp;&amp; hasCreditCardCount == 0) then $c.setCheckResult(false); drools.halt();endrule &quot;如果申请人现有的信用卡数量大于10，那么不通过&quot; salience 10 no-loop true when $c:CreditCardApplyInfo(hasCreditCardCount &gt; 10) then $c.setCheckResult(false); drools.halt();end//确定额度rule &quot;如果申请人有房有车，或者月收入在20000以上，那么发放的信用卡额度为15000&quot; salience 1 no-loop true activation-group &quot;quota_group&quot; when $c:CreditCardApplyInfo(checkResult == true &amp;&amp; ((hasHouse == true &amp;&amp; hasCar == true) || (monthlyIncome &gt; 20000))) then $c.setQuota(15000);endrule &quot;如果申请人没房没车，但月收入在10000~20000之间，那么发放的信用卡额度为6000&quot; salience 1 no-loop true activation-group &quot;quota_group&quot; when $c:CreditCardApplyInfo(checkResult == true &amp;&amp; hasHouse == false &amp;&amp; hasCar == false &amp;&amp; monthlyIncome &gt;= 10000 &amp;&amp; monthlyIncome &lt;= 20000) then $c.setQuota(6000);endrule &quot;如果申请人没房没车，月收入在10000以下，那么发放的信用卡额度为3000&quot; salience 1 no-loop true activation-group &quot;quota_group&quot; when $c:CreditCardApplyInfo(checkResult == true &amp;&amp; hasHouse == false &amp;&amp; hasCar == false &amp;&amp; monthlyIncome &lt; 10000) then $c.setQuota(3000);endrule &quot;如果申请人有房没车或者没房但有车，月收入在10000以下，那么发放的信用卡额度为5000&quot; salience 1 no-loop true activation-group &quot;quota_group&quot; when $c:CreditCardApplyInfo(checkResult == true &amp;&amp; ((hasHouse == true &amp;&amp; hasCar == false) || (hasHouse == false &amp;&amp; hasCar == true)) &amp;&amp; monthlyIncome &lt; 10000) then $c.setQuota(5000);endrule &quot;如果申请人有房没车或者是没房但有车，月收入在10000~20000之间，那么发放的信用卡额度为8000&quot; salience 1 no-loop true activation-group &quot;quota_group&quot; when $c:CreditCardApplyInfo(checkResult == true &amp;&amp; ((hasHouse == true &amp;&amp; hasCar == false) || (hasHouse == false &amp;&amp; hasCar == true)) &amp;&amp; monthlyIncome &gt;= 10000 &amp;&amp; monthlyIncome &lt;= 20000) then $c.setQuota(8000);end 第六步：创建RuleService package com.itheima.drools.service;import com.itheima.drools.entity.CreditCardApplyInfo;import org.kie.api.KieBase;import org.kie.api.runtime.KieSession;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class RuleService &#123; @Autowired private KieBase kieBase; //调用Drools规则引擎实现信用卡申请 public CreditCardApplyInfo creditCardApply(CreditCardApplyInfo creditCardApplyInfo)&#123; KieSession session = kieBase.newKieSession(); session.insert(creditCardApplyInfo); session.fireAllRules(); session.dispose(); return creditCardApplyInfo; &#125;&#125; 第七步：创建RuleController package com.itheima.drools.controller;import com.itheima.drools.entity.CreditCardApplyInfo;import com.itheima.drools.service.RuleService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/rule&quot;)public class RuleController &#123; @Autowired private RuleService ruleService; @RequestMapping(&quot;/creditCardApply&quot;) public CreditCardApplyInfo creditCardApply(@RequestBody CreditCardApplyInfo creditCardApplyInfo)&#123; creditCardApplyInfo = ruleService.creditCardApply(creditCardApplyInfo); return creditCardApplyInfo; &#125;&#125; 第八步：创建启动类DroolsApplication package com.itheima.drools;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class DroolsApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DroolsApplication.class); &#125;&#125; 第九步：导入静态资源文件到resources&#x2F;static目录下 9.3 保险产品准入规则9.3.1 决策表前面的课程中我们编写的规则文件都是drl形式的文件，Drools除了支持drl形式的文件外还支持xls格式的文件（即Excel文件）。这种xls格式的文件通常称为决策表（decision table）。 决策表（decision table）是一个“精确而紧凑的”表示条件逻辑的方式，非常适合商业级别的规则。决策表与现有的drl文件可以无缝替换。Drools提供了相应的API可以将xls文件编译为drl格式的字符串。 一个决策表的例子如下： 决策表语法： 关键字 说明 是否必须 RuleSet 相当于drl文件中的package 必须，只能有一个。如果没有设置RuleSet对应的值则使用默认值rule_table Sequential 取值为Boolean类型。true表示规则按照表格自上到下的顺序执行，false表示乱序 可选 Import 相当于drl文件中的import，如果引入多个类则类之间用逗号分隔 可选 Variables 相当于drl文件中的global，用于定义全局变量，如果有多个全局变量则中间用逗号分隔 可选 RuleTable 它指示了后面将会有一批rule，RuleTable的名称将会作为以后生成rule的前缀 必须 CONDITION 规则条件关键字，相当于drl文件中的when。下面两行则表示 LHS 部分，第三行则为注释行，不计为规则部分，从第四行开始，每一行表示一条规则 每个规则表至少有一个 ACTION 规则结果关键字，相当于drl文件中的then 每个规则表至少有一个 NO-LOOP 相当于drl文件中的no-loop 可选 AGENDA-GROUP 相当于drl文件中的agenda-group 可选 在决策表中还经常使用到占位符，语法为$后面加数字，用于替换每条规则中设置的具体值。 上面的决策表例子转换为drl格式的规则文件内容如下： package rules;import com.itheima.drools.entity.PersonInfoEntity;import java.util.List;global java.util.List listRules;rule &quot;personCheck_10&quot; salience 65535 agenda-group &quot;sign&quot; when $person : PersonInfoEntity(sex != &quot;男&quot;) then listRules.add(&quot;性别不对&quot;);endrule &quot;personCheck_11&quot; salience 65534 agenda-group &quot;sign&quot; when $person : PersonInfoEntity(age &lt; 22 || age &gt; 25) then listRules.add(&quot;年龄不合适&quot;);endrule &quot;personCheck_12&quot; salience 65533 agenda-group &quot;sign&quot; when $person : PersonInfoEntity(salary &lt; 10000) then listRules.add(&quot;工资太低了&quot;);end 要进行决策表相关操作，需要导入如下maven坐标： &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-decisiontables&lt;/artifactId&gt; &lt;version&gt;7.10.0.Final&lt;/version&gt;&lt;/dependency&gt; 通过下图可以发现，由于maven的依赖传递特性在导入drools-decisiontables坐标后，drools-core和drools-compiler等坐标也被传递了过来 Drools提供的将xls文件编译为drl格式字符串的API如下： String realPath = &quot;C:\\\\testRule.xls&quot;;//指定决策表xls文件的磁盘路径File file = new File(realPath);InputStream is = new FileInputStream(file);SpreadsheetCompiler compiler = new SpreadsheetCompiler();String drl = compiler.compile(is, InputType.XLS); Drools还提供了基于drl格式字符串创建KieSession的API： KieHelper kieHelper = new KieHelper();kieHelper.addContent(drl, ResourceType.DRL);KieSession session = kieHelper.build().newKieSession(); 基于决策表的入门案例： 第一步：创建maven工程drools_decisiontable_demo并配置pom.xml文件 &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-decisiontables&lt;/artifactId&gt; &lt;version&gt;7.10.0.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt;&lt;/dependency&gt; 第二步：创建实体类PersonInfoEntity package com.itheima.drools.entity;public class PersonInfoEntity &#123; private String sex; private int age; private double salary; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public double getSalary() &#123; return salary; &#125; public void setSalary(double salary) &#123; this.salary = salary; &#125;&#125; 第三步：创建xls规则文件（可以直接使用资料中提供的testRule.xls文件） 第四步：创建单元测试 @Testpublic void test1() throws Exception&#123; String realPath = &quot;d:\\\\testRule.xls&quot;;//指定决策表xls文件的磁盘路径 File file = new File(realPath); InputStream is = new FileInputStream(file); SpreadsheetCompiler compiler = new SpreadsheetCompiler(); String drl = compiler.compile(is, InputType.XLS); System.out.println(drl); KieHelper kieHelper = new KieHelper(); kieHelper.addContent(drl, ResourceType.DRL); KieSession session = kieHelper.build().newKieSession(); PersonInfoEntity personInfoEntity = new PersonInfoEntity(); personInfoEntity.setSex(&quot;男&quot;); personInfoEntity.setAge(35); personInfoEntity.setSalary(1000); List&lt;String&gt; list = new ArrayList&lt;String&gt;(); session.setGlobal(&quot;listRules&quot;,list); session.insert(personInfoEntity); session.getAgenda().getAgendaGroup(&quot;sign&quot;).setFocus(); session.fireAllRules(); for (String s : list) &#123; System.out.println(s); &#125; session.dispose();&#125; 9.3.2 规则介绍各保险公司针对人身、财产推出了不同的保险产品，作为商业保险公司，筛选出符合公司利益最大化的客户是非常重要的，即各保险产品的准入人群是不同的，也就是说保险公司会针对不同的人群特征，制定不同的产品缴费和赔付规则。 我们来看一下某保险产品准入规则的简化版，当不满足以下规则时，系统模块需要返回准入失败标识和失败原因 规则1： 保险公司是：PICC规则2： 销售区域是：北京、天津规则3： 投保人年龄：0 ~ 17岁规则4： 保险期间是：20年、25年、30年规则5： 缴费方式是：趸交（一次性交清）或年交规则6： 保险期与交费期规则一：保险期间为20年期交费期间最长10年交且不能选择[趸交]规则7： 保险期与交费期规则二：保险期间为25年期交费期间最长15年交且不能选择[趸交]规则8： 保险期与交费期规则三：保险期间为30年期交费期间最长20年交且不能选择[趸交]规则9： 被保人要求：（投保年龄+保险期间）不得大于40周岁规则10： 保险金额规则：投保时约定，最低为5万元，超过部分必须为1000元的整数倍规则11： 出单基本保额限额规则：线上出单基本保额限额62.5万元，超62.5万元需配合契调转线下出单 在本案例中规则文件是一个Excel文件，业务人员可以直接更改这个文件中指标的值，系统不需要做任何变更。 9.3.3 实现步骤本案例还是基于Spring Boot整合Drools的架构来实现。 第一步：创建maven工程insuranceInfoCheck并配置pom.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starters&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;insuranceInfoCheck&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;!--drools规则引擎--&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-core&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-templates&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-api&lt;/artifactId&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-spring&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;version&gt;7.6.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.*&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 第二步：创建&#x2F;resources&#x2F;application.yml文件 server: port: 8080spring: application: name: insuranceInfoCheck 第三步：创建实体类InsuranceInfo package com.itheima.drools.entity;/** * 保险信息 */public class InsuranceInfo &#123; private String param1;//保险公司 private String param2;//方案代码 private String param3;//渠道号 private String param4;//销售区域 private String param5;//投保年龄 private String param6;//保险期间 private String param7;//缴费期间 private String param8;//缴费方式 private String param9;//保障类型 private String param10;//等待期 private String param11;//犹豫期 private String param12;//职业类型 private String param13;//保额限制 private String param14;//免赔额 private String param15;//主险保额 private String param16;//主险保费 private String param17;//附加险保额 private String param18;//附加险保费 private String param19;//与投保人关系 private String param20;//与被保人关系 private String param21;//性别 private String param22;//证件 private String param23;//保费 private String param24;//保额 //getter setter省略&#125; 第四步：创建决策表文件（也可以直接使用实战资料中提供的insuranceInfoCheck.xls文件） 第五步：封装工具类KieSessionUtils package com.itheima.drools.utils;import com.itheima.drools.entity.InsuranceInfo;import com.itheima.drools.entity.PersonInfoEntity;import org.drools.decisiontable.InputType;import org.drools.decisiontable.SpreadsheetCompiler;import org.kie.api.builder.Message;import org.kie.api.builder.Results;import org.kie.api.io.ResourceType;import org.kie.api.runtime.KieSession;import org.kie.internal.utils.KieHelper;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.InputStream;import java.util.ArrayList;import java.util.List;public class KieSessionUtils &#123; private KieSessionUtils() &#123; &#125; // 把xls文件解析为String public static String getDRL (String realPath) throws FileNotFoundException &#123; File file = new File(realPath); // 例如：C:\\\\abc.xls InputStream is = new FileInputStream(file); SpreadsheetCompiler compiler = new SpreadsheetCompiler(); String drl = compiler.compile(is, InputType.XLS); System.out.println(drl); return drl; &#125; // drl为含有内容的字符串 public static KieSession createKieSessionFromDRL(String drl) throws Exception&#123; KieHelper kieHelper = new KieHelper(); kieHelper.addContent(drl, ResourceType.DRL); Results results = kieHelper.verify(); if (results.hasMessages(Message.Level.WARNING, Message.Level.ERROR)) &#123; List&lt;Message&gt; messages = results.getMessages(Message.Level.WARNING, Message.Level.ERROR); for (Message message : messages) &#123; System.out.println(&quot;Error: &quot;+message.getText()); &#125; // throw new IllegalStateException(&quot;Compilation errors were found. Check the logs.&quot;); &#125; return kieHelper.build().newKieSession(); &#125; // realPath为Excel文件绝对路径 public static KieSession getKieSessionFromXLS(String realPath) throws Exception &#123; return createKieSessionFromDRL(getDRL(realPath)); &#125;&#125; 第六步：创建RuleService类 package com.itheima.drools.service;import com.itheima.drools.entity.InsuranceInfo;import com.itheima.drools.utils.KieSessionUtils;import org.kie.api.runtime.KieSession;import org.springframework.stereotype.Service;import java.util.ArrayList;import java.util.List;@Servicepublic class RuleService &#123; public List&lt;String&gt; insuranceInfoCheck(InsuranceInfo insuranceInfo) throws Exception&#123; KieSession session = KieSessionUtils.getKieSessionFromXLS(&quot;D:\\\\rules.xls&quot;); session.getAgenda().getAgendaGroup(&quot;sign&quot;).setFocus(); session.insert(insuranceInfo); List&lt;String&gt; listRules = new ArrayList&lt;&gt;(); session.setGlobal(&quot;listRules&quot;, listRules); session.fireAllRules(); return listRules; &#125;&#125; 第七步：创建RuleController类 package com.itheima.drools.controller;import com.itheima.drools.entity.InsuranceInfo;import com.itheima.drools.service.RuleService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.HashMap;import java.util.List;import java.util.Map;@RestController@RequestMapping(&quot;/rule&quot;)public class RuleController &#123; @Autowired private RuleService ruleService; @RequestMapping(&quot;/insuranceInfoCheck&quot;) public Map insuranceInfoCheck()&#123; Map map = new HashMap(); //模拟数据，实际应为页面传递过来 InsuranceInfo insuranceInfo = new InsuranceInfo(); insuranceInfo.setParam1(&quot;picc&quot;); insuranceInfo.setParam4(&quot;上海&quot;); insuranceInfo.setParam5(&quot;101&quot;); insuranceInfo.setParam6(&quot;12&quot;); insuranceInfo.setParam7(&quot;222&quot;); insuranceInfo.setParam8(&quot;1&quot;); insuranceInfo.setParam13(&quot;3&quot;); try &#123; List&lt;String&gt; list = ruleService.insuranceInfoCheck(insuranceInfo); if(list != null &amp;&amp; list.size() &gt; 0)&#123; map.put(&quot;checkResult&quot;,false); map.put(&quot;msg&quot;,&quot;准入失败&quot;); map.put(&quot;detail&quot;,list); &#125;else&#123; map.put(&quot;checkResult&quot;,true); map.put(&quot;msg&quot;,&quot;准入成功&quot;); &#125; return map; &#125; catch (Exception e) &#123; e.printStackTrace(); map.put(&quot;checkResult&quot;,false); map.put(&quot;msg&quot;,&quot;未知错误&quot;); return map; &#125; &#125;&#125; 第八步：创建启动类DroolsApplication package com.itheima.drools;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class DroolsApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DroolsApplication.class); &#125;&#125;","categories":[{"name":"其他","slug":"others","permalink":"http://wuzguo.com/blog/categories/others/"}],"tags":[{"name":"Drools","slug":"Drools","permalink":"http://wuzguo.com/blog/tags/Drools/"},{"name":"规则引擎","slug":"规则引擎","permalink":"http://wuzguo.com/blog/tags/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E/"}],"author":"Zak"},{"title":"Vi 和 Vim 的三种模式","slug":"os/vi_vim_three_modes","date":"2020-07-02T03:32:00.000Z","updated":"2022-08-01T06:35:23.844Z","comments":true,"path":"2020/07/02/os/vi_vim_three_modes.html","link":"","permalink":"http://wuzguo.com/blog/2020/07/02/os/vi_vim_three_modes.html","excerpt":"","text":"vi 和 vim 的三种模式使用 Vim 编辑文件时，存在 3 种工作模式，分别是命令模式、输入模式和编辑模式，这三种工作模式可随意切换，如图所示。 一、命令模式​ 使用 Vim 编辑文件时，默认处于命令模式。此模式下，可使用方向键（上、下、左、右键）或 k、j、h、i 移动光标的位置，还可以对文件内容进行复制、粘贴、替换、删除等操作。 命令 含义 dd 删除光标当前行 dnd 删除n行 u 撤销上一步 yy 复制光标当前行 p 粘贴 x,X 删除一个字母 dw 删除一个词 yw 复制一个词 shift+^ 移动到行头 shift+$ 移动到行尾 shift+g 移动到页尾 数字1+shift+g 移动到页头 数字N+shift+g 移动到目标 二、输入模式在输入模式下，Vim 可以对文件执行写操作，类似于在 Windows 系统的文档中输入内容。 使 Vim 进行输入模式的方式是在命令模式状态下输入 i、I、a、A、o、O 等插入命令（各指令的具体功能如表 3 所示），当编辑文件完成后按 Esc 键即可返回命令模式。 命令 含义 i，I 当前光标前 a，A 当前光标后 o，O 当前光标行的下一行 s , S s: 删除当前字符并进入编辑 S: 删除整行并进入编辑 R 进入替换模式 三、编辑模式编辑模式用于对文件中的指定内容执行保存、查找或替换等操作。 使 Vim 切换到编辑模式的方法是在命令模式状态下按“：”键，此时 Vim 窗口的左下方出现一个“：”符号，这是就可以输入相关指令进行操作了。 指令执行后 Vim 会自动返回命令模式。如想直接返回命令模式，按 Esc 即可。 命令 含义 :w 保存 :q 退出 :! 强制执行 :%s&#x2F;old字符&#x2F;new字符 批量替换 &#x2F; 要查找的词 n 查找下一个，N 往上查找 N 查找下一个，n 往上查找 :set nu &#x2F; :set nonu 显示行号 &#x2F;关闭行号","categories":[{"name":"操作系统","slug":"os","permalink":"http://wuzguo.com/blog/categories/os/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://wuzguo.com/blog/tags/linux/"},{"name":"vi","slug":"vi","permalink":"http://wuzguo.com/blog/tags/vi/"},{"name":"vim","slug":"vim","permalink":"http://wuzguo.com/blog/tags/vim/"}],"author":"Zak"},{"title":"Shell 工具","slug":"os/shell_tutorial_5","date":"2020-06-30T09:51:00.000Z","updated":"2022-08-01T06:35:23.778Z","comments":true,"path":"2020/06/30/os/shell_tutorial_5.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/30/os/shell_tutorial_5.html","excerpt":"","text":"1. cutcut的工作就是”剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出。 1.1 基本用法cut [选项参数] filename 说明：默认分隔符是制表符 1.2 选项参数说明 选项参数 功能 -f 列号，提取第几列 -d 分隔符，按照指定分隔符分割列 -c 指定具体的字符 1.3 案例实操 数据准备 [root@docker-ecs data]# touch cut.txt[root@docker-ecs data]# vim cut.txtdong shenguan zhenwo wolai laile le 切割 cut.txt 第一列 [root@docker-ecs data]# cut -d &quot; &quot; -f 1 cut.txt dongguanwolaile 切割cut.txt第二、三列 [root@docker-ecs data]# cut -d &quot; &quot; -f 2,3 cut.txt shenzhenwolaile 在 cut.txt 文件中切割出 guan [root@docker-ecs data]# cat cut.txt | grep &quot;guan&quot; | cut -d &quot; &quot; -f 1guan 选取系统PATH变量值，第2个“：”开始后的所有路径： [root@docker-ecs data]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/java/jdk1.8.0_192/bin:/usr/java/jdk1.8.0_192/jre/bin:/root/bin [root@docker-ecs data]# echo $PATH | cut -d: -f 2-/usr/local/bin:/usr/sbin:/usr/bin:/usr/java/jdk1.8.0_192/bin:/usr/java/jdk1.8.0_192/jre/bin:/root/bin 切割 ifconfig 后打印的 IP地址 [root@docker-ecs data]# ifconfig eth0 | grep &quot;inet&quot; | cut -d: -f 2 | cut -d &quot; &quot; -f 1192.168.1.102 2. sedsed是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”，接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 2.1 基本用法sed [选项参数] ‘command’ filename 2.2 选项参数说明 选项参数 功能 -e 直接在指令列模式上进行sed的动作编辑。 -i 直接编辑文件 2.3 命令功能描述 命令 功能描述 a 新增，a的后面可以接字串，在下一行出现 d 删除 s 查找并替换 2.4 案例实操 数据准备 [root@docker-ecs data]# touch sed.txt[root@docker-ecs data]# vim sed.txtdong shenguan zhenwo wolai laile le 将“mei nv” 这个单词插入到 sed.txt 第二行下，打印。 [root@docker-ecs data]# sed &#x27;2a mei nv&#x27; sed.txt dong shenguan zhenmei nvwo wolai laile le[root@docker-ecs data]# cat sed.txt dong shenguan zhenwo wolai laile le 注意：文件并没有改变 删除sed.txt文件所有包含wo的行 [root@docker-ecs data]# sed &#x27;/wo/d&#x27; sed.txtdong shenguan zhenlai laile le 将sed.txt文件中wo替换为ni [root@docker-ecs data]# sed &#x27;s/wo/ni/g&#x27; sed.txt dong shenguan zhenni nilai laile le 注意：‘g’ 表示global，全部替换 将sed.txt文件中的第二行删除并将wo替换为ni [root@docker-ecs data]# sed -e &#x27;2d&#x27; -e &#x27;s/wo/ni/g&#x27; sed.txt dong shenguan zhenni nilai laile le 3. awk一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理。 3.1 基本用法awk [选项参数] ‘ pattern1{action1} pattern2{action2}…’ filename pattern：表示AWK在数据中查找的内容，就是匹配模式 action：在找到匹配内容时所执行的一系列命令 3.2 选项参数说明 选项参数 功能 -F 指定输入文件折分隔符 -v 赋值一个用户定义变量 3.3 案例实操 数据准备 [root@docker-ecs data]# sudo cp /etc/passwd ./ 搜索passwd文件以root关键字开头的所有行，并输出该行的第7列。 [root@docker-ecs data]# awk -F: &#x27;/^root/&#123;print $7&#125;&#x27; passwd /bin/bash 搜索passwd文件以root关键字开头的所有行，并输出该行的第1列和第7列，中间以“，”号分割。 [root@docker-ecs data]# awk -F: &#x27;/^root/&#123;print $1&quot;,&quot;$7&#125;&#x27; passwd root,/bin/bash 注意：只有匹配了pattern的行才会执行action 只显示 &#x2F;etc&#x2F;passwd 的第一列和第七列，以逗号分割，且在所有行前面添加列名user，shell在最后一行添加”hello,&#x2F;bin&#x2F;word”。 [root@docker-ecs data]# awk -F : &#x27;BEGIN&#123;print &quot;user, shell&quot;&#125; &#123;print $1&quot;,&quot;$7&#125; END&#123;print &quot;hello,/bin/word&quot;&#125;&#x27; passwduser, shellroot,/bin/bashbin,/sbin/nologin......hello，/bin/word 注意：BEGIN 在所有数据读取行之前执行；END 在所有数据执行之后执行。 将passwd文件中的用户id增加数值1并输出 [root@docker-ecs data]# awk -v i=1 -F: &#x27;&#123;print $3+i&#125;&#x27; passwd12345...... 3.4 awk的内置变量 变量 说明 FILENAME 文件名 NR 已读的记录数 NF 浏览记录的域的个数（切割后，列的个数） 3.5 案例实操 统计 passwd 文件名，每行的行号，每行的列数 [root@docker-ecs data]# awk -F: &#x27;&#123;print &quot;filename:&quot; FILENAME &quot;, linenum:&quot; NR &quot;,columns:&quot; NF&#125;&#x27; passwdfilename:passwd, linenum:1,columns:7filename:passwd, linenum:2,columns:7filename:passwd, linenum:3,columns:7filename:passwd, linenum:4,columns:7...... 切割IP [root@docker-ecs data]# ifconfig eth0 | grep &quot;inet&quot; | awk -F: &#x27;&#123;print $2&#125;&#x27; | awk -F &quot; &quot; &#x27;&#123;print $1&#125;&#x27; 192.168.1.102 查询sed.txt中空行所在的行号 [root@docker-ecs data]# awk &#x27;/^$/&#123;print NR&#125;&#x27; sed.txt 2468 4. sortsort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。 4.1 基本语法sort (选项) (参数) 选项 说明 -n 依照数值的大小排序 -r 以相反的顺序来排序 -t 设置排序时所用的分隔字符 -k 指定需要排序的列 参数：指定待排序的文件列表 4.2 案例实操 数据准备 [root@docker-ecs data]# touch sort.sh[root@docker-ecs data]# vim sort.sh bb:40:5.4bd:20:4.2xz:50:2.3cls:10:3.5ss:30:1.6 按照“：”分割后的第三列倒序排序。 [root@docker-ecs data]# sort -t : -nrk 3 sort.sh bb:40:5.4bd:20:4.2cls:10:3.5xz:50:2.3ss:30:1.6","categories":[{"name":"操作系统","slug":"os","permalink":"http://wuzguo.com/blog/categories/os/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://wuzguo.com/blog/tags/shell/"},{"name":"linux","slug":"linux","permalink":"http://wuzguo.com/blog/tags/linux/"},{"name":"bash","slug":"bash","permalink":"http://wuzguo.com/blog/tags/bash/"},{"name":"centos","slug":"centos","permalink":"http://wuzguo.com/blog/tags/centos/"}],"author":"Zak"},{"title":"Shell 函数","slug":"os/shell_tutorial_4","date":"2020-06-30T02:47:00.000Z","updated":"2022-08-01T06:35:23.818Z","comments":true,"path":"2020/06/30/os/shell_tutorial_4.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/30/os/shell_tutorial_4.html","excerpt":"","text":"1. 系统函数1.1 basename基本语法basename [string &#x2F; pathname] [suffix] （功能描述：basename命令会删掉所有的前缀包括最后一个（‘&#x2F;’）字符，然后将字符串显示出来。 选项： suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。 1.2 案例实操（1）截取该 &#x2F;data&#x2F;hello.sh 路径的文件名称 [root@docker-ecs data]# basename ./hello.sh hello.sh[root@docker-ecs data]# basename /data/hello.sh hello.sh[root@docker-ecs data]# basename /data/hello.sh .shhello[root@docker-ecs data]# basename /data/hello.sh .xxhello.sh 1.3 dirname基本语法​ dirname 文件绝对路径 （功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）） 1.4 案例实操（1）获取banzhang.txt文件的路径 [root@docker-ecs data]# dirname /data/hello.sh /data 2. 自定义函数2.1 基本语法[ function ] funname[()]&#123;Action;[return int;]&#125;funname 2.2 经验技巧​ （1）必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先编译。 ​ （2）函数返回值，只能通过$?系统变量获得，可以显示加：return返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255) 3．案例实操 ​ （1）计算两个输入参数的和 [root@docker-ecs data]# touch fun.sh[root@docker-ecs data]# vim fun.sh#!/bin/bashfunction sum()&#123; s=0 s=$[ $1 + $2 ] echo &quot;$s&quot;&#125; read -p &quot;Please input the number1: &quot; n1;read -p &quot;Please input the number2: &quot; n2;sum $n1 $n2; [root@docker-ecs data]# chmod 777 fun.sh[root@docker-ecs data]# ./fun.sh Please input the number1: 2Please input the number2: 57","categories":[{"name":"操作系统","slug":"os","permalink":"http://wuzguo.com/blog/categories/os/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://wuzguo.com/blog/tags/shell/"},{"name":"linux","slug":"linux","permalink":"http://wuzguo.com/blog/tags/linux/"},{"name":"bash","slug":"bash","permalink":"http://wuzguo.com/blog/tags/bash/"},{"name":"centos","slug":"centos","permalink":"http://wuzguo.com/blog/tags/centos/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（十三）","slug":"aiot/aiot_tutorial_13","date":"2020-06-29T05:50:00.000Z","updated":"2022-08-01T06:35:23.633Z","comments":true,"path":"2020/06/29/aiot/aiot_tutorial_13.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/29/aiot/aiot_tutorial_13.html","excerpt":"","text":"在前面的课程里，我们使用的是 MQTT 3.1.1 版本，也是目前支持和使用最广泛的版本。2017 年 8 月，OASIS MQTT Technical Committee 正式发布了用于 Public Review 的 MQTT 5.0 的草案。2018 年，MQTT 5.0 已正式发布，虽然目前支持 MQTT 5.0 的 Broker 和 Client 库还比较有限，但是作为 MQTT 未来的发展方向，我认为了解 5.0 的新特性还是很有必要的，也许看完你就想马上迁移到 MQTT 5.0 呢！ MQTT 5.0 在 MQTT 3.1.1 的基础上做了很多改变，同时也不是向下兼容的。在这里我挑了几个我认为比较实用的新特性进行介绍，这些新特性能够解决在 3.1.1 版本中较难处理的问题。 作为 MQTT 3.1.1 的后续版本，为什么版本号直接变成了 5.0？因为 3.1.1 在 CONNECT 的时候指定的 Protocol Version 为 4，所以后续版本只有使用 5 了。本节课核心内容： 用户属性（User Properties） 共享订阅（Shared Subscriptions） 消息过期（Publication Expiry Interval） 重复主题 Broker 能力查询 双向 DISCONNECT 13.1 用户属性（User Properties）5.0 中可以在 PUBLISH、CONNECT 和带有 Return Code 的数据包中夹带一个和多个用户属性数据： 在 PUBLISH 包中携带的用户属性由发送方的应用定义，随消息被 Broker 转发到消息的订阅方； CONNECT 和 ACKs 消息里面也可以带发送者自定义的用户属性数据。 在实际的项目中，我们除了关心收到的消息内容，往往也想知道这个消息来自于谁。例如：ClientA 收到 ClientB 发布的消息后，ClientA 想给 ClientB 发送一个回复，这时 ClientA 必须知道 ClientB 订阅的主题才能将消息传递给 ClientB。在 MQTT 3.1.1 中，我们通常是在消息数据中包含发布方的信息，比如它订阅的主题等。5.0 以后就可以把这些信息放在 User Properties 里面了。 13.2 共享订阅（Shared Subscriptions）在 3.1.1 和之前的版本，订阅同一主题的订阅者都会同样收到来自这个主题的所有消息。例如你需要处理一个传感器的数据，假设这个传感器上传的数据量非常大且频率很高，你没有办法启动多个 Client 来分担处理的工作。你做得最多的是启动一个 Client 来接收传感器的数据，并将这些数据分配给后面多个 Worker 来处理。这个用于接收数据的 Client 就是系统的瓶颈和单点故障之一。 通常，我们可以通过主题分片。比如，让传感器依次发布到 &#x2F;topic1…&#x2F;topicN 来变通地解决这个问题，但仅仅是解决了部分问题，同时也提高了系统的复杂度。 而在 5.0 里面，MQTT 可以实现 Producer&#x2F;Consumer 模式了。多个 Client（Consumer）可以一起订阅一个共享主题（Producer），来自这个主题的消息会依次均衡地发布给这些 Client，实现订阅者的负载均衡，最终解决了这个问题。 这个功能在传统的队列系统，比如 RabbitMQ 里面很常见。如果你不想升级到 5.0，EMQTT 在 3.1.1 上已经支持这个功能了，详情可以查询官方文档。 13.3 消息过期（Publication Expiry Interval）假设你设计了一个基于 MQTT 的共享单车平台，用户通过平台下发一条开锁指令给一辆单车，但是不巧的是，单车的网络信号（比如 GSM）这时恰好断了，用户摇了摇头走开去找其他车了。过了两个小时以后，单车的网络恢复了，它收到了两个小时以前的开锁指令，该怎么做？ 为了处理这种情况，在 3.1.1 和之前的版本，我们往往都是在消息数据里带一个消息过期时间，在接收端来判断消息是否过期。但是这要求设备端的时间和 Server 端保持一致，对于一些电量不是很充足的设备，一但断电，之后再启动时间就会变得不准确，会导致异常的出现。 5.0 中终于包含了消息过期的功能，在发布的时候可以指定这个消息在多久之后过期，Broker 不会将已过期的离线消费发送到 Client。 13.4 重复主题在 MQTT 3.1.1 和之前的版本里，PUBLISH 数据包每次都需要带上发布的主题名，即使你每次发布的都是同一个主题。 在 5.0 里面，如果你将一条 PUBLISH 的主题名设为长度为 0 的字符串，那么 Broker 会使用你上一次发布的主题。这样降低了多次发布到同一主题（往往都是这样）的额外开销，对网络资源和处理资源都有限的系统非常有用。 13.5 Broker 能力查询在 5.0 里面，CONNACK 包含了一些预定义的头部数据，用于标识 Broker 支持哪些 MQTT 协议功能。 Pre-defined Header 数据类型 描述 Retain Available Boolean 是否支持 Retained 消息 Maximum QoS Number Client 可以用于订阅和发布的最大 QoS Wildcard available Boolean 订阅时是否可以使用通配符主题 Subscription identifiers available Boolean 是否支持 Subscription Identifier（5.0 特性） Shared Subscriptions available Boolean 是否支持共享订阅 Maximum Message Size Number 可发送的最大消息长度 Server Keep Alive Number Broker 支持的最大 Keep Alive 值 Client 在连接之后就可以知道 Broker 是否支持自己所要用到的功能，这对于一些通用的 MQTT 设备生产商或者 Client 库的开发者很有用。 13.6 双向 DISCONNECT在 3.1.1 和之前的版本里，只有 Client 在主动断开时会向 Broker 发送 DISCONNECT 包。如果因为某种错误 Broker 要断开和 Client 的连接，它只能直接断开底层 TCP 连接，Client 则不会知道自己连接断开的原因，也无法解决错误，只是简单地重新连接、被断开、重新连接…… 在 5.0 里面，Broker 在主动断开和 Client 的连接时也会发送 DISCONNECT 包。同时，从 Client 到 Broker，以及从 Broker 到 Client 的 CONNCET 包里面都会包含一个 Reason Code，来标识断开的原因。 Reason code 发送方 描述 0 Client 或 Broker 正常断开连接，不发送遗愿消息 4 Client 正常断开，但是要求 Broker 发送遗愿消息 129 Client 或 Broker MQTT 数据包格式错误 135 Broker 请求未授权 143 Broker 主题过滤器格式正确，但是 Broker 不接收 144 Client 或 Broker 主题名格式正确，但是 Client 或者 Broker 不接收 153 Client 或者 Broker 消息体格式不正确 154 Broker 不支持 Retained 消息 155 Broker QoS 等级不支持 158 Broker 不支持共享订阅 162 Broker 订阅时不支持通配符主题名 上面列举的就是我认为能够解决 3.1.1 中一些难题的新特性，如果你不想升级到 5.0，也不用担心，我们仍然可以使用上面提到的一些 Workaround 来解决这些问题。 13.7 小结到这里本门课程就结束了，如果你耐着性子学完了这最后一课，那么祝贺你，市场上比你还熟悉 MQTT 协议的应该不多了，你可以自主地设计你的 IoT 应用和平台了。我在物联网行业从业多年，也希望和大家一起交流进步。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"Shell的流程控制","slug":"os/shell_tutorial_3","date":"2020-06-29T05:24:00.000Z","updated":"2022-08-01T06:35:23.812Z","comments":true,"path":"2020/06/29/os/shell_tutorial_3.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/29/os/shell_tutorial_3.html","excerpt":"","text":"1. if 判断1.1 基本语法if [ 条件判断式 ];then 程序 fi 或者 if [ 条件判断式 ] then 程序 elif [ 条件判断式 ] ​ then ​ 程序 else ​ 程序 fi 注意事项： [ 条件判断式 ]，中括号和条件判断式之间必须有空格 if 后要有空格 1.2 案例实操（1）输入一个数字，如果是1，则输出hello world，如果是2，则输出world hello，如果是其它，什么也不输出。 [root@docker-ecs data]# touch if.sh[root@docker-ecs data]# vim if.sh#!/bin/bashif [ $1 -eq &quot;1&quot; ]thenecho &quot;hello world&quot;elif [ $1 -eq &quot;2&quot; ]thenecho &quot;world hello&quot;fi[root@docker-ecs data]# chmod 777 if.sh [root@docker-ecs data]# ./if.sh 2world hello[root@docker-ecs data]# ./if.sh 1hello world 2. case 语句2.1 基本语法case $变量名 in “值1”） 如果变量的值等于值1，则执行程序1 ;; “值2”） 如果变量的值等于值2，则执行程序2 ;; …省略其他分支… *） 如果变量的值都不是以上的值，则执行此程序 ;; esac 注意事项： case行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束。 双分号“**;;**”表示命令序列结束，相当于java中的break。 最后的“*）”表示默认模式，相当于java中的default。 2.2 案例实操（1）输入一个数字，如果是1，则输出hellO，如果是2，则输出world，如果是其它，输出hello world。 [root@docker-ecs data]# touch case.sh[root@docker-ecs data]# vim case.sh#!/bin/bashcase $1 in&quot;1&quot;)echo &quot;hellO&quot;;;&quot;2&quot;)echo &quot;world&quot;;;*)echo &quot;hello world&quot;;;esac[root@docker-ecs data]# chmod 777 case.sh[root@docker-ecs data]# ./case.sh 1hellO[root@docker-ecs data]# ./case.sh 2world[root@docker-ecs data]# ./case.sh 3hello world 3. for 循环3.1 基本语法1​ for (( 初始值;循环控制条件;变量变化 )) do 程序 done 3.2 案例实操（1）从1加到100 [root@docker-ecs data]# touch for1.sh[root@docker-ecs data]# vim for1.sh#!/bin/bashs=0for((i=0;i&lt;=100;i++))dos=$[$s+$i]doneecho $s [root@docker-ecs data]# chmod 777 for1.sh [root@docker-ecs data]# ./for1.sh &quot;5050&quot; 3.3 基本语法2for 变量 in 值1 值2 值3… do 程序 done 3.4 案例实操​ （1）打印所有输入参数 [root@docker-ecs data]# touch for2.sh[root@docker-ecs data]# vim for2.sh#!/bin/bash#打印数字for i in $*doecho &quot;xiao love $i &quot;done[root@docker-ecs data]# chmod 777 for2.sh [root@docker-ecs data]# bash for2.sh cls xz bdxiao love clsxiao love xzxiao love bd （2）比较$*和$@区别 （a）$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数。 [root@docker-ecs data]# touch for.sh [root@docker-ecs data]# vim for.sh #!/bin/bash for i in $* do echo &quot;xiao love $i &quot; done for j in $@ do echo &quot;xiao love $j&quot; done [root@docker-ecs data]# bash for.sh cls xz bd xiao love cls xiao love xz xiao love bd xiao love cls xiao love xz xiao love bd （b）当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“$1 $2 …$n”的形式输出所有参数；“$@”会将各个参数分开，以“$1” “$2”…”$n”的形式输出所有参数。 [root@docker-ecs data]# vim for.sh#!/bin/bash for i in &quot;$*&quot;# $*中的所有参数看成是一个整体，所以这个for循环只会循环一次 do echo &quot;xiao love $i&quot;done for j in &quot;$@&quot;# $@中的每个参数都看成是独立的，所以“$@”中有几个参数，就会循环几次 do echo &quot;xiao love $j&quot; done[root@docker-ecs data]# chmod 777 for.sh[root@docker-ecs data]# bash for.sh cls xz bdxiao love cls xz bdxiao love clsxiao love xzxiao love bd 4. while 循环4.1 基本语法while [ 条件判断式 ] do 程序 done 4.2 案例实操从1加到100 [root@docker-ecs data]# touch while.sh[root@docker-ecs data]# vim while.sh#!/bin/bashs=0i=1while [ $i -le 100 ]dos=$[$s+$i]i=$[$i+1]doneecho $s[root@docker-ecs data]# chmod 777 while.sh [root@docker-ecs data]# ./while.sh 5050 5. read读取控制台输入5.1 基本语法read (选项) (参数) 选项： -p：指定读取值时的提示符； -t：指定读取值时等待的时间（秒）。 参数 变量：指定读取值的变量名 5.2 案例实操提示7秒内，读取控制台输入的名称 [root@docker-ecs data]# touch read.sh[root@docker-ecs data]# vim read.sh#!/bin/bashread -t 7 -p &quot;enter your name in 7 seconds &quot; NAMEecho $NAME[root@docker-ecs data]# ./read.sh enter your name in 7 seconds hellohello","categories":[{"name":"操作系统","slug":"os","permalink":"http://wuzguo.com/blog/categories/os/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://wuzguo.com/blog/tags/shell/"},{"name":"linux","slug":"linux","permalink":"http://wuzguo.com/blog/tags/linux/"},{"name":"bash","slug":"bash","permalink":"http://wuzguo.com/blog/tags/bash/"},{"name":"centos","slug":"centos","permalink":"http://wuzguo.com/blog/tags/centos/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（十二）","slug":"aiot/aiot_tutorial_12","date":"2020-06-29T04:50:00.000Z","updated":"2022-08-01T06:35:23.719Z","comments":true,"path":"2020/06/29/aiot/aiot_tutorial_12.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/29/aiot/aiot_tutorial_12.html","excerpt":"","text":"到目前为止，我们使用的都是一个公有的 Broker，对于学习和演示来说，应该是足够的。但是对于实际生产来说，我们需要有一个私有、可控的 Broker。 正如本课程开头所说，现在很多云服务商都提供了 MQTT Broker 服务，在这里我列举几个较大的： 阿里云的物联网套件 腾讯云的 IoT Hub 青云的 EMQ IoT Hub 百度天工 云服务商的 MQTT Broker 服务是一个很好的选择，接入和配置也很简单，你只需要阅读相应的产品文档，照着步骤一步一步来就可以了。 如果你因为某种原因无法使用公有云的服务，或者你需要可控性、定制性更强的 Broker，你也可以选择自行搭建 MQTT Broker。 这一课我们来学习如何自行搭建 MQTT Broker。本节课核心内容： EMQTT Client 身份验证 传输层安全 12.1 EMQTT首先我想给大家推荐的 Broker 是 EMQTT，它是由国人开发和维护的一款开源 MQTT Broker。推荐它的理由是： 性能和可靠性，EMQTT 是用 Erlang 语言编写的，在电信行业工作的同学可能了解，电信行业里很多核心的应用系统都是用 Erlang 编写的； 纵向扩展能力，在 8 核 32G 的主机上，可以容纳超过 100 万 MQTT Client 接入； 横向扩展能力，支持多机组成集群； 基于插件的功能扩展，官方提供了很多插件来扩展功能和与其他业务系统集成，如果你熟悉 Erlang 语言，也可以自行通过插件的方式扩展； 项目由公司开发和维护，并提供商业服务。 实际上，上面提到的青云的 IoT Hub 就是基于 EMQTT 实现的，我自己在实际生产中也使用 EMQTT 多年了，对它的性能和稳定性是相当认可的。 12.1.1 安装和运行 这里我以 Mac 系统上的安装流程为例，来讲解如何安装和使用 EMQTT Broker。如果你没有使用 Mac 系统也不用担心，除了下载的二进制文件有区别以外，大部分使用方法在类 Unix 系统都是一样的。 安装 Erlang 运行环境 首先我们需要安装 Erlang 运行环境，在 Mac 上的安装方式是使用 Erlang 源代码编译： 下载 Erlang&#x2F;OTP 21.0 源文件包 otp_src_21.0.tar.gz 解压缩 tar -xzf otp_src_21.0.tar.gz cd otp_src_21.0.tar.gz ./configure make sudo make install 安装完毕以后在终端输入 erl，即可进入 Erlang 的交互 Shell。 12.1.1.2 安装 EMQTT 安装 EMQTT 非常简单，下载 Mac 上的二进制包 emqttd-macosx-v2.3.11.zip，之后解压缩： unzip emqttd-macosx-v2.3.11.zip &amp;&amp; cd emqttd 你可以通过 bin/emqttd start 来运行 EMQTT Broker。如果没有问题，你就会看到终端输出： emqttd 2.3.11 is started successfully! 现在 EMQTT Broker 以守护进程的方式运行。 可以查看当前 Broker 状态： bin/emqttd_ctl status 没有问题的话会输出： node &#x27;emq@127.0.0.1&#x27; is startedemqttd 2.3.11 is running 关闭 Broker： bin/emqttd stop 默认情况下，EMQTT Broker 的 MQTT 端口是 1883，WebSocket 的端口是 8083。 12.2 Client 身份验证在前面课程里，我们使用的 Public Broker 没有对 Client 的身份做任何检验，任何 Client 都可以接入。而在实际生产中，我们应该对接入 Client 的身份进行校验，只允许通过校验的 Client 接入。 EMQTT 以插件的方式提供了多种 Client 授权方式，这里我以 PostgresSQL 数据库授权插件（emqx-auth-pgsql）为例，讲解一下如何进行 Client 授权的设置。 PostgresSQL 授权插件的原理很简单。在 PostgresSQL 数据库中建一张用户表来存储用于校验身份的用户名和密码，Broker 使用 Client 在 CONNECT 数据包里指定的用户名和密码，在这张表里面进行匹配，如果匹配通过，则运行接入；否则断开连接，不允许接入。 继续之前请先安装 PostgresSQL。 emqx-auth-pgsql 的配置文件位于 etc&#x2F;plugins&#x2F;emqx-auth-pgsql.conf，以下是需要配置的几项： auth.pgsql.server = 127.0.0.1:5432，PostgresSQL 的 IP 地址和端口，使用默认值； auth.pgsql.username = mqtt，数据库用户名； auth.pgsql.password = mqtt，数据库密码； auth.pgsql.database = mqtt，数据库名称； auth.pgsql.auth_query = select password from mqtt_user where username = &#39;%u&#39; limit 1，匹配时使用的查询 SQL，使用默认值； auth.pgsql.password_hash = plain，mqtt_user 表 password 字段的加密方式，这里为了演示简单起见，设为 plain 不加密，可选的值还有 md5 | sha | sha256 | bcrypt。 然后用 # 注销掉 auth.pgsql.acl_query，这个是控制 Client 对某个主题的 Publish 和 Subscribe 的权限。我们暂时不用这个功能。 接下来我们在 PostgresSQL 中创建相应的数据库： CREATE DATABASE mqtt WITH OWNER = mqtt ENCODING = &#x27;UTF8&#x27; TABLESPACE = pg_default LC_COLLATE = &#x27;C&#x27; LC_CTYPE = &#x27;C&#x27; CONNECTION LIMIT = -1; 然后创建用户表，并插入一条数据： CREATE TABLE mqtt_user( id SERIAL primary key, is_superuser boolean, username character varying(100), password character varying(100));INSERT INTO mqtt_user(is_superuser， username, password) VALUES (false， &#x27;testuser&#x27;, &#x27;123456&#x27;) 加载 emqx-auth-pgsql 插件： bin/emqttd_ctl plugins load emq_auth_pgsql 不出意外的话，会得到这样的输出： Start apps: [emq_auth_pgsql]Plugin emq_auth_pgsql loaded successfully. 接下来我们来测试一下。 运行代码： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://127.0.0.1:1883&#x27;)client.on(&#x27;connect&#x27;, function (connack) &#123; console.log(`return code: $&#123;connack.returnCode&#125;, sessionPresent: $&#123;connack.sessionPresent&#125;`) client.end()&#125;)client.on(&quot;error&quot;, function (error) &#123; console.log(`$&#123;error&#125;`)&#125;) 会得到以下输出： Error: Connection refused: Bad username or password 然后在连接的时候我们把正确的用户名和密码加进去： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://127.0.0.1:1883&#x27;, &#123; username: &#x27;testuser&#x27;, password: &#x27;123456&#x27;&#125;)client.on(&#x27;connect&#x27;, function (connack) &#123; console.log(`return code: $&#123;connack.returnCode&#125;, sessionPresent: $&#123;connack.sessionPresent&#125;`) client.end()&#125;) 那么 Client 可以正确地连接到 Broker，得到以下输入： return code: 0, sessionPresent: false 这样我们就完成了 Client 的身份验证。 12.3 传输层安全到目前为止，本课程中的 MQTT 代码都是使用明文来传输 MQTT 数据包，包括含有 username、password 的 CONNECT 数据包。在实际的生产环境中，这样显然是不安全的。 MQTT 支持传输层加密。通常我们在生产环境中，都需要使用 SSL 来传输 MQTT 数据包，使用传输层加密的 MQTT 被称为 MQTTS（类似于 HTTP 之于 HTTPS）。 我们使用的 Public Broker 支持 MQTTS，你只需要在连接时修改一下 Broker URL，将 “mqtt” 换成 “mqtts” 就可以了： var client = mqtt.connect(&#x27;mqtts://iot.eclipse.org&#x27;） EMQTT 当然也是支持 MQTTS 的，你可以在 etc&#x2F;certs 下配置你的 SSL 证书。EMQTT 也自带了一份自签署的证书，可以开箱即用地在 8883 端口使用 MQTTS。但是因为是自签署的证书，所以你需要关闭客户端的证书验证。 var client = mqtt.connect(&#x27;mqtts://127.0.0.1:8883&#x27;, &#123; rejectUnauthorized: false&#125;） 12.4 小结这一课里我们学习了如何搭建自己的 MQTT Broker，如何进行 Client 身份验证，以及如何使用 MQTTS 加强 Broker 的安全性。EMQTT 还提供了很多其他扩展功能，有兴趣的话可以查看 EMQTT 文档进一步了解。下一课我们来学习 MQTT 的最新版本，MQTT 5.0 的新特性。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"Shell 的基本使用","slug":"os/shell_tutorial_2","date":"2020-06-29T04:14:00.000Z","updated":"2022-08-01T06:35:23.678Z","comments":true,"path":"2020/06/29/os/shell_tutorial_2.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/29/os/shell_tutorial_2.html","excerpt":"","text":"1. 变量1.1 系统变量1.1.1 常用系统变量​ $HOME、$PWD、$SHELL、$USER等 1.1.2 案例实操查看系统变量的值 [root@docker-ecs data]# echo $HOME/root 显示当前Shell中所有变量：set [root@docker-ecs data]# setBASH=/bin/bashBASH_ALIASES=()BASH_ARGC=()BASH_ARGV=()...... 1.2 自定义变量1.2.1 基本语法 定义变量：变量&#x3D;值 撤销变量：unset 变量 声明静态变量：readonly 变量，注意：不能unset 1.2.2 变量定义规则 变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写。 等号两侧不能有空格 在bash中，变量默认类型都是字符串类型，无法直接进行数值运算。 变量的值如果有空格，需要使用双引号或单引号括起来。 1.2.3 案例实操 定义变量A [root@docker-ecs data]# A=5[root@docker-ecs data]# echo $A5 给变量A重新赋值 [root@docker-ecs data]# A=8[root@docker-ecs data]# echo $A8 撤销变量A [root@docker-ecs data]# unset A[root@docker-ecs data]# echo $A 声明静态的变量B&#x3D;2，不能unset [root@docker-ecs data]# readonly B=2[root@docker-ecs data]# echo $B2[root@docker-ecs data]# B=3-bash: B: readonly variable[root@docker-ecs data]# echo $B2[root@docker-ecs data]# unset B-bash: unset: B: cannot unset: readonly variable 在bash中，变量默认类型都是字符串类型，无法直接进行数值运算 [root@docker-ecs data]# C=1+2[root@docker-ecs data]# echo $C1+2 变量的值如果有空格，需要使用双引号或单引号括起来 [root@docker-ecs data]# D=hello world-bash: world: command not found[root@docker-ecs data]# D=&quot;hello world&quot;[root@docker-ecs data]# echo $Dhello world 可把变量提升为全局环境变量，可供其他Shell程序使用 export 变量名 [root@docker-ecs data]$ vim helloworld.sh 在hello.sh文件中增加echo $B #!/bin/bashecho &quot;hello world&quot;echo $B[root@docker-ecs data]# ./hellO.sh Hello world 发现并没有打印输出变量B的值。 [root@docker-ecs data]# export B[root@docker-ecs data]# ./hello.sh hello world2 1.3 特殊变量：$n1.3.1 基本语法​ $n （功能描述：n为数字，$0代表该脚本名称，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如${10}） 1.3.2 案例实操输出该脚本文件名称、输入参数1和输入参数2的值 [root@docker-ecs data]# touch param.sh [root@docker-ecs data]# vim param.sh#!/bin/bashecho &quot;$0 $1 $2&quot;[root@docker-ecs data]# chmod 777 param.sh[root@docker-ecs data]# ./param.sh 1 2./param.sh 1 2 1.4 特殊变量：$#1.4.1 基本语法​ $# （功能描述：获取所有输入参数个数，常用于循环）。 1.4.2 案例实操获取输入参数的个数 [root@docker-ecs data]# vim param.sh#!/bin/bashecho &quot;$0 $1 $2&quot;echo $#[root@docker-ecs data]# chmod 777 param.sh[root@docker-ecs data]# ./param.sh 1 2 3param.sh 1 2 3 1.5 特殊变量：$*、$@1.5.1 基本语法​ $* （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体） ​ $@ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待） 1.5.2 案例实操（1）打印输入的所有参数 [root@docker-ecs data]# vim param.sh#!/bin/bashecho &quot;$0 $1 $2&quot;echo $#echo $*echo $@[root@docker-ecs data]# bash param.sh 1 2 3param.sh 1 231 2 31 2 3 1.6 特殊变量：$？1.6.1 基本语法$？ （功能描述：最后一次执行的命令的返回状态。如果这个变量的值为 0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。） 1.6.2 案例实操判断 hello.sh 脚本是否正确执行 [root@docker-ecs data]# ./hello.sh hello world[root@docker-ecs data]# echo $?0 2. 运算符2.1 基本语法 “$((运算式))”或“$[运算式]” expr + , - , *, &#x2F;, % 加，减，乘，除，取余 注意：expr 运算符间要有空格 2.2 案例实操： 计算3+2的值 [root@docker-ecs data]# expr 1+21+2[root@docker-ecs data]# expr 1 +2expr: syntax error[root@docker-ecs data]# expr 1+ 2expr: syntax error[root@docker-ecs data]# expr 1 + 23 计算3-2的值 [root@docker-ecs data]# expr 3 - 2 1 计算（2+3）X4的值 expr 一步完成计算 [root@docker-ecs data]# expr `expr 2 + 3` \\* 420 采用$[运算式]方式 [root@docker-ecs data]# S=$[(2+3)*4][root@docker-ecs data]# echo $S20[root@docker-ecs data]# expr S = $[(2+3)*5]0[root@docker-ecs data]# echo $S20 3. 条件判断3.1 基本语法[ condition ]（注意condition前后要有空格） 注意：条件非空即为true，[ hello ]返回true，[] 返回false。 3.2 常用判断条件 两个整数之间比较 &#x3D; 字符串比较 -lt 小于（less than） -le 小于等于（less equal） -eq 等于（equal） -gt 大于（greater than） -ge 大于等于（greater equal） -ne 不等于（Not equal） 按照文件权限进行判断 -r 有读的权限（read） -w 有写的权限（write） -x 有执行的权限（execute） 按照文件类型进行判断 -f 文件存在并且是一个常规的文件（file） -e 文件存在（existence） -d 文件存在并是一个目录（directory） 3.3 案例实操 23是否大于等于22 [root@docker-ecs data]# [ 23 -ge 22 ][root@docker-ecs data]# echo $?0 hello.sh 是否具有写权限 [root@docker-ecs data]# [ -w hello.sh ][root@docker-ecs data]# echo $?0 &#x2F;data&#x2F;cls.txt目录中的文件是否存在 [root@docker-ecs data]# [ -e /data/cls.txt ][root@docker-ecs data]# echo $?0[root@docker-ecs data]# [ -e /data/clsw.txt ][root@docker-ecs data]# echo $?1 （多条件判断（&amp;&amp; 表示前一条命令执行成功时，才执行后一条命令，|| 表示上一条命令执行失败后，才执行下一条命令） [root@docker-ecs data]# [ condition ] &amp;&amp; echo OK || echo notokOK[root@docker-ecs data]# [ condition ] &amp;&amp; [ ] || echo notoknotok[root@docker-ecs data]# [ condition ] &amp;&amp; [ ] &amp;&amp; echo OK || echo notoknotok","categories":[{"name":"操作系统","slug":"os","permalink":"http://wuzguo.com/blog/categories/os/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://wuzguo.com/blog/tags/shell/"},{"name":"linux","slug":"linux","permalink":"http://wuzguo.com/blog/tags/linux/"},{"name":"bash","slug":"bash","permalink":"http://wuzguo.com/blog/tags/bash/"},{"name":"centos","slug":"centos","permalink":"http://wuzguo.com/blog/tags/centos/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（十一）","slug":"aiot/aiot_tutorial_11","date":"2020-06-29T03:50:00.000Z","updated":"2022-08-01T06:35:23.796Z","comments":true,"path":"2020/06/29/aiot/aiot_tutorial_11.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/29/aiot/aiot_tutorial_11.html","excerpt":"","text":"在这一课里我们来实现 Web 订阅端。本节课核心内容： MQTT over WebSocket 连接到 Broker 处理消息 11.1 MQTT over WebSocket我们要实现的是一个可以在浏览器里运行的 MQTT Client。MQTT 基于 TCP 协议，在目前主流的浏览器里面，使用 JavaScript 直接打开一个 TCP 连接是不可能的，所以在浏览器里面直接使用 MQTT 目前是没有办法的。 Socket API 可以解决这个问题，但是浏览器对 Socket API 的支持还非常有限。而我们可以应用 MQTT over WebSocket 来在浏览器中使用 MQTT，因为大部分主流浏览器都支持 WebSocket。MQTT over WebSocket 实现原理是把 MQTT 数据包封装在 WebSocket 帧里进行发送： MQTT over WebSocket 也需要 Broker 支持，不过目前大部分 Broker 都是支持的，包括本课程里面使用的 Public Broker。 11.2 连接到 Broker首先需要在 HTML 里面加上支持 MQTT over WebSocket 的 JS 文件： &lt;script src=&quot;https://unpkg.com/mqtt@2.18.6/dist/mqtt.min.js&quot;&gt;&lt;/script&gt; 然后连接到 Broker： var client = mqtt.connect(&quot;ws://iot.eclipse.org/ws&quot;) 注意这里 Broker 的 URL 中的协议部分变成了 “ws”。 在这里我们没有指定 Client Identifier，而 Client 库或 Broker 会为我们自动生成一个 Client Identifier。这样打开多个 Web 订阅端时，就不会发生冲突。但是这样是无法使用持久化会话的，所以在实际项目中，你应该为每一个 Web 订阅端分配一个唯一 Client Identifier，比如把用户 ID 作为 Client Identifier 的一部分。 接下来订阅相关主题： client.subscribe(&quot;front_door/detection/objects&quot;, &#123; qos: 1&#125;, function (err) &#123; if (err != undefined) &#123; console.log(&quot;subscribe failed&quot;) &#125; else &#123; console.log(`subscribe succeeded`) &#125;&#125;) 11.3 处理消息上一课中讲到，在接受消息的时候，我们需要对消息进行去重： var receivedMessages = new Set();client.on(&quot;message&quot;, function (_, payload) &#123; var jsonMessage = JSON.parse(payload.toString()) if (!receivedMessages.has(jsonMessage.id)) &#123; receivedMessages.add(jsonMessage.id) //接下来把结果显示在页面上面 &#125;&#125;) 为了演示简单，这里使用了一个 Set 来保存已收到消息的 ID。实际项目中，可以用稍微复杂一点的数据结构，比如支持 Expiration 的缓存来存储已收到消息的 ID。 然后把接收到的结果在页面上显示出来（这里使用 Table 来显示）： var date = new Date(jsonMessage.timestamp * 1000)$(&#x27;#results tr:last&#x27;).after(`&lt;tr&gt;&lt;td&gt;$&#123;date.toLocaleString()&#125;&lt;/td&gt;&lt;td&gt;$&#123;jsonMessage.objects&#125;&lt;/td&gt;&lt;td&gt;&lt;imgsrc=&quot;$&#123;jsonMessage.image_url&#125;&quot; height=&quot;200&quot;&gt;&lt;/td&gt;&lt;/tr&gt;`); Web 订阅端的最终效果是这样的： 你可以在 https://github.com/sufish/mqtt_browser 找到全部代码。 11.4 小结我们花了两节课完成了一个 IoT+AI 的实战项目，在这个框架下还可以继续扩展新功能，比如将训练好的新模型从云端下发到设备端，以提升识别效率等。有兴趣的读者可以自行扩展。下一课里我们来学习如何搭建自己的 Broker。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（十）","slug":"aiot/aiot_tutorial_10","date":"2020-06-29T02:50:00.000Z","updated":"2022-08-01T06:35:23.849Z","comments":true,"path":"2020/06/29/aiot/aiot_tutorial_10.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/29/aiot/aiot_tutorial_10.html","excerpt":"","text":"在接下来的课程里，我们来完成一个 IoT+AI 的实战项目。本节课核心内容： 如何在 MQTT 里面传输大文件 消息去重 消息数据编码 实现 Android 发布端 发布识别结果 10.1 如何在 MQTT 里面传输大文件我们前面提到过，一个 MQTT 数据包最大可以达到约 256M，所以对于传输图片的需求，最简单直接的方式就把图片数据直接包含在 PUBLISH 包里面进行传输。 还有一种更好的做法。在发布数据之前，先把图片上传到云端的某个图片存储里，然后 PUBLISH 包里面只包含图片的 URL，当订阅端接收这个数据之后，它再通过图片的 URL 来获取图片。这样的做法较前面有几个优点。 对订阅端来说，它可以在有需要的时候再下载图片数据，而第一种做法，每次都必须接收图片的全部数据。 这种做法可以处理文件大于 256M 的情况，而第一种做法，必须把文件分割为多个 PUBLISH 包，订阅端接收后再重新组合，非常麻烦。 节省带宽，如果图片数据直接放在 PUBLISH 包中，那么 Broker 就需要预留相对大的带宽。目前在中国，带宽还是比较贵的。PUBLISH 包中只包含 URL 的话，每一个 PUBLISH 包都很小，那么 Broker 的带宽需求就很小了。虽然上传图片也需要带宽，但是如果你使用云存储，比如阿里云 OSS、七牛等，从它们那里购买上传和下载图片的带宽要便宜很多。同时，这些云存储服务商建设了很多 CDN，通常上传和下载图片比直接通过 PUBLISH 包传输要快一些。 节约存储和处理能力，因为 Broker 需要存储 Client 未接收的消息，所以如果图片包含在 PUBLISH 包里面，Broker 需要预留相当的存储空间；而使用云存储的话，存储的成本比自建要便宜得多。 在这个实战项目里，我们用第二种方式来传输图片，使用七牛作为图片存储。 10.2 消息去重为了兼顾效率和可靠性，我们使用 QoS1 来传输消息。QoS1 有一个问题，就是可能会收到重复的消息，所以需要在应用里面手动对消息进行去重。 我们可以在消息数据里面带一个唯一的消息 ID，通常是 UUID。订阅端需要保存已接收消息的 ID，当收到新消息的时候，通过消息的 ID 来判断是否是重复的消息，如果是，则丢弃。 10.3 消息数据编码我们需要对消息数据进行编码，方便订阅端对数据进行解析。通常可以使用 JSON 格式，如果设备的计算和存储能力有限，也可以使用 Protocol Buffer 这类对内存消耗更少、解析更快的编码方式。 这个项目里面，我们使用 JSON 作为数据编码格式。一个消息数据格式如下： &#123;&#x27;id&#x27;: &lt;消息 ID&gt;, timestamp:&lt;UNIX 时间戳&gt;, image_url: &lt;图片&gt;, objects:[图片中物体名称的数组]&#125; 接下来我们先来实现数据的发布端。 10.4 实现 Android 发布端10.4.1 连接到 Broker 首先包含 MQTT Android 库的依赖： repositories &#123; maven &#123; url &quot;https://repo.eclipse.org/content/repositories/paho-snapshots/&quot; &#125;&#125;dependencies &#123; compile &#x27;org.eclipse.paho:org.eclipse.paho.client.mqttv3:1.1.0&#x27;&#125; 接下来连接到 Broker，注意这里使用 AndriodID 作为 Client Identifier 的一部分，保证 Client Identifier 不会冲突： String clientId = &quot;client_&quot; + Settings.Secure.getString(getApplicationContext().getContentResolver(), Settings.Secure.ANDROID_ID); mqttAsyncClient = new MqttAsyncClient(&quot;tcp://iot.eclipse.org:1883&quot;, clientId, new MqttDefaultFilePersistence(getApplicationContext().getApplicationInfo().dataDir)); mqttAsyncClient.connect(null, new IMqttActionListener() &#123; @Override public void onSuccess(IMqttToken asyncActionToken) &#123; runOnUiThread(new Runnable() &#123; @Override public void run() &#123; Toast.makeText(getApplicationContext(), &quot;已连接到 Broker&quot;, Toast.LENGTH_LONG).show(); &#125; &#125;); &#125; @Override public void onFailure(IMqttToken asyncActionToken, final Throwable exception) &#123; runOnUiThread(new Runnable() &#123; @Override public void run() &#123; Toast.makeText(getApplicationContext(), &quot;连接 Broker 失败:&quot; + exception.getMessage(), Toast.LENGTH_LONG).show(); &#125; &#125;); &#125; &#125;); 10.4.2 上传图片 首先包含七牛安卓库的依赖： compile &#x27;com.qiniu:qiniu-android-sdk:7.3.+&#x27; 然后上传图片： uploadManager.put(getBytesFromBitmap(image), null, upToken, new UpCompletionHandler() &#123; @Override public void complete(String key, ResponseInfo info, JSONObject response) &#123; if(info.isOK())&#123; //上传成功以后 PUBLISH &#125; &#125;&#125;, null); 10.5 发布识别结果最后，在图片上传成功以后，将识别结果发布到 “front_door&#x2F;detection&#x2F;objects” 这个主题： JSONObject jsonMesssage = new JSONObject();jsonMesssage.put(&quot;id&quot;, randomUUID());jsonMesssage.put(&quot;timestamp&quot;, timestamp);jsonMesssage.put(&quot;objects&quot;, objects);jsonMesssage.put(&quot;image_url&quot;, &quot;http://&quot; + QINIU_DOMAIN + &quot;\\\\&quot; + response.getString(&quot;key&quot;)); mqttAsyncClient.publish(&quot;front_door/detection/objects&quot;, new MqttMessage(jsonMesssage.toString().getBytes())); 10.6 小结本节课我们完成了 Android 发布端的代码，你可以在 https://github.com/sufish/object_detection_mqtt 找到全部代码。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（九）","slug":"aiot/aiot_tutorial_9","date":"2020-06-29T01:50:00.000Z","updated":"2022-08-01T06:35:23.714Z","comments":true,"path":"2020/06/29/aiot/aiot_tutorial_9.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/29/aiot/aiot_tutorial_9.html","excerpt":"","text":"这一课我们来学习 MQTT 协议中的 Keep Alive 机制。本节课核心内容： Keep Alive 代码实践 如何在移动端保持 MQTT 连接 9.1 Keep Alive在上一课中，我们提到过 Broker 需要知道 Client 是否非正常地断开了和它的连接，以发送遗愿消息。实际上 Client 也需要能够很快地检测到它失去了和 Broker 的连接，以便重新连接。 MQTT 协议是基于 TCP 的一个应用层协议，理论上 TCP 协议在丢失连接时会通知上层应用，但是 TCP 有一个半打开连接的问题（half-open connection）。这里我不打算深入分析 TCP 协议，需要记住的是，在这种状态下，一端的 TCP 连接已经失效，但是另外一端并不知情，它认为连接依然是打开的，它需要很长的时间才能感知到对端连接已经断开了，这种情况在使用移动或者卫星网络的时候尤为常见。 仅仅依赖 TCP 层的连接状态监测是不够的，于是 MQTT 协议设计了一套 Keep Alive 机制。回忆一下，在建立连接的时候，我们可以传递一个 Keep Alive 参数，它的单位为秒，MQTT 协议中约定：在 1.5*Keep Alive 的时间间隔内，如果 Broker 没有收到来自 Client 的任何数据包，那么 Broker 认为它和 Client 之间的连接已经断开；同样地, 如果 Client 没有收到来自 Broker 的任何数据包，那么 Client 认为它和 Broker 之间的连接已经断开。 MQTT 还有一对 PINGREQ&#x2F;PINGRESP 数据包，当 Broker 和 Client 之间没有任何数据包传输的时候，可以通过 PINGREQ&#x2F;PINGRESP 来满足 Keep Alive 的约定和侦测连接状态。 9.1.1 PINGREQPINGREQ 数据包没有可变头（Variable header）和消息体（Payload），当 Client 在一个 Keep Alive 时间间隔内没有向 Broker 发送任何数据包，比如 PUBLISH 和 SUBSCRIBE 的时候，它应该向 Broker 发送 PINGREQ 数据包。 9.1.2 PINGRESPPINGRESP 数据包没有可变头（Variable header）和消息体（Payload），当 Broker 收到来自 Client 的 PINGREQ 数据包，它应该回复 Client 一个 PINGRESP 数据包。 对于 Keep Alive 机制，我们还需要记住以下几点： 如果在一个 Keep Alive 时间间隔内，Client 和 Broker 有过数据包传输，比如 PUBLISH，Client 就没有必要再使用 PINGREQ 了，在网络资源比较紧张的情况下这点很重要； Keep Alive 值是由 Client 指定的，不同的 Client 可以指定不同的值； Keep Alive 的最大值为 18 小时 12 分 15 秒； Keep Alive 值如果设为 0 的话，代表不使用 Keep Alive 机制。 9.2 代码实践我们首先来完成一个 Client 的代码， 它会把发送和收到的 PINGREQ&#x2F;PINGRESP 打印出来。 完整代码 keepalive.js 如下： var mqtt = require(&#x27;mqtt&#x27;)var dateTime = require(&#x27;node-datetime&#x27;);var client = mqtt.connect(&#x27;mqtt://iot.eclipse.org&#x27;, &#123; clientId: &quot;mqtt_sample_id_chapter_9&quot;, clean: false, keepalive: 5&#125;)client.on(&#x27;connect&#x27;, function () &#123; client.on(&#x27;packetsend&#x27;, function (packet) &#123; console.log(`$&#123;dateTime.create().format(&#x27;H:M:S&#x27;)&#125;: send $&#123;packet.cmd&#125;`) &#125;) client.on(&#x27;packetreceive&#x27;, function (packet) &#123; console.log(`$&#123;dateTime.create().format(&#x27;H:M:S&#x27;)&#125;: receive $&#123;packet.cmd&#125;`) &#125;)&#125;) 这里为了演示起见，把 Keep Alive 的时间间隔设为了 5 秒。 运行 node keepalive.js，会得到以下输出： 19:42:44: send pingreq19:42:44: receive pingresp19:42:49: send pingreq19:42:49: receive pingresp19:42:54: send pingreq19:42:54: receive pingresp......... 可以看到，每隔 5 秒就会有一个 PINGREQ&#x2F;PINGRESP 的交互。 接下来 Client 每隔 4 秒钟发送一个 PUBLISH 数据包，我们来看看是否还会触发 PINGREQ&#x2F;PINGRESP。 完整的代码 keepalive_with_publish.js 如下： var mqtt = require(&#x27;mqtt&#x27;)var dateTime = require(&#x27;node-datetime&#x27;);var client = mqtt.connect(&#x27;mqtt://iot.eclipse.org&#x27;, &#123; clientId: &quot;mqtt_sample_id_chapter_9&quot;, clean: false, keepalive: 5&#125;)client.on(&#x27;connect&#x27;, function () &#123; client.on(&#x27;packetsend&#x27;, function (packet) &#123; console.log(`$&#123;dateTime.create().format(&#x27;H:M:S&#x27;)&#125;: send $&#123;packet.cmd&#125;`) &#125;) client.on(&#x27;packetreceive&#x27;, function (packet) &#123; console.log(`$&#123;dateTime.create().format(&#x27;H:M:S&#x27;)&#125;: receive $&#123;packet.cmd&#125;`) &#125;) setInterval(function () &#123; client.publish(&quot;foo/bar&quot;, &quot;test&quot;) &#125;, 4 * 1000)&#125;) 运行 node chapter-9/keepalive_with_publish.js，会得到以下输出： 19:54:37: send publish19:54:41: send publish19:54:45: send publish...... 正如之前讲的，如果在一个 Keep Alive 的时间间隔内，Client 和 Broker 之间有传输过数据包，那么就不会触发 PINGREQ&#x2F;PINGRESP。 9.3 如何在移动端保持 MQTT 连接通常在移动端使用 MQTT 的时候，都会碰到一个问题：App 被切入后台后，怎么才能保持 MQTT 的连接并继续接收消息呢？接下来我们就 Android 和 iOS 分别来讲一下。 9.3.1 Android在 Android 上，我们可以在一个 Service 中创建和保持 MQTT 连接，这样即使 App 被切入后台了，这个 Service 还在运行，那么 MQTT 的连接还存在，也能接收消息。类似如下的代码： public class MQTTService extends Service&#123; ........ @Override public int onStartCommand(Intent intent, int flags, int startId) &#123; ....... mqttClient.connect(...) ....... &#125; .......&#125; 接收到消息后，我们可以通过一些方式，比如广播，来通知 App 来处理这些消息。 9.3.2 iOSiOS 的机制与 Android 不同，在 App 被切入后台时，你没有办法在后台运行 App 的任何代码（当然，iOS 提供了几种可以后台运行的类型，比如 Download、Audio 等，但是如果你的 App 假借这些方式来运行后台程序，是过不了审核的，所以这里只讨论正常的情况），所以无法通过 MQTT 的连接来获取消息。 在 iOS 上面，App 切入后台以后，正确接收 MQTT 消息的方式是： Publisher 发布一条或者多条消息； Publisher 通过某种渠道（比如 HTTP API）告知 App 的应用服务器，然后服务器通过苹果的 APNs 向对应的 iOS 订阅者推送一条消息； 用户点击推送，App 进入前台； App 重新建立和 Broker 的连接； App 收到 Publisher 刚刚发送的一条或多条消息。 App 端的代码类似如下： -(void)application:(UIApplication *)app didReceiveRemoteNotification:(NSDictionary *)userInfo &#123; if([app applicationState] == UIApplicationStateInactive) &#123; [mqttClient connect] &#125;&#125;... 注意： 实际上，当下国内主流的 Android 系统都有后台清理功能， App 切入后台以后，它的服务，即使是前台服务（Foreground Service）也会很快地被杀掉，除非 App 被厂商或者用户加入白名单。所以在 Android 上最好还是利用厂商的推送通道，比如华为推送、小米推送等，在 App 被切入后台时采用和 iOS 上一样的机制来接收 MQTT 的消息。 9.4 小结到此为止我们学习完了 MQTT 协议及其所有特性，你可在 https://github.com/sufish/mqtt-nodejs-sample 找到所有的示例代码，接下来我们进行一个 IoT+AI 的实战。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"Shell 入门","slug":"os/shell_tutorial_1","date":"2020-06-29T01:25:00.000Z","updated":"2022-08-01T06:35:23.622Z","comments":true,"path":"2020/06/29/os/shell_tutorial_1.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/29/os/shell_tutorial_1.html","excerpt":"","text":"1. 概述Shell 是一个命令行解析器，它接受应用程序&#x2F;用户命令，然后调用操作系统的内核。它还是一个功能强大的编程语言，具有易编写，易调试，灵活性强的特点。 2. 解析器2.1 Linux提供的Shell解析器[root@docker-ecs /]# cat /etc/shells/bin/sh/bin/bash/sbin/nologin/bin/dash[logviewer@ip-10-20-1-182 ~]# ll | grep bash 2.2 bash和sh的关系[root@docker-ecs bin]# ll | grep bash-rwxr-xr-x. 1 root root 906568 Mar 22 2017 bashlrwxrwxrwx. 1 root root 4 Jul 4 2018 sh -&gt; bash 2.3 Centos的默认解析器[root@docker-ecs /]# echo $SHELL/bin/bash 3. 第一个脚本3.1 脚本格式​ 脚本以#!&#x2F;bin&#x2F;bash开头，指定解析器 3.2 第一个Shell脚本[root@docker-ecs data]# touch hello.sh[root@docker-ecs data]# vi hello.sh 在 hello.sh 中输入如下内容 #!/bin/bashecho &quot;hello, world !&quot; 3.3 脚本的常用执行方式3.3.1 采用bash或sh+脚本的相对路径或绝对路径（不用赋予脚本+x权限）sh+脚本的相对路径 [root@docker-ecs data]# sh hello.shhello, world ! sh+脚本的绝对路径 [root@docker-ecs data]# sh /data/hello.sh hello, world ! bash+脚本的相对路径 [root@docker-ecs data]# bash hello.sh hello, world ! bash+脚本的绝对路径 [root@docker-ecs data]# bash /data/hello.sh hello, world ! 3.3.2 采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）首先要赋予 hello.sh 脚本的+x权限 [root@docker-ecs data]# chmod 777 hello.sh 执行脚本 相对路径 [root@docker-ecs data]# ./hello.sh hello, world ! 绝对路径 [root@docker-ecs data]# /data/hello.sh hello, world ! 注意：第一种执行方法，本质是bash解析器帮你执行脚本，所以脚本本身不需要执行权限。第二种执行方法，本质是脚本需要自己执行，所以需要执行权限。 3.4 第二个Shell脚本（多命令处理）需求：在 &#x2F;data&#x2F; 目录下创建一个 cls.txt 并在文件中增加 “ I love cls”。 案例实操： [root@docker-ecs data]# touch batch.sh[root@docker-ecs data]# vi batch.sh 在 batch.sh 中输入如下内容 #!/bin/bashcd /datatouch cls.txtecho &quot;I love cls&quot; &gt;&gt; cls.txt 执行脚本，查看结果 [root@docker-ecs data]# cat cls.txt i love cls","categories":[{"name":"操作系统","slug":"os","permalink":"http://wuzguo.com/blog/categories/os/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://wuzguo.com/blog/tags/shell/"},{"name":"linux","slug":"linux","permalink":"http://wuzguo.com/blog/tags/linux/"},{"name":"bash","slug":"bash","permalink":"http://wuzguo.com/blog/tags/bash/"},{"name":"centos","slug":"centos","permalink":"http://wuzguo.com/blog/tags/centos/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（八）","slug":"aiot/aiot_tutorial_8","date":"2020-06-28T00:50:00.000Z","updated":"2022-08-01T06:35:23.839Z","comments":true,"path":"2020/06/28/aiot/aiot_tutorial_8.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/28/aiot/aiot_tutorial_8.html","excerpt":"","text":"在这一课里面我们来学习一下 Retained 消息和 LWT。本节课核心内容： Retained 消息 代码实践：发布和接收 Retained 消息 LWT 代码实践：监控 Client 连接状态 8.1 Retained 消息让我们来看一下这个场景： 你有一个温度传感器，它每三个小时向一个 Topic 发布当前的温度。那么问题来了，有一个新的订阅者在它刚刚发布了当前温度之后订阅了这个主题，那么这个订阅端什么时候能才能收到温度消息？ 对的，它必须等到三个小时以后，温度传感器再次发布消息的时候才能收到。在这之前，这个新的订阅者对传感器的温度数据一无所知。 怎么来解决这个问题呢？ 这个时候就轮到 Retained 消息出场解决这个问题了。Retained 消息是指在 PUBLISH 数据包中 Retain 标识设为 1 的消息，Broker 收到这样的 PUBLISH 包以后，将保存这个消息，当有一个新的订阅者订阅相应主题的时候，Broker 会马上将这个消息发送给订阅者。 Retain 消息有以下一些特点： 一个 Topic 只能有 1 条 Retained 消息，发布新的 Retained 消息将覆盖老的 Retained 消息； 如果订阅者使用通配符订阅主题，它会收到所有匹配的主题上的 Retained 消息； 只有新的订阅者才会收到 Retained 消息，如果订阅者重复订阅一个主题，也会被当做新的订阅者，然后收到 Retained 消息； Retained 消息发送到订阅者时，消息的 Retain 标识仍然是 1，订阅者可以判断这个消息是否是 Retained 消息，以做相应的处理。 注意：Retained 消息和持久性会话没有任何关系，Retained 消息是 Broker 为每一个 Topic 单独存储的，而持久性会话是 Broker 为每一个 Client 单独存储的。 如果你想删除一个 Retained 消息也很简单，只要向这个主题发布一个 Payload 长度为 0 的 Retained 消息就可以了。 那么开头我们提到的那个场景的解决方案就很简单了，温度传感器每 3 个小时发布当前的温度的 Retained 消息，那么无论新的订阅者什么时候进行订阅，它都能收到温度传感器上一次发布的数据。 8.2 代码实践：发布和接收 Retained 消息在这里我们编写一个发布 Retained 消息的发布者，和一个接受消息的订阅端，订阅端在接收消息的时候将消息的 Retain 标识和内容打印出来。 完整的代码 publish_retained.js 如下： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;, &#123; clientId: &quot;mqtt_sample_publisher_1&quot;, clean: false&#125;)client.on(&#x27;connect&#x27;, function (connack) &#123; if(connack.returnCode == 0)&#123; client.publish(&quot;home/2ndfloor/201/temperature&quot;, JSON.stringify(&#123;current: 25&#125;), &#123;qos: 0, retain: 1&#125;, function (err) &#123; if(err == undefined) &#123; console.log(&quot;Publish finished&quot;) client.end() &#125;else&#123; console.log(&quot;Publish failed&quot;) &#125; &#125;) &#125;else&#123; console.log(`Connection failed: $&#123;connack.returnCode&#125;`) &#125;&#125;) 完整的代码 subscribe_retained.js 如下： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;, &#123; clientId: &quot;mqtt_sample_subscriber_id_chapter_8&quot;, clean: false&#125;)client.on(&#x27;connect&#x27;, function (connack) &#123; if(connack.returnCode == 0) &#123; if (connack.sessionPresent == false) &#123; console.log(&quot;subscribing&quot;) client.subscribe(&quot;home/2ndfloor/201/temperature&quot;, &#123; qos: 0 &#125;, function (err, granted) &#123; if (err != undefined) &#123; console.log(&quot;subscribe failed&quot;) &#125; else &#123; console.log(`subscribe succeeded with $&#123;granted[0].topic&#125;, qos: $&#123;granted[0].qos&#125;`) &#125; &#125;) &#125; &#125;else &#123; console.log(`Connection failed: $&#123;connack.returnCode&#125;`) &#125;&#125;)client.on(&quot;message&quot;, function (_, message, packet) &#123; var jsonPayload = JSON.parse(message.toString()) console.log(`retained: $&#123;packet.retain&#125;, temperature: $&#123;jsonPayload.current&#125;`)&#125;) 我们首先运行 node publish_retained.js，再运行 node subscribe_retained.js，会得到以下输出： retained: true, temperature: 25 可见我们在 Publisher 发布之后再订阅主题也能收到 Retained 消息。 然后我们再运行一次 node publish_retained.js，在运行 subscribe_retained.js 的终端会有以下输出： retained: false, temperature: 25 Broker 收到 Retained 消息以后，只是保存这个消息，然后按照正常的转发逻辑转发给订阅者，所以对订阅者来说，这个是一个普通的 MQTT 消息，所以 Retain 标识为 0。 然后 Ctrl+C 关闭掉 subscribe_retained.js，然后重新运行，终端不会有任何输出，可见 Retained 消息只对新订阅的订阅者有效。 8.3 LWTLWT 全称为 Last Will and Testament，也就是我们在连接到 Broker 时提到的遗愿，包括遗愿主题、遗愿 QoS、遗愿消息等。 顾名思义，当 Broker 检测到 Client 非正常地断开连接的时候，就会向遗愿主题里面发布一条消息。遗愿相关的设置是在建立连接的时候，在 CONNECT 数据包里面指定的。 Will Flag：是否使用 LWT Will Topic：遗愿主题名，不可使用通配符 Will Qos：发布遗愿消息时使用的 QoS Will Retain：遗愿消息的 Retain 标识 Will Message：遗愿消息内容 Broker 在以下情况下认为 Client 是非正常断开连接的： Broker 检测到底层的 I&#x2F;O 异常； Client 未能在 Keep Alive 的间隔内和 Broker 之间有消息交互； Client 在关闭底层 TCP 连接前没有发送 DISCONNECT 数据包； Broker 因为协议错误关闭和 Client 的连接，比如 Client 发送了一个格式错误的 MQTT 数据包。 如果 Client 通过发布 DISCONNECT 数据包断开连接，这个属于正常断开连接，不会触发 LWT 的机制，同时，Broker 还会丢弃掉这个 Client 在连接时指定的 LWT 参数。 通常，如果我们关心一个设备，比如传感器的连接状态，可以使用 LWT。在接下来的代码实践里面，我们会使用 LWT 和 Retained 消息来实现对一个 Client 的连接状态监控。 8.4 代码实践：监控 Client 连接状态实现 Client 连接状态监控的原理很简单： Client 在连接的时候指定 Will Topic 为“client&#x2F;status”，遗愿消息为“offline”，Will Retain&#x3D;1； Client 在连接成功以后向同一个主题“client&#x2F;status”，发布一个内容为“online”的 Retained 消息。 那么订阅者在任何时候订阅“client&#x2F;status”，都会获取 Client 当前的连接状态。 client.js 代码如下： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://iot.eclipse.org&#x27;, &#123; clientId: &quot;mqtt_sample_publisher_chapter_8&quot;, clean: false, will:&#123; topic : &#x27;client/status&#x27;, qos: 1, retain: true, payload: JSON.stringify(&#123;status: &#x27;offline&#x27;&#125;) &#125;&#125;)client.on(&#x27;connect&#x27;, function (connack) &#123; if(connack.returnCode == 0)&#123; client.publish(&quot;client/status&quot;, JSON.stringify(&#123;status: &#x27;online&#x27;&#125;), &#123;qos: 1, retain: 1&#125;) &#125;else&#123; console.log(`Connection failed: $&#123;connack.returnCode&#125;`) &#125;&#125;) monitor.js 代码如下： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://iot.eclipse.org&#x27;, &#123; clientId: &quot;mqtt_sample_subscriber_id_chapter_8_2&quot;, clean: false&#125;)client.on(&#x27;connect&#x27;, function () &#123; client.subscribe(&quot;client/status&quot;, &#123;qos: 1&#125;)&#125;)client.on(&quot;message&quot;, function (_, message) &#123; var jsonPayload = JSON.parse(message.toString()) console.log(`client is $&#123;jsonPayload.status&#125;`)&#125;) 在 monitor.js 中，我们每次连接的时候都重新订阅 “client&#x2F;status”，这样的话每次运行都能收到关于 Client 连接状态的 Retained 消息。 首先运行 node client.js，然后运行 node monitor.js，会得到以下输出： client is online 在运行 client.js 的终端上，使用 Ctrl+C 终止 client.js，之后在运行 monitor.js 的终端上会得到以下输出： client is offline 然后重新运行 node client.js，在运行 monitor.js 的终端上会得到以下输出： client is offline Ctrl+C 终止 monitor.js，然后重新运行 node monitor.js，会得到以下输出： client is offline 这样我们就完美地监控了 Client 的连接状态。 8.5 小结在这一课我们学习了 Retained 消息和 LWT，并利用这两个特性完成了对 Client 连接状态进行监控的功能，下一课我们将学习 Keep Alive 和在移动端的连接保活。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（七）","slug":"aiot/aiot_tutorial_7","date":"2020-06-27T23:50:00.000Z","updated":"2022-08-01T06:35:23.704Z","comments":true,"path":"2020/06/28/aiot/aiot_tutorial_7.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/28/aiot/aiot_tutorial_7.html","excerpt":"","text":"QoS0 和 QoS1 是相对简单的 QoS 等级，QoS2 不仅要确保 Receiver 能收到 Sender 发送的消息，还要保证消息不重复。它的重传和应答机制就要复杂一些，同时开销也是最大的。下面就让我们来看一下 QoS2 的机制。本节课核心内容： QOS2 QoS 和会话（Session） 如何选择 QoS 7.1 QOS2在 QoS2 下，一条消息的传递流程如下： QoS 使用 2 套请求&#x2F;应答流程（一个 4 段的握手）来确保 Receiver 收到来自 Sender 的消息，且不重复： Sender 发送 QoS 为 2 的 PUBLISH 数据包，数据包 Packet Identifier 为 P，并在本地保存该 PUBLISH 包； Receiver 收到 PUBLISH 数据包以后，在本地保存 PUBLISH 包的 Packet Identifier P，并回复 Sender 一个 PUBREC 数据包，PUBREC 数据包可变头中的 Packet Identifier 为 P，没有消息体（Payload）； 当 Sender 收到 PUBREC，它就可以安全地丢弃掉初始的 Packet Identifier 为 P 的 PUBLISH 数据包，同时保存该 PUBREC 数据包，同时回复 Receiver 一个 PUBREL 数据包，PUBREL 数据包可变头中的 Packet Identifier 为 P，没有消息体；如果 Sender 在一定时间内没有收到 PUBREC，它会把 PUBLISH 包的 DUP 标识设为 1，重新发送该 PUBLISH 数据包（Payload）； 当 Receiver 收到 PUBREL 数据包，它可以丢弃掉保存的 PUBLISH 包的 Packet Identifier P，并回复 Sender 一个 PUBCOMP 数据包，PUBCOMP 数据包可变头中的 Packet Identifier 为 P，没有消息体（Payload）； 当 Sender 收到 PUBCOMP 包，那么它认为数据包传输已完成，它会丢弃掉对应的 PUBREC 包。如果 Sender 在一定时间内没有收到 PUBCOMP 包，它会重新发送 PUBREL 数据包。 我们可以看到在 QoS2 中，完成一次消息的传递，Sender 和 Reciever 之间至少要发送四个数据包，QoS2 是最安全也是最慢的一种 QoS 等级了。 我们可以运行上一课中的 publish_with_qos.js 和 subscribe_with_qos.js 来验证一下： node publish_with_qos.js --qos=2 输出为： send: publishreceive: pubrecsend: pubrelreceive: pubcomp node subscribe_with_qos.js --qos=2 输出为： receive: publishsend: pubrecreceive: pubrelsend: pubcomp 当然，和上一课讲的一样，如果 Publish QoS 为 1，Subscribe QoS 为 2，或者 Publish QoS 为 2，Subscribe QoS 为 1，那么实际 Subscribe 接收消息的 QoS 仍然为 1。 7.2 QoS 和会话（Session）如果 Client 想接收离线消息，必须使用持久化的会话（Clean Session &#x3D; 0）连接到 Broker，这样 Broker 才会存储 Client 在离线期间没有确认接收的 QoS 大于 1 的消息。 7.3 如何选择 QoS在以下情况下你可以选择 QoS0： Client 和 Broker 之间的网络连接非常稳定，例如一个通过有线网络连接到 Broker 的测试用 Client； 可以接受丢失部分消息，比如你有一个传感器以非常短的间隔发布状态数据，所以丢一些也可以接受； 你不需要离线消息。 在以下情况下你应该选择 QoS1： 你需要接收所有的消息，而且你的应用可以接受并处理重复的消息； 你无法接受 QoS2 带来的额外开销，QoS1 发送消息的速度比 QoS2 快很多。 在以下情况下你应该选择 QoS2： 你的应用必须接收到所有的消息，而且你的应用在重复的消息下无法正常工作，同时你也能接受 QoS2 带来的额外开销。 实际上，QoS1 是应用最广泛的 QoS 等级，QoS1 发送消息的速度很快，而且能够保证消息的可靠性。虽然使用 QoS1 可能会收到重复的消息，但是在应用程序里面处理重复消息，通常并不是件难事。在实战的课程里面我们会看到如何在应用里对消息去重。 7.4 小结在本节课里面我们学习了 QoS2 消息传递的流程，以及选择 QoS 的最佳实践，下一课我们将学习 Retained Message 和 LWT（Last Will and Testament）。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（六）","slug":"aiot/aiot_tutorial_6","date":"2020-06-27T22:50:00.000Z","updated":"2022-08-01T06:35:23.617Z","comments":true,"path":"2020/06/28/aiot/aiot_tutorial_6.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/28/aiot/aiot_tutorial_6.html","excerpt":"","text":"在前面的课程中我们多次提到了 QoS（Quality of Service）的概念，CONNECT、PUBLISH、SUBSCRIBE 中都有 QoS 的标识，那么 MQTT 提供的 QoS 是什么呢？本节课核心内容： MQTT 中的 QoS 等级 QoS0 QoS1 代码实践 6.1 MQTT 中的 QoS 等级作为最初用来在网络带宽窄、信号不稳定的环境下传输数据的协议，MQTT 设计了一套保证消息稳定传输的机制，包括消息应答、存储和重传。在这套机制下，提供了三种不同层次 QoS： QoS0，At most once，至多一次； QoS1，At least once，至少一次； QoS2，Exactly once，确保只有一次。 什么意思呢，QoS 是消息的发送方（Sender）和接受方（Receiver）之间达成的一个协议： QoS0 代表，Sender 发送的一条消息，Receiver 最多能收到一次，也就是说 Sender 尽力向 Receiver 发送消息，如果发送失败，也就算了； QoS1 代表，Sender 发送的一条消息，Receiver 至少能收到一次，也就是说 Sender 向 Receiver 发送消息，如果发送失败，会继续重试，直到 Receiver 收到消息为止，但是因为重传的原因，Receiver 有可能会收到重复的消息； QoS2 代表，Sender 发送的一条消息，Receiver 确保能收到而且只收到一次，也就是说 Sender 尽力向 Receiver 发送消息，如果发送失败，会继续重试，直到 Receiver 收到消息为止，同时保证 Receiver 不会因为消息重传而收到重复的消息。 要注意的是，QoS 是 Sender 和 Receiver 之间达成的协议，不是 Publisher 和 Subscriber 之间达成的协议。也就是说 Publisher 发布一条 QoS1 的消息，只能保证 Broker 能至少收到一次这个消息；至于对应的 Subscriber 能否至少收到一次这个消息，还要取决于 Subscriber 在 Subscribe 的时候和 Broker 协商的 QoS 等级。 接下来我们来看一下 QoS0 和 QoS1 的机制，并讨论一下什么是 QoS 降级。 6.2 QoS0QoS0 是最简单的一个 QoS 等级了，在这个 QoS 等级下，Sender 和 Receiver 之间一次消息的传递流程如下： Sender 向 Receiver 发送一个包含消息数据的 PUBLISH 包，然后不管结果如何，丢弃掉已发送的 PUBLISH 包，一条消息的发送完成。 6.3 QoS1QoS 要保证消息至少到达 Sender 一次，所以有一个应答的机制，在 Qos1 等级下的 Sender 和 Receiver 的一次消息的传递流程如下。 Sender 向 Receiver 发送一个带有消息数据的 PUBLISH 包， 并在本地保存这个 PUBLISH 包。 Receiver 收到 PUBLISH 包以后，向 Sender 发送一个 PUBACK 数据包，PUBACK 数据包没有消息体（Payload），在可变头中（Variable header）中有一个包标识（Packet Identifier），和它收到的 PUBLISH 包中的 Packet Identifier 一致。 Sender 收到 PUBACK 之后，根据 PUBACK 包中的 Packet Identifier 找到本地保存的 PUBLISH 包，然后丢弃掉，一次消息的发送完成。 如果 Sender 在一段时间内没有收到 PUBLISH 包对应的 PUBACK，它将该 PUBLISH 包的 DUP 标识设为 1（代表是重新发送的 PUBLISH 包），然后重新发送该 PUBLISH 包。重复这个流程，直到收到 PUBACK，然后执行第 3 步。 6.4 代码实践这里我们实现一个发布端和一个订阅端，可以通过命令行参数来指定发布和订阅的 QoS，同时，通过捕获“packetsend”和“packetreceive”事件，将发送和接受到的 MQTT 数据包的类型打印出来。 完整的代码 publish_with_qos.js 如下： var args = require(&#x27;yargs&#x27;).argv;var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;, &#123; clientId: &quot;mqtt_sample_publisher_2&quot;, clean: false&#125;)client.on(&#x27;connect&#x27;, function (connack) &#123; if (connack.returnCode == 0) &#123; client.on(&#x27;packetsend&#x27;, function (packet) &#123; console.log(`send: $&#123;packet.cmd&#125;`) &#125;) client.on(&#x27;packetreceive&#x27;, function (packet) &#123; console.log(`receive: $&#123;packet.cmd&#125;`) &#125;) client.publish(&quot;home/sample_topic&quot;, JSON.stringify(&#123;data: &#x27;test&#x27;&#125;), &#123;qos: args.qos&#125;) &#125; else &#123; console.log(`Connection failed: $&#123;connack.returnCode&#125;`) &#125;&#125;) 完整的代码 subscribe_with_qos.js 如下： var args = require(&#x27;yargs&#x27;).argv;var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;, &#123; clientId: &quot;mqtt_sample_subscriber_id_2&quot;, clean: false&#125;)client.on(&#x27;connect&#x27;, function (connack) &#123; if (connack.returnCode == 0) &#123; client.subscribe(&quot;home/sample_topic&quot;, &#123;qos: args.qos&#125;, function () &#123; client.on(&#x27;packetsend&#x27;, function (packet) &#123; console.log(`send: $&#123;packet.cmd&#125;`) &#125;) client.on(&#x27;packetreceive&#x27;, function (packet) &#123; console.log(`receive: $&#123;packet.cmd&#125;`) &#125;) &#125;) &#125; else &#123; console.log(`Connection failed: $&#123;connack.returnCode&#125;`) &#125;&#125;) 在 subscribe_with_qos.js 中， Client 每次连接到 Broker 之后都会按照参数指定的 QoS 重新订阅主题，订阅成功以后才开始捕获接收和发送的数据包，所以 Client 在连接之后，重新订阅之前收到的离线消息不会被打印出来。 我们可以通过 node publish_with_qos.js –qos&#x3D;xxx 和 node subscribe_with_qos.js –qos&#x3D;xxx 来运行这两个 JS 程序。 接下来我们用 4 种参数组合来运行这两 JS 程序，看看输出分别是什么。 注意：需要先运行 subscribe_with_qos.js 再运行 publish_with_qos.js，确保接收到消息可以打印出来。 6.4.1 发布使用 QoS0，订阅使用 QoS0node publish_with_qos.js –qos&#x3D;0 输出为： send: publish node subscribe_with_qos.js –qos&#x3D;0 输出为： receive: publish 结果显而易见，Publisher 到 Broker，Broker 到 Subscriber 都是用的 QoS0。 6.4.2 发布使用 QoS1，订阅使用 QoS1node publish_with_qos.js –qos&#x3D;1 输出为： send: publishreceive: puback node subscribe_with_qos.js –qos&#x3D;1 输出为： receive: publishsend: puback 同样地，结果显而易见，Publisher 到 Broker，Broker 到 Subscriber 都是用的 QoS1。 6.4.3 发布使用 QoS0，订阅使用 QoS1node publish_with_qos.js –qos&#x3D;0 输出为： send: publish node subscribe_with_qos.js –qos&#x3D;1 输出为： receive: publish 这里就有点奇怪了， 很明显 Broker 到 Subscriber 这段使用的是 QoS0，和 Subscriber 订阅时指定的 QoS 不一样。原因我们后面来讲。 6.4.4 发布使用 QoS1，订阅使用 QoS0 node publish_with_qos.js –qos&#x3D;1 输出为： send: publishreceive: puback node subscribe_with_qos.js –qos&#x3D;0 输出为： receive: publish 和设定的一样， Publisher 到 Broker 使用 QoS1，Broker 到 Subscriber 使用的 QoS0。Publisher 使用 QoS1 发布消息，但是消息到 Subscriber 却是 QoS0。也就是说有可能无法收到消息，这种现象叫做 QoS 的降级（QoS Degrade）。 这里有一个很重要的计算方法，在 MQTT 协议中，从 Broker 到 Subscriber 这段消息传递的实际 QoS 等于：Publisher 发布消息时指定的 QoS 等级和 Subscriber 在订阅时与 Broker 协商的 QoS 等级，这两个 QoS 等级中的最小那一个。 *Actual Subscribe QoS &#x3D; MIN(Publish QoS, Subscribe QoS)* 这也就解释了“publish qos&#x3D;0, subscribe qos&#x3D;1”的情况下 Subscriber 的实际 QoS 为 0，以及“publish qos&#x3D;1, subscribe qos&#x3D;0”时出现 QoS 降级的原因。 理解了实际 Subscriber QoS 的计算方法，你才能很好地设计你系统里面 Publisher 和 Subscriber 使用的 QoS。例如，如果你希望 Subscriber 至少收到一次 Publisher 的消息，那么你要确保 Publisher 和 Subscriber 都使用不小于 1 的 QoS 等级。 6.5 小结在这一课里面，我们学习了相对比较简单的两种 QoS 等级，同时学习了实际 QoS 的计算方法，接下来我们学习相对复杂的 QoS2 以及 QoS 的最佳实践。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（五）","slug":"aiot/aiot_tutorial_5","date":"2020-06-27T21:50:00.000Z","updated":"2022-08-01T06:35:23.645Z","comments":true,"path":"2020/06/28/aiot/aiot_tutorial_5.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/28/aiot/aiot_tutorial_5.html","excerpt":"","text":"接下来我们来学习如何订阅一个主题，并接收消息。本节课核心内容： 订阅 取消订阅 代码实践 5.1 订阅订阅主题的流程如下： Client 向 Broker 发送一个 SUBSCRIBE 数据包，其中包含了 Client 想要订阅的主题以及其他一些参数； Broker 收到 SUBSCRIBE 数据包后，向 Client 发送一个 SUBACK 数据包作为应答。 接下来我们看数据包的具体内容。 5.1.1 SUBSCRIBE5.1.1.1 可变头（Variable header）数据包标识（Packet Identifier）：两个字节，用来唯一标识一个数据包，数据包标识只需要保证在从 Sender 到 Receiver 的一次消息交互中保持唯一。 5.1.1.2 消息体（Payload）订阅列表（List of Subscriptions）：SUBSCRIBE 的消息体中包含 Client 想要订阅的主题列表，列表中的每一项由订阅主题名和对应的 QoS 组成。主题名中可以包含通配符，单层通配符“+”和多层通配符“#”。使用包含通配符的主题名可以订阅满足匹配条件的所有主题。为了和 PUBLISH 中的主题区分，我们叫 SUBSCRIBE 中的主题名为主题过滤器（Topic Filter）。 单层通配符“+”：就如之前我们讲的，MQTT 的主题是具有层级概念的，不同的层级之间用“&#x2F;”分割，“+”可以用来指代任意一个层级。 例如“home&#x2F;2ndfloor&#x2F;+&#x2F;temperature”，可匹配： home&#x2F;2ndfloor&#x2F;201&#x2F;temperature home&#x2F;2ndfloor&#x2F;202&#x2F;temperature 不可匹配： home&#x2F;2ndfloor&#x2F;201&#x2F;livingroom&#x2F;temperature home&#x2F;3ndfloor&#x2F;301&#x2F;temperature 多层通配符“#”：“#”和“+”的区别在于，“#”可以用来指定任意多个层级，但是“#”必须是 Topic Filter 的最后一个字符，同时它必须跟在“&#x2F;”后面，除非 Topic Filter 只包含“#”这一个字符。 例如“home&#x2F;2ndfloor&#x2F;#”，可匹配： home&#x2F;2ndfloor home&#x2F;2ndfloor&#x2F;201 home&#x2F;2ndfloor&#x2F;201&#x2F;temperature home&#x2F;2ndfloor&#x2F;202&#x2F;temperature home&#x2F;2ndfloor&#x2F;201&#x2F;livingroom&#x2F;temperature 不可匹配： home&#x2F;3ndfloor&#x2F;301&#x2F;temperature 注意：“#”是一个合法的 Topic Filter，代表所有的主题;而“home#”不是一个合法的 Topic Filter，因为“#”号需要跟在“&#x2F;”后面。 SUBSCRIBE 数据包中 QoS 代表针对某一个或者一组主题，Client 希望 Broker 在发送来自这些主题的消息给它时，消息使用的 QoS 级别，我们在《第06课：QoS0 和 QoS1》里面再详细讨论。 5.1.2 SUBACK为了确认每一次的订阅，Broker 收到 SUBSCRIBE 之后会回复一个 SUBACK 数据包作为应答。 5.1.2.1 可变头（Variable header）数据包标识（Packet Identifier）：两个字节，用来唯一标识一个数据包，数据包标识只需要保证在从 Sender 到 Receiver 的一次消息交互中保持唯一。 5.1.2.2 消息体（Payload）返回码（return codes）：SUBBACK 数据包包含了一组返回码，返回码的数量和顺序和 SUBSCRIBE 数据包的订阅列表对应，用于标识订阅类别中的每一个订阅项的订阅结果。 返回码 含义 0 订阅成功， 最大可用 QoS 为 0 1 订阅成功，最大可用 QoS 为 1 2 订阅成功， 最大可用 QoS 为 2 128 订阅失败 返回码 0~2 代表订阅成功，同时 Broker 授予 Subscriber 不同的 QoS 等级，这个等级可能会和 Subscriber 在 SUBSCRIBE 数据包中要求的不一样。 返回码 128 代表订阅失败，比如 Client 没有权限订阅某个主题，或者要求订阅的主题格式不正确等。 5.1.3 代码实践：订阅一个主题接下来我们来写订阅并处理消息的代码，我们订阅在上一课中的 publisher.js 中的主题，并通过捕获 “message” 事件获取接收的消息并打印出来。 通常我们在建立和 Broker 的连接之后就可以开始订阅了，但是这里有一个小小的优化，如果你建立的是持久会话的连接，那么有可能 Broker 已经保存你在之前的连接时订阅的主题，你就没有必要再发起 SUBSCRIBE 请求了，这个小优化在网络带宽或者设备处理能力较差的情况尤为重要。 完整的代码 subscriber.js 如下： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;, &#123; clientId: &quot;mqtt_sample_subscriber_id_1&quot;, clean: false&#125;)client.on(&#x27;connect&#x27;, function (connack) &#123; if(connack.returnCode == 0) &#123; if (connack.sessionPresent == false) &#123; console.log(&quot;subscribing&quot;) client.subscribe(&quot;home/2ndfloor/201/temperature&quot;, &#123; qos: 1 &#125;, function (err, granted) &#123; if (err != undefined) &#123; console.log(&quot;subscribe failed&quot;) &#125; else &#123; console.log(`subscribe succeeded with $&#123;granted[0].topic&#125;, qos: $&#123;granted[0].qos&#125;`) &#125; &#125;) &#125; &#125;else &#123; console.log(`Connection failed: $&#123;connack.returnCode&#125;`) &#125;&#125;)client.on(&quot;message&quot;, function (_, message, _) &#123; var jsonPayload = JSON.parse(message.toString()) console.log(`current temperature is $&#123;jsonPayload.current&#125;`)&#125;) 在终端上运行 node subscriber.js 我们会得到以下输出： subscribingsubscribe succeeded with home/2ndfloor/201/temperature, qos: 1 第一次运行这个代码的时候，Broker 上面没有保存这个 Client 的会话，所以需要进行订阅，现在 CTRL+C 终止这段代码的运行，然后重新运行，因为 Broker 上面已经保存了这个 Client 的会话，所以就不需要再订阅了，你就不会看到订阅相关的输出了。 在上一课中，我们运行过 publisher.js，向“home&#x2F;2ndfloor&#x2F;201&#x2F;temperature”这个主题发布过一个消息，但是这发生在 subscriber.js 订阅该主题之前，所以现在 Subscriber 不会收到任何消息，我们需要再运行一次 publish.js，然后在运行 subscriber.js 的终端上会输出： current temperature is 25 好了，我们终于通过 MQTT 协议完成了一次点到点的消息传递，同时我们也验证了，建立持久性会话连接之后，Broker 会保存 Client 的订阅信息。 5.2 取消订阅Subcriber 也可以取消对某些主题的订阅，取消订阅的流程如下： Client 向 Broker 发送一个 UNSUBSCRIBE 数据包，其中包含了 Client 想要取消订阅的主题； Broker 收到 UNSUBSCRIBE 数据包后，向 Client 发送一个 UNSUBACK 数据包作为应答。 接下来我们看数据包的具体内容。 5.2.1 UNSUBSCRIBE5.2.1.1 可变头（Variable header）数据包标识（Packet Identifier）：两个字节，用来唯一标识一个数据包，数据包标识只需要保证在从 Sender 到 Receiver 的一次消息交互中保持唯一。 5.2.1.2 消息体（Payload）主题列表（List of Topics）：UNSUBSCRIBE 的消息体中包含 Client 想要取消订阅的主题过滤器列表，这些主题过滤器和 SUBSCRIBE 数据包中一样，可以包含通配符。UNSUBSCRIBE 消息体里面不再包含主题过滤器对应的 QoS 了。 5.2.2 UNSUBACKBroker 收到 UNSUBSCRIBE 之后会回复一个 UNSUBACK 数据包作为应答： 5.2.2.1 可变头（Variable header）数据包标识（Packet Identifier）：两个字节，用来唯一标识一个数据包，数据包标识只需要保证在从 Sender 到 Receiver 的一次消息交互中保持唯一。 5.2.2.2 消息体（Payload）UNSUBACK 数据包没有消息体。 5.3 代码实践：取消订阅我们要完成的代码很简单，在建立连接之后取消对之前订阅的主题。 完整的代码 unsubscribe.js 如下： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;, &#123; clientId: &quot;mqtt_sample_subscriber_id_1&quot;, clean: false&#125;)client.on(&#x27;connect&#x27;, function (connack) &#123; if (connack.returnCode == 0) &#123; console.log(&quot;unsubscribing&quot;) client.unsubscribe(&quot;home/2ndfloor/201/temperature&quot;, function (err) &#123; if (err != undefined) &#123; console.log(&quot;unsubscribe failed&quot;) &#125; else &#123; console.log(&quot;unsubscribe succeeded&quot;) &#125; client.end() &#125;) &#125; else &#123; console.log(`Connection failed: $&#123;connack.returnCode&#125;`) &#125;&#125;) 在终端上运行 node unsubscribe.js，会得到以下输出： unsubscribingunsubscribe succeeded 在这里取消了对“home&#x2F;2ndfloor&#x2F;201&#x2F;temperature”的订阅，所以再运行 subscriber.js 和 publisher.js，再运行 subscribe.js 的终端不会再有消息的打印信息了。如何要使 subscriber.js 重新订阅这个主题，读者可以动下脑筋然后自己动手实现一下。 5.4 小结我们终于完成了发布订阅的学习，并第一次实现了消息的点到点传输， 接下来我们开始学习 MQTT 中的一个非常重要的特性：三种 QoS 等级。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（四）","slug":"aiot/aiot_tutorial_4","date":"2020-06-27T20:50:00.000Z","updated":"2022-08-01T06:35:23.688Z","comments":true,"path":"2020/06/28/aiot/aiot_tutorial_4.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/28/aiot/aiot_tutorial_4.html","excerpt":"","text":"接下来我们来学习 MQTT 协议中的消息订阅与发布。本节课核心内容： 订阅与发布模型 PUBLISH 代码实践：发布消息 4.1 订阅与发布模型在第一课中，我们介绍了 MQTT 基于订阅与发布的消息模型，MQTT 协议的订阅与发布是基于主题的（Topic），一个典型的 MQTT 消息发送与接收的流程如下： ClientA 连接到 Broker； ClientB 连接到 Broker，并订阅主题 Topic1； ClientA 发送给 Broker 一条消息，主题为 Topic1； Broker 收到 ClientA 的消息，发现 ClientB 订阅了 Topic1，然后将消息转发到 ClientB； ClientB 从 Broker 接收到该消息。 和传统的队列有点不同，如果 ClientB 在 ClientA 发布消息之后再订阅 Topic 1，ClientB 不会收到该条消息。 MQTT 通过订阅与发布模型对消息的发布者和订阅者进行解耦，发布者在发布消息时并不需要订阅方也连接到 Broker，只要订阅方之前订阅过相应主题，那么它在连接到 Broker 之后就可以收到发布方在它离线期间发布的消息。为了方便起见，在本课程中我们称这种消息为离线消息。 接收离线的消息需要 Client 使用持久化会话，且发布时消息的 QoS 大于 1。 在我们往下继续学习之前，很有必要搞清楚两组概念：发布者（Publisher）和订阅者（Subscriber），发送方（Sender）和接收方（Recevier）。弄清楚这两个概念才能很好理解订阅和发布的流程，以及之后 QoS 的概念。 4.1.1 Publisher 和 SubscriberPublisher 和 Subscriber 是相对于 Topic 来说的身份，如果一个 Client 向某个 Topic 发布消息，那么它就是 Publisher；如果一个 Client 订阅了某个 Topic，那么它就是 Subscriber。在上面的例子中，ClientA 是 Publisher， ClientB 是 Subscriber。 4.1.2 Sender 和 ReceiverSender 和 Receiver 是相对于消息传输方向的身份，仍然是上面的例子： 当 ClientA 发布消息时，它发送给 Broker 一条消息，那么 ClientA 是 Sender，Broker 是 Receiver； 当 Broker 转发消息给 ClientB 时，Broker 是 Sender，ClientB 是 Receiver。 Publisher&#x2F;Subscriber、Sender&#x2F;Receiver 这两组概念最大的区别就是，Publisher 和 Subscriber 只可能是 Client。而 Sender&#x2F;Receiver 有可能是 Client 和 Broker。解释清楚这两个不同的概念之后，我们接下来看一下 PUBLISH 消息包。 4.2 PUBLISHPUBLISH 数据包是用于在 Sender 和 Receiver 之间传输消息数据的，也就是说，当 Publisher 要向某个 Topic 发布一条消息的时候，Publisher 会向 Broker 发送一个 PUBLISH 数据包；当 Broker 要将一条消息转发给订阅了某条主题的 Subscriber 时，Broker 也会向 Subscriber 发送一条 PUBLISH 数据包。PUBLISH 数据包的内容如下。 4.2.1 固定头消息重复标识（DUP flag）：1bit，0 或者 1，当 DUP flag &#x3D; 1 的时候，代表该消息是一条重发消息，因 Receiver 没有确认收到之前的消息而重新发送的。这个标识只在 QoS 大于 0 的消息中使用。 QoS：2bit，0、1 或者 2，代表 PUBLISH 消息的 QoS level，我们在 QoS 课程再详细讲解。 Retain 标识（Retain flag）：1bit，0 或者 1，在从 Client 发送到 Broker 的 PUBLISH 消息中被设为 1 的时候，Broker 应该保存该条消息，当之后有任何新的 Subscriber 订阅 PUBLISH 消息中指定的主题时，都会先收到该条消息，这种消息也叫 Retained 消息；在从 Broker 发送到 Client 的 PUBLISH 消息中被设为 1 的时候，代表该条消息是一条 Retained 消息。 4.2.2 可变头数据包标识（ Packet Identifier）：2bit，用来标识一个唯一数据包，数据包标识只需要保证在从 Sender 到 Receiver 的一次消息交互（比如发送、应答为一次交互）中保持唯一。只在 QoS 大于 1 的消息中使用，因为只有 QoS 大于 1 的消息有应答流程，我们会在《第06课：QoS0 和 QoS1》详细讲解。 主题名称（Topic Name）：主题名称是一个 UTF-8 编码的字符串，用来命名该消息发布到哪一个主题，Topic Name 可以是长度大于等于 1 任何一个字符串（可包含空格），但是在实际项目中，我们最好还是遵循以下一些最优方法。 主题名称应该包含层级，不同的层级用 / 划分，比如，2 楼 201 房间的温度感应器可以用这个主题：“home&#x2F;2ndfloor&#x2F;201&#x2F;temperature”。 主题名称开头不要使用 /，例如：“&#x2F;home&#x2F;2ndfloor&#x2F;201&#x2F;temperature”。 不要在主题中使用空格。 只使用 ASCII 字符。 主题名称在可读的前提下尽量短。 主题是大小写敏感的，“Home” 和 “home” 是两个不同的主题。 可以将设备的唯一标识加到主题中，比如：“warehouse&#x2F;shelf&#x2F;shelf1_ID&#x2F;status”。 主题尽量精确，不要使用泛用的主题，例如在 201 房间有三个传感器，温度、亮度和湿度，那么你应该使用三个主题名称：“home&#x2F;2ndfloor&#x2F;201&#x2F;temperature”、“home&#x2F;2ndfloor&#x2F;201&#x2F;brightness”和“home&#x2F;2ndfloor&#x2F;201&#x2F;humidity”，而不是让三个传感器都使用“home&#x2F;2ndfloor&#x2F;201”。 以 $ 开头的主题属于 Broker 预留的系统主题，通常用于发布 Broker 的内部统计信息，比如 $SYS/broker/clients/connected，应用程序不要使用 $ 开头的主题收发数据。 4.2.3 消息体（Payload）PUBLISH 消息的消息体中包含的是该消息要发送的具体数据，数据可以是任何格式的，二进制数据、文本、JSON 等，由应用程序来定义。在实际生产中，我们可以使用 JSON、Protocol Buffer 等对数据进行编码。 当 Receiver 收到来自 Sender 的 PUBLISH 消息时，根据 QoS 的不同，还有后续的应答流程。我们在 QoS 课程再详细讲解。 当 PUBLISH 消息的 QoS&#x3D;0 时， Receiver 不做任何应答。 4.3 代码实践：发布消息接下来我们写一小段代码，向一个主题发布一条 QoS 为 1 的使用 JSON 编码的数据，然后退出： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;, &#123; clientId: &quot;mqtt_sample_publisher_1&quot;, clean: false&#125;)client.on(&#x27;connect&#x27;, function (connack) &#123; if(connack.returnCode == 0)&#123; client.publish(&quot;home/2ndfloor/201/temperature&quot;, JSON.stringify(&#123;current: 25&#125;), &#123;qos: 1&#125;, function (err) &#123; if(err == undefined) &#123; console.log(&quot;Publish finished&quot;) client.end() &#125;else&#123; console.log(&quot;Publish failed&quot;) &#125; &#125;) &#125;else&#123; console.log(`Connection failed: $&#123;connack.returnCode&#125;`) &#125;&#125;) 运行 node publisher.js，会得到以下输出： Publish finished 4.4 小结在本节课我们学习了 MQTT 订阅和发布的模型，弄清楚了 Publisher&#x2F;Subscriber、Sender&#x2F;Receiver 的区别，并编写了发布消息的代码。接下来我们学习如何接收刚刚发布的消息。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（三）","slug":"aiot/aiot_tutorial_3","date":"2020-06-27T19:50:00.000Z","updated":"2022-08-01T06:35:23.683Z","comments":true,"path":"2020/06/28/aiot/aiot_tutorial_3.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/28/aiot/aiot_tutorial_3.html","excerpt":"","text":"在上一课中我们详细地了解了从Client到Broker的连接建立，接下来看一下如何关闭连接。本节课核心内容： 3.1 Client 主动关闭连接Client 主动关闭连接的流程非常简单，只需要向 Broker 发送一个 DISCONNECT 数据包就可以了。DISCONNECT数据包没有可变头（Variableheader）和消息体（Payload）。在 Client 发送完 DISCONNECT 之后，就可以关闭底层的 TCP 连接了，不需要等待 Broker 的回复（Broker 也不会对 DISCONNECT 数据包回复）。这里读者可能有一个疑问，为什么需要在关闭TCP连接之前，发送一个和 Broker 没有交互的 DISCONNECT 数据包，而不是直接关闭底层的 TCP 连接？这里涉及到 MQTT 协议的一个特性，Broker 需要判断 Client 是否正常地断开连接。当 Broker 收到 Client 的 DISCONNECT 数据包的时候，它认为 Client 是正常地断开连接，那么它会丢弃当前连接指定的遗愿消息（Will Message）。如果B roker 检测到 Client 连接丢失，但又没有收到 DISCONNECT 消息包，它会认为 Client 是非正常断开连接，就会向在连接的时候指定的遗愿主题（Will Topic）发布遗愿消息（WillMessage） 3.2 Broker主动关闭连接MQTT 协议规定 Broker 在没有收到 Client 的 DISCONNECT 数据包之前都应该保持和 Client 连接，只有 Broker 在 KeepAlive 的时间间隔里，没有收到 Client 的任何 MQTT 数据包的时候会主动关闭连接。一些 Broker 的实现在 MQTT 协议上做了一些拓展，支持 Client 的连接管理，可以主动地断开和某个 Client 的连接。Broker 主动关闭连接之前不会向 Client 发送任何 MQTT 数据包，直接关闭底层的 TCP 连接就完事了。 3.3 代码实践下面就到了大家最喜欢的代码环节了，这里我们将用代码来展示MQTT连接的建立，和断开各种情况下的示例。 在这里我们使用 Node.js 的 MQTT 库，请确保已安装 Node.js，并通过 npm install mqtt –save 安装了 MQTT 库。 这里我们使用一个公共的 Broker：mqtt.eclipse.org。 3.3.1 建立持久会话的连接首先引用MQTT库： var mqtt = require (&#x27;mqtt&#x27;) 然后建立连接： var client = mqtt.connect( &#x27;mqtt://mqtt.eclipse.org:1883&#x27;, &#123; clientId: &quot;mqtt_sample_id_1&quot;, clean: false&#125;) 这里我们通过 Client ID 选项指定 Client Identifier，并通过 Clean 选项设定 Clean Session 为 false，代表我们要建立一个持久化会话的连接。 接下来我们通过捕获 connect 事件将 CONNACK 包 Return Code 和 Session Present Flag 打印出来，然后断开连接： client.on(&#x27;connect&#x27;, function ( connack ) &#123; console.log (`returncode: $&#123;connack.returnCode&#125;, sessionPresent:$&#123;connack.sessionPresent&#125;`)client.end() 完整的代码 persistent_connection.js 如下： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;,&#123; clientId:&quot;mqtt_sample_id_1&quot;, clean:false&#125;)client.on(&#x27;connect&#x27;,function(connack)&#123; console.log(`returncode: $&#123;connack.returnCode&#125;, sessionPresent: $&#123;connack.sessionPresent&#125;`) client.end()&#125;) 我们在终端上运行： node persistent_connection.js 会得到以下输出： returncode: 0, sessionPresent: false 连接成功，因为是 “mqtt_sample_id_1” 的 Client 第一次建立连接，所以 SessionPresent 为 false。 再次运行 node persistent_connection.js，输出就会变成： returncode: 0, sessionPresent: true 3.3.2 建立非持久会话的连接我们只需要将 Clean 选项设为 true，就可以建立一个非持久会话的连接了，完整的代码 non_persistent_connetion.js 如下： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;,&#123; clientId: &quot;mqtt_sample_id_1&quot;, clean: true&#125;)client.on(&#x27;connect&#x27;, function(connack ) &#123; console.log(`returncode:$&#123;connack.returnCode&#125;, sessionPresent: $&#123;connack.sessionPresent&#125;`) client.end()&#125;) 我们在终端上运行： node persistent_connection.js 会得到以下输出： returncode: 0, sessionPresent: false 无论运行多少次，SessionPresent 都将为 false。 3.3.3 使用相同的 Client Identifier 进行连接接下来我们看一下如果两个 Client 使用同样的 Client Identifier 会发生什么。我们把代码稍微调整下，在连接成功的时候保持连接，然后捕获 offline 事件，在 Client 的连接被关闭的时候打印出来。 完整的代码 identifcal.js 如下： var mqtt = require(&#x27;mqtt&#x27;)var client = mqtt.connect(&#x27;mqtt://mqtt.eclipse.org:1883&#x27;,&#123; clientId:&quot;mqtt_identical_1&quot;,&#125;)client.on(&#x27;connect&#x27;, function(connack) &#123; console.log(`returncode: $&#123;connack.returnCode&#125;, sessionPresent: $&#123;connack.sessionPresent&#125;`)&#125;)client.on(&#x27;offline&#x27;, function() &#123; console.log(&quot;client went offline&quot;)&#125;) 然后我们打开两个终端，分别在上面运行 node identifcal.js，然后我们会看到在两个终端上不停地出现以下打印： returncode: 0, sessionPresent: falseclient went offlinereturncode: 0, sessionPresent: falseclient went offlinereturncode: 0, sessionPresent: false....... 在 MQTT 中，在两个 Client 使用相同的 ClientI dentifier 进行连接时，如果第二个 Client 连接成功，Broker 会关闭和第一个已经连接上的 Client 连接。由于我们使用的 MQTT 库实现了断线重连的功能，所以当连接被 Broker 关闭时，它又会尝试重新连接，结果就是这两个 Client 交替地把对方顶下线，我们就会看到这样的打印输出。因此在实际应用中，一定要保证每一个设备使用的 Client Identifier 是唯一的。 如果你观察到一个 Client 不停地上线下线，那么有很大可能是 Client Identifier 冲突的问题。 3.4 小结在本节课中我们学习了 MQTT 连接关闭的过程，并且学习了连接建立和关闭的相关代码，下一课我们来学习发布和订阅的概念，实现消息在 Client 之间的传输。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（二）","slug":"aiot/aiot_tutorial_2","date":"2020-06-27T18:50:00.000Z","updated":"2022-08-01T06:35:23.773Z","comments":true,"path":"2020/06/28/aiot/aiot_tutorial_2.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/28/aiot/aiot_tutorial_2.html","excerpt":"","text":"Client在可以发布和订阅消息之前，必须先连接到Broker，下面我们来看一下Client连接到Broker的流程。本节课核心内容： Client 连接到 Broker 的流程 CONNECT CONNACK 2.1 Client 连接到 Broker 的流程Client建立到Broker的连接流程如下图所示： 2.2 CONNECT​ 连接的建立由 Client 端发起，Client 端首先向 Broker 发送一个 CONNECT 数据包，CONNECT 数据包包含以下内容（这里我们略过 Fixedheader）。 2.2.1 可变头（Variable header）在 CONNECT 数据包可变头中，含有以下信息。 「协议名称」（ProtocolName）：值固定为字符 “MQTT”。 「协议版本」（ProtocolLevel）：对 MQTT 3.1.1 来说，值为 4。 「用户名标识」（UserNameFlag）：消息体中是否有用户名字段，1 bit，0 或者 1。 「密码标识」（PasswordFlag）：消息体中是否有密码字段，1 bit，0 或者 1。 「遗愿消息 Retain 标识」（Will Retain）：标识遗愿消息是否是Retain消息，1bit，0或者1。 「遗愿消息 QOS 标识」（Will QOS）：标识遗愿消息的 QOS，2bit，0、1或者2。 「遗愿标识」（Will Flag）：标识是否使用遗愿消息，1 bit，0 或者 1。 「会话清除标识」（CleanSession）：标识Client是否建立一个持久化的会话，1 bit，0 或者 1，当 CleanSession 的标识设为0时，代表 Client 希望建立一个持久会的连接，Broker 将存储该 Client 订阅的主题和未接受的消息，否则 Broker 不会存储这些数据，同时在建立连接时清除这个 Client 之前存在的持久化会话所保存的数据。 「连接保活」（KeepAlive）:设置一个单位为秒的时间间隔，Client 和 Broker 之间在这个时间间隔之内需要至少有一次消息交互，否则 Client 和 Broker 会认为它们之间的连接已经断开。 2.2.2 消息体（Payload）CONNECT 数据包的消息体中包含以下数据。 「客户端标识符」（Client Identifier）：Client Identifier 是用来标识 Client 身份的字段，在 MQTT 3.1.1 的版本中，这个字段的长度是1到23个字节，而且只能包含数字和26个字母（包括大小写），Broker 通过这个字段来区分不同的 Client。所以在连接的时候，Client 应该保证它的 Identifier 是唯一的，通常我们可以使用比如 UUID，唯一的设备硬件标识，或者 Android 设备的 DEVICE_ID 等作为 Client Identifier 的取值来源。 MQTT 协议中要求 Client 连接时必须带上 Client Identifier，但是也允许 Broker 在实现时 Client Identifier 为空，这时 Broker 会为 Client 分配一个内部唯一的Identifier。如果你需要使用持久化会话，那就必须自己为Client设定一个唯一的Identifier。 「用户名」（Username）：如果可变头中的用户名标识设为1，那么消息体中将包含用户名字段，Broker 可以使用用户名和密码来对接入的Client进行验证，只允许已授权的Client接入。注意不同的Client需要使用不同的 ClientIdentifier，但它们可以使用同样的用户名和密码进行连接。 「密码」（Password）：如果可变头中的密码标识设为1，那么消息体中将包含密码字段。 「遗愿主题」（Will Topic）：如果可变头中的遗愿标识设为1，那么消息体中将包含遗愿主题，当 Client 非正常地中断连接的时候，Broker 将向指定的遗愿主题中发布遗愿消息。 「遗愿消息」（Will Message）：如果可变头中的遗愿标识设为1，那么消息体中将包含遗愿消息，当Client非正常地中断连接的时候，Broker 将向指定的遗愿主题中发布由该字段指定的内容。我们会在后续的课程里面详细讨论。 2.3 CONNACK当 Broker 收到 Client 的 CONNECT 数据包之后，将检查并校验 CONNECT 数据包的内容，之后回复 Client 一个 CONNACK 数据包。CONNACK 数据包包含以下内容（这里我们略过 Fixedheader）。 2.3.1 可变头（Variable header）CONNACK 数据包的可变头中，含有以下信息。 「会话存在标识」（Session Present Flag）：用于标识在 Broker上，是否已存在该Client（用 Client Identifier 区分）的持久性会话，1 bit，0 或者 1。 当 Client 在连接时设置 Clean Session &#x3D; 1，则 CONNACK 中的 Session Presen tFlag 始终为 0； 当 Client 在连接时设置 Clean Session &#x3D; 0，那么就有两种情况——如果 Broker 上面保存了这个 Client 之前留下的持久性会话，那么 CONNACK 中的 Session Present Flag 值为 1； 如果 Broker 没有保存该 Client 的任何会话数据，那么 CONNACK 中的 Session Present Flag 值为 0。Session Present Flag 这个特性是在 MQTT 3.1.1 版本中新加入的，之前的版本中并没有这个标识。 「连接返回码」（Connect Return code）：用于标识 Client 是 Broker 的连接是否建立成功，连接返回码有以下一些值： Return Code 连接状态 0 连接已建立 1 连接被拒绝，不允许的协议版本 2 连接被拒绝，Client Identifier 被拒绝 3 连接被拒绝，服务器不可用 4 连接被拒绝，错误的用户名或密码 5 连接被拒绝，未授权 这里重点讲一下 Return Code 4 和 5 。Return Code 4 在 MQTT 协议中的含义是Username 和 Password 的格式不正确， 但是在大部分的 Broker 实现中， 在使用错误的用户名密码时， 得到的返回码也是 4 。所以这里我们认为 4 就是代表错误的用户名或密码。Return Code 5 一般在 Broker 不使用用户名和密码而使用 IP 地址或者 Client Identifier 进行验证的时候使用， 来标识 Client 没有通过验证。 2.3.2 消息体（Payload）CONNACK 没有消息体。 当 Client 向 Broker 发送 CONNECT 数据包并获得 Return Code 为 0 的CONNACK 包后， 就代表连接建立成功， 可以发布和接受消息了。 2.4 小结本节课我们了解了 Client 连接到 Broker 的流程，接下来我们学习连接的关闭，以及 MQTT 连接建立与关闭的实例代码。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"手把手教你入门AIoT（一）","slug":"aiot/aiot_tutorial_1","date":"2020-06-27T17:50:00.000Z","updated":"2022-08-01T06:35:23.865Z","comments":true,"path":"2020/06/28/aiot/aiot_tutorial_1.html","link":"","permalink":"http://wuzguo.com/blog/2020/06/28/aiot/aiot_tutorial_1.html","excerpt":"","text":"​物联网被认为是继计算机、互联网之后，信息技术行业的第三次浪潮。随着基础通讯设施的不断完善，尤其是 5G 的出现，进一步降低了万物互联的门槛和成本。说到物联网不得不讲下物联网通讯。​物联网通讯是物联网的一个核心内容，目前物联网的通讯协议并没有一个统一的标准，比较常见的有 MQTT、CoAP、DDS、XMPP 等，在这其中，MQTT（消息队列遥测传输协议）应该是应用最广泛的标准之一。​所以入门物联网，掌握 MQTT 是一个非常必要的步骤。 1.1 MQTT 是什么MQTT 协议是什么？简单地来说 MQTT 协议有以下特性： 基于 TCP 协议的应用层协议； 采用 C&#x2F;S 架构； 使用订阅&#x2F;发布模式，将消息的发送方和接受方解耦； 提供 3 种消息的 QoS（Quality of Service）: 至多一次，最少一次，只有一次； 收发消息都是异步的，发送方不需要等待接收方应答。 虽然 MQTT 协议名称有 Message Queue 两个词，但是它并不是一个像 RabbitMQ 那样的一个消息队列，这是初学者最容易搞混的一个问题。MQTT 跟传统的消息队列相比，有以下一些区别： 在传统消息队列中，在发送消息之前，必须先创建相应的队列；在 MQTT 中，不需要预先创建要发布的主题（可订阅的 Topic）； 在传统消息队列中，未被消费的消息总是会被保存在某个队列中，直到有一个消费者将其消费；在 MQTT 中，如果发布一个没有被任何客户端订阅的消息，这个消息将被直接扔掉； 在传统消息队列中，一个消息只能被一个客户端获取，在 MQTT 中，一个消息可以被多个订阅者获取，MQTT 协议也不支持指定消息被单一的客户端获取。 MQTT 协议可以为大量的低功率、工作网络环境不可靠的物联网设备提供通讯保障。而它的应用范围也不仅如此，在移动互联网领域也大有作为：很多 Android App 的推送功能，都是基于 MQTT 实现的，也有一些 IM 的实现，是基于 MQTT 的。 1.2 MQTT 协议的通信模型就像我们在之前提到的，MQTT 的通信是通过发布&#x2F;订阅的方式来实现的，消息的发布方和订阅方通过这种方式来进行解耦，它们没有直接地连接，它们需要一个中间方。在 MQTT 里面我们称之为 Broker，用来进行消息的存储和转发。一次典型的 MQTT 消息通信流程如下所示： 发布方将消息发送到 Broker； Broker 接收到消息以后，检查下都有哪些订阅方订阅了此类消息，然后将消息发送到这些订阅方； 订阅方从 Broker 获取该消息。 接下来的内容我们将发送方称为 Publisher，将订阅方称为 Subscriber。 1.3 MQTT Client任何终端，嵌入式设备也好，服务器也好，只要运行了 MQTT 的库或者代码，我们都称为 MQTT 的 Client。Publisher 和 Subscriber 都属于 Client，Pushlisher 或者 Subscriber 只取决于该 Client 当前的状态——是在发布还是在订阅消息。当然，一个 Client 可以同时是 Publisher 和 Subscriber。 MQTT Client 库在很多语言中都有实现，包括 Android、Arduino、Ruby、C、C++、C#、Go、iOS、Java、JavaScript，以及 .NET 等。如果你要查看相应语言的库实现，可以在这里 (https://github.com/mqtt/mqtt.github.io/wiki/libraries) 找到。 本系列课程我们主要使用 Node.js 的 MQTT Client 库来进行演示，所以需要先安装 Node.js，然后安装 MQTT Client 的 Node.js 包： npm install mqtt -g 1.4 MQTT Broker如前面所讲的，Broker 负责接收 Publisher 的消息，并发送给相应的 Subscriber，它是整个 MQTT 订阅&#x2F;发布的核心。在实际应用中，一个 MQTT Broker 还应该提供以下一些功能： 可以横向扩展，比如集群，来满足大量的 Client 接入； 可以扩展接入业务系统； 易于监控，满足高可用性。 本系列文章我们使用一个公共的 MQTT Broker —— iot.eclipse.org 做演示，同时也会学习如何搭建一个 MQTT Broker。 1.5 MQTT 协议数据包MQTT 协议的数据包格式非常简单，一个 MQTT 协议数据包由下面三个部分组成： 固定头（Fixed header）：存在于所有的 MQTT 数据包中，用于表示数据包类型及对应标识，表明数据包大小； 可变头（Variable header）：存在于部分类型的 MQTT 数据包中，具体内容由相应类型的数据包决定； 消息体（Payload）：存在于部分 MQTT 数据包中，存储消息的具体数据。 接下来看一下固定头的格式，可变头和消息体我们将在讲解各种具体类型的 MQTT 协议数据包的时候 case by case 地讨论。 固定头格式： Bit 7 6 5 4 3 2 1 0 字节 1 MQTT 数据包类型 MQTT 数据包 Flag， 内容由数据包类型指定 字节 2…… 数据包剩余长度 固定头的第一个字节的高 4 位 bit 用于指定该数据包的类型，MQTT 的数据包有以下一些类型： 名称 值 方向 描述 Reserved 0 不可用 保留位 CONNECT 1 Client 到 Broker Client 请求连接到 Broker CONNACK 2 Broker 到 Client 连接确认 PUBLISH 3 双向 发布消息 PUBACK 4 双向 发布确认 PUBREC 5 双向 发布收到 PUBREL 6 双向 发布释放 PUBCOMP 7 双向 发布完成 SUBSCRIBE 8 Client 到 Broker Client 请求订阅 SUBACK 9 Broker 到 Client 订阅确认 UNSUBSCRIBE 10 Client 到 Broker Client 请求取消订阅 UNSUBACK 11 Broker 到 Client 取消订阅确认 PINGREQ 12 Client 到 Broker PING 请求 PINGRESP 13 Broker 到 Client PING 应答 DISCONNECT 14 Client 到 Broker Client 主动中断连接 Reserved 15 不可用 保留位 固定头的低 4 位 bit 用于指定数据包的 Flag，不同的数据包类型，其 Flag 的定义是不一样的，每种数据包对应的 Flag 如下： 数据包 标识位 Bit 3 Bit 2 Bit 1 Bit 0 CONNECT 保留位 0 0 0 0 CONNACK 保留位 0 0 0 0 PUBLISH MQTT 3.1.1 使用 DUP QoS QoS RETAIN PUBACK 保留位 0 0 0 0 PUBREC 保留位 0 0 0 0 PUBREL 保留位 0 0 0 0 PUBCOMP 保留位 0 0 0 0 SUBSCRIBE 保留位 0 0 0 0 SUBACK 保留位 0 0 0 0 UNSUBSCRIBE 保留位 0 0 0 0 UNSUBACK 保留位 0 0 0 0 PINGREQ 保留位 0 0 0 0 PINGRESP 保留位 0 0 0 0 DISCONNECT 保留位 0 0 0 0 从固定头的第 2 字节开始是用于标识 MQTT 数据包长度的字段，最少一个字节，最大四个字节，每一个字节的低 7 位用于标识值，范围为 0~127。最高位的 1 位是标识位，用来说明是否有后续字节来标识长度。例如：标识为 0，代表为没有后续字节；标识为 1，代表后续还有一个字节用于标识包长度。MQTT 协议规定最多可以用四个字节来标识包长度。 所以这四个字节最多可以标识的包长度为：(0xFF, 0xFF, 0xFF, 0x7F) &#x3D; 268435455 字节，约 256M，这个是 MQTT 协议中数据包的最大长度。 1.7 总结我们在这一课中学习了 MQTT 的通信模型，以及 Client 和 Broker 的概念，同时也学习了 MQTT 数据包的格式。接下来我们开始收发数据的第一步：从 Client 连接到 Broker。","categories":[{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"}],"tags":[{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"Git彩蛋","slug":"tools/git_egg_1","date":"2018-03-15T07:15:23.000Z","updated":"2022-08-01T06:35:23.747Z","comments":true,"path":"2018/03/15/tools/git_egg_1.html","link":"","permalink":"http://wuzguo.com/blog/2018/03/15/tools/git_egg_1.html","excerpt":"","text":"最近学到一条非常流弊的Git命令，记录一下，先看下效果： 命令为：git log --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit --date=relative 如果嫌命令太长可以给他设置个别名，方式如下：git config --global alias.lg &quot;log --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit --date=relative&quot; 下次再用的时候就只需要敲：git lg 即可。","categories":[{"name":"工具","slug":"tools","permalink":"http://wuzguo.com/blog/categories/tools/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://wuzguo.com/blog/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"http://wuzguo.com/blog/tags/GitHub/"},{"name":"彩蛋","slug":"彩蛋","permalink":"http://wuzguo.com/blog/tags/%E5%BD%A9%E8%9B%8B/"}],"author":"Zak"},{"title":"Selenium的基本使用","slug":"others/selenium_instructions","date":"2018-03-15T06:54:12.000Z","updated":"2022-08-01T06:35:23.693Z","comments":true,"path":"2018/03/15/others/selenium_instructions.html","link":"","permalink":"http://wuzguo.com/blog/2018/03/15/others/selenium_instructions.html","excerpt":"","text":"Selenium 简介 Selenium 是 ThroughtWorks 一个强大的基于浏览器的开源自动化测试工具，它通常用来编写Web应用的自动化测试。Selenium 2，又名 WebDriver，它的主要新功能是集成了 Selenium 1.0 以及 WebDriver（WebDriver 曾经是 Selenium 的竞争对手）。也就是说 Selenium 2 是 Selenium 和 WebDriver 两个项目的合并，即 Selenium 2 兼容 Selenium，它既支持 Selenium API 也支持 WebDriver API。 安装使用 依赖包下载 Python 下载地址：https://www.python.org/downloads/(下载2.7版本) Selenium RC获取地址：http://selenium-release.storage.googleapis.com/index.html 各种版本 Selenium IDE获取地址：http://release.seleniumhq.org/selenium-ide/2.9.0/ 各种版本 Selenium(Webdriver)下载地址:https://pypi.python.org/pypi/selenium 各种版本 Chromedriver获取地址：https://npm.taobao.org/mirrors/chromedriver/ 各种版本 安装 具体过程赘述。python 安装完成后需要将安装目录加入环境变量，然后在再命令行敲 python -V 能正确显示版本号就表示配置完成。我本地的执行效果如下： 安装Python的Selenium 依赖包 通过pip 安装 pip install selenium 通过下载包安装直接下载Selenium包：https://pypi.python.org/pypi/selenium，解压后进入目录，然后执行： python setup.py install 如果在IDE（如：PyCharm）中，引入 from selenium import webdriver 后提示未安装selenium 报错，可以直接点击结报错的按钮安装。 简单使用 环境以及服务都安装完成后，就可以写代码测试，我这里使用Python代码写了个简单的demo，如下： # coding=utf-8from selenium import webdriverimport timeif __name__ == &quot;__main__&quot;: path =&#x27;D:/test/selenium/chromedriver.exe&#x27; driver = webdriver.Chrome(executable_path = path) driver.implicitly_wait(5) driver.maximize_window() driver.get(&quot;http://www.baidu.com&quot;) time.sleep(3) driver.quit() 如果执行正常，Selenium 会在本地打开Chrome浏览器，并进入百度首页，3秒钟后正常关闭浏览器。 远程调用 以上只能在本地使用，如果要远程执行用例，需要启动 Selenium 服务端，由于Selenium 是一个jar包，所以需要依赖jre，没有安装JDK的需要自己安装，我在本地直接执行命令如下：java -jar selenium-server-standalone-2.53.1.jar ，输入以下信息表示启动成功： 然后执行以下脚本： # coding=utf-8import timeimport seleniumfrom selenium import webdriverfrom selenium.webdriver import DesiredCapabilitiesif __name__ == &quot;__main__&quot;: driver = selenium.webdriver.remote.webdriver.WebDriver(command_executor=&quot;http://172.16.0.55:4444/wd/hub&quot;, desired_capabilities=DesiredCapabilities.CHROME)driver.implicitly_wait(5)driver.maximize_window()driver.get(&quot;http://www.baidu.com&quot;)time.sleep(3)driver.quit() 效果同上，只是在远程机器上打开浏览器。 各种异常 Selenium 放大浏览器报错disconnected:unable to connect renderer解决办法 问题的原因是chrome浏览器版本太高跟chromedriver.exe不兼容导致的，解决方案： 降低chrome版本。 升高chromedriver.exe版本，地址：https://npm.taobao.org/mirrors/chromedriver/，我采用这种方式顺利解决。 报错 &#39;chromedriver&#39; executable needs to be in PATH.解决办法 这种错误是执行脚本找不到chromedriver导致的，解决方案： 将 chromedriver.exe 文件放在PATH环境变量中，需要注意的是PATH只需要指定 chromedriver.exe 文件所在的目录即可。 直接将chromedriver.exe 文件放在.py程序的目录下。 在代码中设置： path =&#x27;D:/test/selenium/chromedriver.exe&#x27;driver = webdriver.Chrome(executable_path = path)","categories":[{"name":"其他","slug":"others","permalink":"http://wuzguo.com/blog/categories/others/"}],"tags":[{"name":"Selenium","slug":"Selenium","permalink":"http://wuzguo.com/blog/tags/Selenium/"},{"name":"WebDriver","slug":"WebDriver","permalink":"http://wuzguo.com/blog/tags/WebDriver/"},{"name":"Python","slug":"Python","permalink":"http://wuzguo.com/blog/tags/Python/"}],"author":"Zak"},{"title":"Intellij IDEA打开SpringBoot工程的问题","slug":"others/idea_springboot_problems","date":"2018-03-07T07:29:45.000Z","updated":"2022-08-01T06:33:30.016Z","comments":true,"path":"2018/03/07/others/idea_springboot_problems.html","link":"","permalink":"http://wuzguo.com/blog/2018/03/07/others/idea_springboot_problems.html","excerpt":"","text":"本人习惯使用Intellij IDEA做Java项目开发，最近在新项目中使用SpringBoot时出现一个很奇怪的现象，新建的SpringBoot的项目在我机器上跑的好好的，在其他同事的机器上就是跑不起来，程序直接退出： ......Disconnected from the target VM, address: &#x27;127.0.0.1:49362&#x27;, transport: &#x27;socket&#x27;Process finished with exit code 1 实在没办法只能debug调试，在org.springframework.boot.devtools.restart.RestartLauncher 类的run方法中抛出异常： @Overridepublic void run() &#123; try &#123; Class&lt;?&gt; mainClass = getContextClassLoader().loadClass(this.mainClassName); Method mainMethod = mainClass.getDeclaredMethod(&quot;main&quot;, String[].class); mainMethod.invoke(null, new Object[] &#123; this.args &#125;); &#125; catch (Throwable ex) &#123; this.error = ex; getUncaughtExceptionHandler().uncaughtException(this, ex); &#125;&#125; 发现没有读取到配置文件。 经过我不懈努力，主要是找两个环境之间的差异，后来发现是IDEA生成的iml文件有问题，需要找到跟启动的项目名称相同的.iml文件（假如你的项目为 hello，那么IDEA一定会给你生成一个 hello.iml文件，一般在项目的.idea目录下或者项目根目录下。） 解决方案：​ 在 hello.iml文件中加上 &lt;sourceFolder url=&quot;file://$MODULE_DIR$/src/main/resources&quot; type=&quot;java-resource&quot; /&gt; 后问题得到解决。 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule=&quot;true&quot; type=&quot;JAVA_MODULE&quot; version=&quot;4&quot;&gt; &lt;component name=&quot;FacetManager&quot;&gt; &lt;facet type=&quot;Spring&quot; name=&quot;Spring&quot;&gt; &lt;configuration/&gt; &lt;/facet&gt; &lt;/component&gt; &lt;component name=&quot;NewModuleRootManager&quot; LANGUAGE_LEVEL=&quot;JDK_1_8&quot;&gt; &lt;output url=&quot;file://$MODULE_DIR$/target/classes&quot; /&gt; &lt;output-test url=&quot;file://$MODULE_DIR$/target/test-classes&quot; /&gt; &lt;content url=&quot;file://$MODULE_DIR$&quot;&gt; &lt;sourceFolder url=&quot;file://$MODULE_DIR$/conf/dev&quot; type=&quot;java-resource&quot; /&gt; &lt;sourceFolder url=&quot;file://$MODULE_DIR$/src/main/java&quot; isTestSource=&quot;false&quot; /&gt; &lt;sourceFolder url=&quot;file://$MODULE_DIR$/src/main/resources&quot; type=&quot;java-resource&quot; /&gt; // ##就是这条## &lt;sourceFolder url=&quot;file://$MODULE_DIR$/src/test/java&quot; isTestSource=&quot;true&quot; /&gt; &lt;excludeFolder url=&quot;file://$MODULE_DIR$/target&quot; /&gt; &lt;/content&gt; ...... 完。","categories":[{"name":"其他","slug":"others","permalink":"http://wuzguo.com/blog/categories/others/"}],"tags":[{"name":"Intellij IDEA","slug":"Intellij-IDEA","permalink":"http://wuzguo.com/blog/tags/Intellij-IDEA/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://wuzguo.com/blog/tags/SpringBoot/"}],"author":"Zak"},{"title":"Angular5踩坑记录之编译出现'JavaScript Heap Out of Memory'的错误","slug":"front/angular5_records_2","date":"2018-03-05T09:40:42.000Z","updated":"2022-08-01T06:33:30.286Z","comments":true,"path":"2018/03/05/front/angular5_records_2.html","link":"","permalink":"http://wuzguo.com/blog/2018/03/05/front/angular5_records_2.html","excerpt":"","text":"在Angular5项目中执行 &#39;ng build --prod&#39; 编译打包时出现以下错误： 解决方案： 扩大node的heap空间，设置方式如下： node --max_old_space_size=8192 node_modules/@angular/cli/bin/ng build --prod 可以直接在项目的package.json中定义命令： &quot;prod&quot;: &quot;node --max_old_space_size=8192 node_modules/@angular/cli/bin/ng build --prod --build-optimizer=true --vendor-chunk=true&quot; 然后打包的时候执行 npm run prod 命令即可。","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"Angular","slug":"Angular","permalink":"http://wuzguo.com/blog/tags/Angular/"},{"name":"Angular5","slug":"Angular5","permalink":"http://wuzguo.com/blog/tags/Angular5/"},{"name":"heap out of memory","slug":"heap-out-of-memory","permalink":"http://wuzguo.com/blog/tags/heap-out-of-memory/"}],"author":"Zak"},{"title":"Angular5踩坑记录之'undefined Is Not a Function'的错误","slug":"front/angular5_records_1","date":"2018-03-05T08:38:06.000Z","updated":"2022-08-01T06:35:23.752Z","comments":true,"path":"2018/03/05/front/angular5_records_1.html","link":"","permalink":"http://wuzguo.com/blog/2018/03/05/front/angular5_records_1.html","excerpt":"","text":"最近在使用Angular5+Ant Desigin开发公司后台管理系统，使用的Angular 5.2.3版本，当把Angular版本升级到5.2.7的时候之前运行的很好的项目突然不能运行了，在控制台包了一个TypeError: undefined is not a function的错误，如下所示： 苦思冥想不得其解遂百度，找到一个博客：https://segmentfault.com/q/1010000013355994 得到答案，是应为我的懒加载的配置有问题，之前配置的有问题可能是Angular版本升级以后对这块功能进行了升级加强导致不能使用了，解决方案就是检查项目的懒加载配置，我的解决方式如下： 在app.module.ts中去掉AdminModule的引用，因为AdminModule是懒加载的，不需要直接引用进来。 在routes.module.ts中去掉 PagesModule、ChartsModule 、DashboardModule 模块，因为这三个模块我也做了懒加载。 所以解决方案就是：懒加载模块不要直接放在 imports中导入，否则会出现 TypeError: undefined is not a function 的错误。","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"Angular","slug":"Angular","permalink":"http://wuzguo.com/blog/tags/Angular/"},{"name":"Angular5","slug":"Angular5","permalink":"http://wuzguo.com/blog/tags/Angular5/"},{"name":"undefined","slug":"undefined","permalink":"http://wuzguo.com/blog/tags/undefined/"}],"author":"Zak"},{"title":"AngularJS与服务端交互","slug":"front/angularjs_http","date":"2017-09-13T13:40:12.000Z","updated":"2022-08-01T06:33:30.164Z","comments":true,"path":"2017/09/13/front/angularjs_http.html","link":"","permalink":"http://wuzguo.com/blog/2017/09/13/front/angularjs_http.html","excerpt":"","text":"一、$http 服务在AngularJS中与远程HTTP服务器交互时会用一个非常关键的服务$http。$http是对原生的XMLHttpRequest对象的封装。$http的各种方式的请求更趋近于Rest风格。 二、参数说明以下是$http服务的使用方式： $http(config) .success( function (data, status, headers, config) &#123; // ... &#125;) .error( function (data, status, headers, config) &#123; // ... &#125;); 参数注释： config 参数为一个json对象，如： &#123;url:&quot;login.do&quot;,method:&quot;post&quot;,data:&#123;name:&quot;12346&quot;,pwd:&quot;123&quot;&#125;&#125; config中可以设置如下参数： method：请求方式，如：GET、POST、PUT、head、delete、jsonp。 url：请求的URL地址。 params：请求参数，将在URL上被拼接成?key=value ，健值对格式。 data：数据，将被放入请求中发送至服务端，健值对格式。 cache：若为true，在http中GET请求时采用默认的 $http cache，否则使用$cacheFactory的实例 timeout： 设置数值或promise对象，如果是数值，则延迟指定的毫秒数在发送请求，如果是promise，那么如果promise对象被查询数据时resolve()，则当前请求会被终止。 withCredentials：设置布尔值，默认情况下CORS请求不会发送cookie，所以如果设置为true，则就会将目标域的cookie包含在请求中。 xsrfHeaderName：cross site request forgery（跨站请求伪造名）。 xsrfCookieName：cross site request （跨站请求伪造cookieName）。 responseType：指定客户端接受数据的类型字符串，分别可以设置以下值： “” arraybuffer blob document（http文档） json text moz-blob(Firefox接收进度事件） moz-chunked-text(文本流） moz-chunked-arraybuffer(ArrayBuffer流） success为请求成功后的回调函数，这里主要是对返回的四个参数进行说明。 data 响应体，包含服务端返回的数据。 status Http相应的状态值。 headers 获取相应Header数据的函数。 ​ 如获取跨域属性配置的值： console.log(headers(&#39;Access-Control-Allow-Origin&#39;)); config 请求中的config对象，同上。 error为请求失败后的回调函数，参数同success相同。 为了方便大家与HTTP服务器进行交互，AngularJS提供了各个请求方式的简写方法： $http.put/post(url,data,config); ，其中 url、data必填，config 可选。 $http.get/delete/jsonp/head(url,config); ， 其中url必填，config可选。 三、then方法1. 使用方式​ AngularJS 可以使用 then() 方法来处理 $http 服务的回调，then() 方法接受两个可选的函数作为参数，表示请求成功（success）和发生错误（error）状态时的处理： promise.then(successFn, errFn, notifyFn); ​ 无论请求成功与否，当结果可用之后都会立刻异步调用successFn或者errFn。这些方法只传递一个参数来调用回调函数，参数对象中包含相应体、状态码、相应头等信息。 ​ 在promise被执行时， notifyFn回调可能会被调用 0 到多次，以提供过程状态的提示。 promise.then( function(resp)&#123; //响应成功时调用，resp是一个响应对象 &#125;, function(resp) &#123; // 响应失败时调用，resp带有错误信息 &#125;); then() 方法接收的响应对象（resp）包含5个属性： data：（字符串或对象），响应体。 status：相应http的状态码,如200。 headers(函数)：头信息的Getter函数，可以接受一个参数，用来获取对应名字的值。 config(对象)：生成原始请求的完整设置对象。 statusText：相应的http状态文本，如 “ok“。 2. 应用场景上面介绍了两种获取请求结果的方式：success()和then()。 success() 就是典型的回调嵌套，代码格式不够美观，而且当出现多级嵌套的时候容易出错。 then()方法就清晰很多且性能较好，适合在链式编程的场景中使用。 在高版本的AngularJS中已经弃用success()的处理方式。 四、客户端缓存当服务端数据很少修改时，可是使用客户端缓存提高请求相应速度，提升客户体验。在AngularJS中可以通过以下两种方式实现缓存： 设置cache字段为true $http.get(&#x27;data/users.json&#x27;,&#123;cache:true&#125;).success(function (data) &#123; $scope.users = data.users;&#125;); 自定义缓存对象，通过$cacheFactory服务获取实例 var app = angular.module(&#x27;myApp&#x27;,[]);//创建MyController，测试$http如何与服务器交互app.controller(&#x27;MyController&#x27;, function ($scope,$http,$cacheFactory) &#123; var lru; $scope.getUsers = function () &#123; if (!lru) &#123; lru = $cacheFactory(&#x27;lru&#x27;, &#123;capacity: 20&#125;); &#125; $http.get(&#x27;data/users.json&#x27;, &#123;cache: lru&#125;).success(function (data) &#123; $scope.users = data.users; &#125;); &#125;&#125;); 缓存对象可使用的方法： info() 获取缓存对象信息。 put(key, value) 存放数据。 get(key) 获取数据。 remove(key) 删除对应key的项。 removeAll() 删除所有项。 destroy() 销毁缓存。 $http.get(&#x27;data/users.json&#x27;, &#123;cache: lru&#125;).success(function (data) &#123; $scope.users = data.users; var info = lru.info(); console.info(info); lru.put(&#x27;2&#x27;, &quot;Tom&quot;);&#125;); 五、全局配置AngularJS可以设置$httpProvider完成http请求的全局配置，可以设置以下属性： 修改AngularJS默认的Content-Type值（application&#x2F;json）。 设置默认缓存。 拦截器：在任何请求发送给服务器之前或者从服务器返回时可以对其拦截，从而为请求添加全局的功能。 场景：身份验证、错误处理等。 拦截器的类型：request、response、requestError、responseError。 创建拦截器：1、创建拦截器工厂服务；2、注册该服务。 代码如下： var app = angular.module(&#x27;myApp&#x27;,[]);//声明一个拦截器工厂服务app.factory(&#x27;myInterceptor&#x27;, function () &#123; //此处可以设置4个属性：request、response、requestError、responseError return &#123; request: function (config) &#123; //可以修改配置对象 config.headers.customInfo=&#x27;Tom&#x27;; console.log(config); return config; &#125;, response: function (config) &#123; console.log(config); return config; &#125;, requestError: function (rejection) &#123; console.log(rejection); return rejection; &#125;, responseError: function (rejection) &#123; console.log(rejection); return rejection; &#125; &#125;&#125;);app.config(function ($httpProvider) &#123; //修改默认Content-Type为url-encoded $httpProvider.defaults.headers.post[&#x27;Content-Type&#x27;]=&#x27;application/x-www.form-urlencoded;charset=utf-8&#x27;; $httpProvider.defaults.headers.put[&#x27;Content-Type&#x27;]=&#x27;application/x-www.form-urlencoded;charset=utf-8&#x27;; //设置默认缓存 $httpProvider.defaults.cache= true; //注册一个拦截器 $httpProvider.interceptors.push(&#x27;myInterceptor&#x27;);&#125;); 六、RESTful类型API调用RESTful类型API调用：Representational State Transfer（表征状态转移）。AngularJS提供$resource服务可以同支持RESTful服务器端数据源进行交互。 1. 使用 $resource 引入angular-resource.js 添加依赖 &#39;ngResource&#39; 获取资源实例：$resource(‘data/:name.json’, &#123;name:’@username’&#125;) 。 基于GET的HTTP方法： get(params, successFn, errorFn) （get方法可以不需要设置回调函数）。 query(params, successFn, errorFn) 。 get和query的唯一不同是AngularJS期望query返回的是数组。 var app = angular.module(&#x27;myApp&#x27;,[&#x27;ngResource&#x27;]);//创建控制器获取用户Tom的数据并显示//此处需要注入$resource，通过RESTful的形式查询数据app.controller(&#x27;MyController&#x27;, function ($scope,$resource) &#123; //获取资源实例 var User = $resource(&#x27;data/:name.json&#x27;,&#123;name:&#x27;@username&#x27;&#125;); //可以直接使用返回结果，好像是一个同步调用 $scope.user = User.get(&#123;username:&#x27;tom&#x27;&#125;); //query方法的使用，它和get方法基本一致，唯一一点不同时AngularJS期待返回结果为数组 User.query(&#123;name:&#x27;users&#x27;&#125;, function (result) &#123; console.log(result); &#125;);&#125;); 2. 配置 $resource 自定义 $resource 方法：设置 $resource 第三个参数对象。$resource提供了五种默认操作：get, query, save, remove, delete。你可以配置一个update操作来完成HTTP PUT： $resource(url,&#123;&#125;,&#123; &#x27;update&#x27;:&#123;method:&#x27;PUT&#x27;&#125;&#125;); 附加属性：资源有两个属性可以同底层数据定义交互 $promise： 为$resource生成的原始promise对象，特别用来同$routeProvider.when()在 resolve 时进行连接。$resolved ：服务器首次响应时被设置为 true。 console.info(User.$promise);console.info(User.$resolved); 配置对象： $resource(url, &#123;&#125;, &#123; &#x27;update&#x27;:&#123; method:&#x27;PUT&#x27; &#125;, &#x27;get&#x27;:&#123; isArray:true, //以数组形式返回结果，等同于query interceptor：&#123; //设置拦截器，此处只能设置response和responseError response:function()&#123; &#125;, responseError:function()&#123; &#125; &#125; &#125;&#125;); 七、使用Restangular1. 特点Restangular通过完全不同的途径实现了XHR通信，弥补了$http和$resource内置服务的缺点，主要优点如下： 支持promise：使用起来符合angularjs习惯并支持链式操作。 promise展开：可以同时操作promise对象。 简单明了的API。 全HTTP方法的支持。 忘记URL：不需要提前指定URL（除了基础URL以外）。 资源嵌套：Restangular可以直接处理嵌套资源，无需创建新的实例。 单例：使用过程中仅需创建一个Restangular资源对象实例。 2. 安装使用 引入Restangular：https://github.com/mgonto/restangular 引入Lo-Dash(或underscore)：https://raw.github.com/lodash/lodash/3.10.1/lodash.min.js 添加依赖，并且注入服务Restangular 。 创建Restangular主对象： Restangular.all(&#x27;accounts&#x27;); //url: /accounts/Restangular.one(&#x27;accounts&#x27;, 1); //url: /accounts/1Restangular.several(&#x27;accounts&#x27;, 1, 2, 3); GET方法使用 var baseAccounts = Restangular.all(&#x27;accounts&#x27;);baseAccounts.getList().then(function(accounts)&#123;...&#125;); scope.accouts = Restangular.all(&#x27;accounts&#x27;).getList().object; //GET - ONE /account/1?p=1Restangular.one(&#x27;accounts&#x27;,1).get(&#123;p:1&#125;).then(function(account)&#123; $scope.curAccount = account;&#125;); POST方法使用 baseAccounts.post(newAccount); PUT方法使用 // UPDATE to /account/1// 前面GET方法使用的时候有赋值：$scope.curAccount = account;$scope.curAccount.name = &#x27;Jerry&#x27;;$scope.curAccount.put(); DELETE方法使用 // DELETE to /account/1// 前面GET方法使用的时候有赋值：$scope.curAccount = account;$scope.curAccount.remove(); 八、参考博客 AngularJS之$http与服务器交互： http://www.cnblogs.com/sytsyt/p/3297872.html AngularJS的$http服务的应用： http://www.cnblogs.com/whh412/p/5644458.html ​","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"AngularJS","slug":"AngularJS","permalink":"http://wuzguo.com/blog/tags/AngularJS/"},{"name":"$http","slug":"http","permalink":"http://wuzguo.com/blog/tags/http/"},{"name":"$resource","slug":"resource","permalink":"http://wuzguo.com/blog/tags/resource/"},{"name":"Restangular","slug":"Restangular","permalink":"http://wuzguo.com/blog/tags/Restangular/"}],"author":"Zak"},{"title":"AngularJS 控制器","slug":"front/angularjs_controllers","date":"2017-09-09T12:17:06.000Z","updated":"2022-08-01T06:33:29.939Z","comments":true,"path":"2017/09/09/front/angularjs_controllers.html","link":"","permalink":"http://wuzguo.com/blog/2017/09/09/front/angularjs_controllers.html","excerpt":"","text":"一、定义​ AngularJS 控制器控制 AngularJS 应用程序的数据。 AngularJS 控制器是常规的 JavaScript 对象，由标准的 JavaScript 对象的构造函数 创建。ng-controller 指令定义了应用程序控制器。 在AngularJS中可以通过以下格式自定义控制器： module.controller(name,controllerFactory) 或者 $controllerProvider.register(name,controllerFactory); 二、使用实例​ AngularJS 应用程序由 ng-app 定义。ng-controller=&quot;myCtrl&quot; 属性是一个 AngularJS 指令。用于定义一个控制器。myCtrl 函数是一个 JavaScript 函数。AngularJS 使用$scope 对象来调用控制器。 ​ 在 AngularJS 中， $scope 是一个应用对象（属于应用变量和函数）。控制器的 $scope （相当于作用域、控制范围）用来保存AngularJS Model（模型）的对象。 &lt;div ng-app=&quot;myApp&quot; ng-controller=&quot;myCtrl&quot;&gt; 名: &lt;input type=&quot;text&quot; ng-model=&quot;firstName&quot;&gt;&lt;br&gt; 姓: &lt;input type=&quot;text&quot; ng-model=&quot;lastName&quot;&gt;&lt;br&gt; &lt;br&gt; 姓名: &#123;&#123;firstName + &quot; &quot; + lastName&#125;&#125;&lt;/div&gt;&lt;script&gt;var app = angular.module(&#x27;myApp&#x27;, []);app.controller(&#x27;myCtrl&#x27;, function($scope) &#123; $scope.firstName = &quot;John&quot;; $scope.lastName = &quot;Doe&quot;;&#125;);&lt;/script&gt; 控制器也可以有方法（变量和函数）： &lt;div ng-app=&quot;myApp&quot; ng-controller=&quot;personCtrl&quot;&gt;名: &lt;input type=&quot;text&quot; ng-model=&quot;firstName&quot;&gt;&lt;br&gt;姓: &lt;input type=&quot;text&quot; ng-model=&quot;lastName&quot;&gt;&lt;br&gt;&lt;br&gt;姓名: &#123;&#123;fullName()&#125;&#125;&lt;/div&gt;&lt;script&gt;var app = angular.module(&#x27;myApp&#x27;, []);app.controller(&#x27;personCtrl&#x27;, function($scope) &#123; $scope.firstName = &quot;John&quot;; $scope.lastName = &quot;Doe&quot;; $scope.fullName = function() &#123; return $scope.firstName + &quot; &quot; + $scope.lastName; &#125;&#125;);&lt;/script&gt; 三、依赖注入AngularJS中控制器使用的对象可以通过显示注入和隐式注入两种方式进行，隐式依赖注入在代码发布过程中经过某些压缩工具压缩后可能不能正常使用，一般情况下建议使用显示注入的方式。实例如下： 隐式注入 var myApp = angular.module(&#x27;myApp&#x27;, []) .factory(&#x27;CustomService&#x27;, [&#x27;$window&#x27;, function (a) &#123; console.log(a); &#125;]) // 隐示的依赖注入 .controller(&#x27;firstController&#x27;, function ($scope, CustomService) &#123; console.log(CustomService); &#125;); 显示注入（建议使用） var myApp = angular.module(&#x27;myApp&#x27;, []) // 显示的依赖注入 .controller(&#x27;secondController&#x27;, [&#x27;$scope&#x27;, &#x27;$filter&#x27;, function (a, b) &#123; console.log(b(&#x27;json&#x27;)([1, 2, 3, 4, 5])); &#125;]); 显示依赖注入第二种方式，一般不建议使用： function otherController(a) &#123; console.log(a);&#125;otherController.$inject = [&#x27;$scope&#x27;]; 然后在HTML中调用控制器： &lt;div ng-app=&quot;myApp&quot;&gt; &lt;div ng-controller=&quot;secondController&quot;&gt; &lt;/div&gt; &lt;div ng-controller=&quot;otherController&quot;&gt; &lt;/div&gt;&lt;/div&gt; 四、正确使用​ Controller不应该尝试做太多的事情。它应该仅仅包含单个视图所需要的业务逻辑，保持Controller的简单性，常见办法是抽出那些不属于Controller的工作到service中，在Controller通过依赖注入来使用这些 service。 不要在Controller中做以下的事情： 任何类型的DOM操作，Controller应该仅仅包含业务逻辑，任何表现逻辑放到Controller中，大大地影响了应用逻辑的可测试性。angular为了自动操作（更新）DOM，提供的数据绑定。如果希望执行我们自定义的DOM操作，可以把表现逻辑抽取到directive中。 Input formatting（输入格式化），使用angular form controls 代替。 Output filtering （输出格式化过滤），使用angular filters 代替。 执行无状态或有状态的、controller共享的代码，使用angular services 代替。 实例化或者管理其他组件的生命周期（例如创建一个服务实例）。 五、参考博客 kittencup 的AngularJS教学视频 http://kittencup.com/","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"AngularJS","slug":"AngularJS","permalink":"http://wuzguo.com/blog/tags/AngularJS/"},{"name":"控制器","slug":"控制器","permalink":"http://wuzguo.com/blog/tags/%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"name":"Controller","slug":"Controller","permalink":"http://wuzguo.com/blog/tags/Controller/"}],"author":"Zak"},{"title":"AngularJS的模块和服务","slug":"front/angularjs_modules_services","date":"2017-09-09T08:56:06.000Z","updated":"2022-08-01T06:33:30.122Z","comments":true,"path":"2017/09/09/front/angularjs_modules_services.html","link":"","permalink":"http://wuzguo.com/blog/2017/09/09/front/angularjs_modules_services.html","excerpt":"","text":"一、模块1. 什么是模块​ 大部分应用都有一个主方法main用来实例化、组织、启动应用。AngularJS应用没有主方法，而是使用模块来声明应用应该如何启动。模块允许通过声明的方式来描述应用中的依赖关系，以及如何进行组装和启动。 2. Angular 的模块​ 模块是组织业务的封装，在一个模块当中定义多个服务。当引入了一个模块的时候，就可以使用这个模块提供的一种或多种服务了。AngularJS 本身的一个默认模块叫做 ng ，它提供了 $http ， $scope等等服务。服务只是模块提供的多种机制中的一种，其它的还有指令directive ，过滤器filter，及其它配置信息。 ​ 也可以在已有的模块中新定义一个服务，也可以先新定义一个模块，然后在新模块中定义新服务。服务是需要显式地的声明依赖（引入）关系的，让 ng 自动地做注入。 3. 模块的优点 启动过程是声明式的，更容易懂。 在单元测试是不需要加载全部模块的，因此这种方式有助于写单元测试。 可以在特定情况的测试中增加额外的模块，这些模块能更改配置，能帮助进行端对端的测试。 第三方代码可以作为可复用的module打包到angular中。 模块可以以任何先后或者并行的顺序加载（因为模块的执行本身是延迟的）。 4. 启动​ 通过ng-app指定对应的模块应用启动。 5. 定义模块angular.module(name[, requires],configFn); configFn 会在模块初始化时执行，可以在里配置模块的服务。 二、服务1. 概念​ 服务本身是一个任意的对象。 ​ AngularJS通过依赖注入机制来使用服务。用 $provider 对象来实现自动依赖注入机制，注入机制通过调用一个provider 的 $get() 方法，把得到的对象作为参数进行相关调用。 var myApp = angular.module(&#x27;myApp&#x27;,[],function($provide)&#123; // 自定义服务 $provide.provider(&#x27;CustomProvider&#x27;,function()&#123; this.$get = function()&#123; return &#123; message : &#x27;CustomService Message&#x27; &#125; &#125; &#125;); // 自定义工厂 $provide.factory(&#x27;CustomFactory&#x27;,function()&#123; return [1,2,3,4,5,6,7]; &#125;); // 自定义服务 $provide.service(&#x27;CustomService&#x27;,function()&#123; this.name = &quot;service&quot;; this.func = function (x) &#123; return x.toString(16);//转16进制 &#125; &#125;)&#125;); 2. 模块的定义 $provider.provider provider是唯一一种可以传进.config() 函数的 service。当你想要在service对象启用之前，先进行模块范围的配置，那就应该用provider。 需要注意的是：在config 函数里注入 provider 时，名字应该是：providerName+Provider。使用 provider 的优点就是，你可以在 provider 对象传递到应用程序的其他部分之前在app.config函数中对其进行修改。 当你使用provider 创建一个 service 时，唯一的可以在你的控制器中访问的属性和方法是通过$get()函数返回内容。 &lt;body&gt;&lt;div ng-app=&quot;myApp&quot; ng-controller=&quot;myCtrl&quot;&gt;&lt;/div&gt;&lt;script&gt; var app = angular.module(&#x27;myApp&#x27;, []); //需要注意的是：在注入provider时，名字应该是：providerName+Provider app.config(function(myProviderProvider)&#123; myProviderProvider.setName(&quot;大圣&quot;); &#125;); app.provider(&#x27;myProvider&#x27;, function() &#123; var name=&quot;&quot;; var test=&#123;&quot;a&quot;:1,&quot;b&quot;:2&#125;; //注意的是，setter方法必须是(set+变量首字母大写)格式 this.setName = function(newName)&#123; name = newName &#125; this.$get = function($http,$q)&#123; return &#123; getData : function()&#123; var d = $q.defer(); $http.get(&quot;url&quot;) //读取数据的函数。 .success(function(response) &#123; d.resolve(response); &#125;) .error(function()&#123; d.reject(&quot;error&quot;); &#125;); return d.promise; &#125;, &quot;lastName&quot;:name, &quot;test&quot;:test &#125; &#125; &#125;); app.controller(&#x27;myCtrl&#x27;, function($scope,myProvider) &#123; alert(myProvider.lastName); alert(myProvider.test.a) myProvider.getData().then(function(data)&#123; //alert(data) &#125;,function(data)&#123; //alert(data) &#125;); &#125;);&lt;/script&gt;&lt;/body&gt; $provider.factory factory是直接把一个函数当成一个对象的$get方法，可以直接返回字符串。用factory就是创建一个对象，为它添加属性，然后把这个对象返回出来。你把service传进controller之后，在controller里这个对象里的属性就可以通过factory使用了。 &lt;body&gt;&lt;div ng-app=&quot;myApp&quot; ng-controller=&quot;myCtrl&quot;&gt; &lt;p&gt;&#123;&#123;r&#125;&#125;&lt;/p&gt;&lt;/div&gt;&lt;script&gt; //创建模型 var app = angular.module(&#x27;myApp&#x27;, []); //通过工厂模式创建自定义服务 app.factory(&#x27;myFactory&#x27;, function() &#123; var service = &#123;&#125;;//定义一个Object对象&#x27; service.name = &quot;张三&quot;; var age;//定义一个私有化的变量 //对私有属性写getter和setter方法 service.setAge = function(newAge)&#123; age = newAge; &#125; service.getAge = function()&#123; return age; &#125; return service;//返回这个Object对象 &#125;); //创建控制器 app.controller(&#x27;myCtrl&#x27;, function($scope, myFactory) &#123; myFactory.setAge(20); $scope.r =myFactory.getAge(); alert(myFactory.name); &#125;);&lt;/script&gt;&lt;/body&gt; $provider.service service是用new关键字实例化的。因此，你应该给this添加属性，然后service返回this。你把service传进controller之后，在controller里this上的属性就可以通过service来使用了。 &lt;div ng-app=&quot;myApp&quot; ng-controller=&quot;myCtrl&quot;&gt; &lt;h1&gt;&#123;&#123;r&#125;&#125;&lt;/h1&gt;&lt;/div&gt;&lt;script&gt; var app = angular.module(&#x27;myApp&#x27;, []); // 注意这里没有返回值 app.service(&#x27;myService&#x27;, function($http,$q) &#123; this.name = &quot;service&quot;; this.myFunc = function (x) &#123; return x.toString(16);//转16进制 &#125; this.getData = function()&#123; var d = $q.defer(); $http.get(&quot;ursl&quot;)//读取数据的函数。 .success(function(response) &#123; d.resolve(response); &#125;) .error(function()&#123; alert(0) d.reject(&quot;error&quot;); &#125;); return d.promise; &#125; &#125;); app.controller(&#x27;myCtrl&#x27;, function($scope, myService) &#123; $scope.r = myService.myFunc(255); myService.getData().then(function(data)&#123; console.log(data);//正确时走这儿 &#125;,function(data)&#123; alert(data)//错误时走这儿 &#125;); &#125;);&lt;/script&gt; 三、包装器包装器decorator：$provide服务提供了在服务实例创建时对其进行拦截的功能，可以对服务进行扩展，甚至完全替代它。常见使用场景： 对服务进行扩展。 对服务封装以便开发时调试和跟踪。 使用方式：$provide.decorator(&#39;UserService&#39;,fn) 。 var app = angular.module(&#x27;myApp&#x27;,[]);//创建一个provider类型的服务app.provider(&#x27;UserService&#x27;, &#123; //默认url url:&#x27;users.json&#x27;, //添加setUrl方法提供修改url的方法 setUrl: function (newUrl) &#123; this.url = newUrl; &#125;, $get: function ($http) &#123;//可以在此处注入其他服务 var currentUser = &#123;&#125;; var self = this;//将外层引用保存下来 return &#123; getCurrentUser:function() &#123; //通过$http请求数据 return $http.get(self.url); &#125;, setCurrentUser:function(user) &#123; currentUser = user; &#125; &#125; &#125;&#125;);//可以通过config函数配置provider类型的服务//此处要对UserService进行包装，因此我们在config函数中注入$provideapp.config(function ($provide, UserServiceProvider) &#123; MyUserServiceProvider.setUrl(&#x27;data/users.json&#x27;); //对UserService包装,第一个参数指定为拦截服务的名称 //此处需要在构造函数中注入$delegate //因为需要输出控制台信息，所以注入$log $provide.decorator(&#x27;UserService&#x27;, function ($delegate, $log) &#123; return &#123; delCurrentUser: function () &#123; //执行所拦截服务之前的方法，这里返回的是Promise对象 var p = $delegate.getCurrentUser(); //额外做些事情：记录一下该次请求所消耗时间 //首先记录请求发起时间 var start = new Date(); p.finally(function () &#123; $log.info(&#x27;执行[getCurrentUser]方法消耗时间：&#x27;+(new Date()-start)+&#x27; 毫秒&#x27;); &#125;); return p; &#125; &#125; &#125;)&#125;); 四、参考博客 自定义服务详解（factory、service、provider）http://blog.csdn.net/zcl_love_wx&#x2F;article&#x2F;details&#x2F;51404390) kittencup 的AngularJS教学视频 http://kittencup.com/","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"AngularJS - 服务","slug":"AngularJS-服务","permalink":"http://wuzguo.com/blog/tags/AngularJS-%E6%9C%8D%E5%8A%A1/"},{"name":"Service","slug":"Service","permalink":"http://wuzguo.com/blog/tags/Service/"}],"author":"Zak"},{"title":"AngularJS的过滤器","slug":"front/angularjs_filters","date":"2017-09-09T06:23:54.000Z","updated":"2022-08-01T06:33:30.222Z","comments":true,"path":"2017/09/09/front/angularjs_filters.html","link":"","permalink":"http://wuzguo.com/blog/2017/09/09/front/angularjs_filters.html","excerpt":"","text":"一、定义过滤器用于对数据的格式化，或者筛选的函数，可以直接在模板中通过一种语法使用： // 单个过滤器&#123;&#123; expression | filter &#125;&#125;// 多个过滤器共同使用&#123;&#123; expression | filter1 | filter2 &#125;&#125;// 过滤器中传递参数&#123;&#123; expression | filter1:param,….&#125;&#125; 二、内置的过滤器AngularJS中常用的内置过滤器有如下： 滤器 描述 currency 格式化数字为货币格式。 filter 从数组项中选择一个子集。 lowercase 格式化字符串为小写。 orderBy 根据某个表达式排列数组。 uppercase 格式化字符串为大写。 number 格式化数字。 json 格式化json对象。 limitTo 限制数组长度或字符串长度。 date 格式化日期。 使用实例： angular.module(&#x27;myApp&#x27;, []) .factory(&#x27;DefaultData&#x27;, function () &#123; return &#123; message: &#x27;Hello World&#x27;, city: [ &#123; name: &#x27;上海11212&#x27;, py: &#x27;shanghai&#x27; &#125;, &#123; name: &#x27;北京&#x27;, py: &#x27;beijing&#x27; &#125;, &#123; name: &#x27;四川&#x27;, py: &#x27;sichuan&#x27; &#125; ] &#125;; &#125;) .controller(&#x27;firstController&#x27;, [&#x27;$scope&#x27;, &#x27;DefaultData&#x27;, &#x27;$filter&#x27;, function ($scope, data, $filter) &#123; $scope.today = new Date; $scope.data = data; // 过滤器 var number = $filter(&#x27;number&#x27;)(3000); var strJson = $filter(&#x27;json&#x27;)($scope.data); console.log(strJson); console.log($scope.data); $scope.checkName = function (obj) &#123; if (obj.py.indexOf(&#x27;h&#x27;) === -1) return false; return true; &#125; &#125;]); &lt;div ng-controller=&quot;firstController&quot;&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt;&#123;&#123;123456789 | number&#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123;12345.6789 | number:3&#125;&#125;&lt;/p&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt;&#123;&#123;999999 | currency&#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123;999999 | currency:&#x27;rmb&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt;default:&#123;&#123; today &#125;&#125;&lt;/p&gt; &lt;p&gt;medium: &#123;&#123; today | date:&#x27;medium&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;short:&#123;&#123; today | date:&#x27;short&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;fullDate:&#123;&#123; today | date:&#x27;fullDate&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;longDate:&#123;&#123; today | date:&#x27;longDate&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;mediumDate:&#123;&#123; today | date:&#x27;mediumDate&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;shortDate:&#123;&#123; today | date:&#x27;shortDate&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;mediumTime:&#123;&#123; today | date:&#x27;mediumTime&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;shortTime:&#123;&#123; today | date:&#x27;shortTime&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt; year: &#123;&#123;today | date : &#x27;y&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;yy&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;yyyy&#x27;&#125;&#125; &lt;/p&gt; &lt;p&gt; month: &#123;&#123;today | date : &#x27;M&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;MM&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;MMM&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;MMMM&#x27;&#125;&#125; &lt;/p&gt; &lt;p&gt; day: &#123;&#123;today | date : &#x27;d&#x27;&#125;&#125; Day in month &#123;&#123;today | date : &#x27;dd&#x27;&#125;&#125; Day in week &#123;&#123;today | date : &#x27;EEEE&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;EEE&#x27;&#125;&#125; &lt;/p&gt; &lt;p&gt; hour: &#123;&#123;today | date : &#x27;HH&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;H&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;hh&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;h&#x27;&#125;&#125; &lt;/p&gt; &lt;p&gt; minute: &#123;&#123;today | date : &#x27;mm&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;m&#x27;&#125;&#125; &lt;/p&gt; &lt;p&gt; second: &#123;&#123;today | date : &#x27;ss&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;s&#x27;&#125;&#125; &#123;&#123;today | date : &#x27;.sss&#x27;&#125;&#125; &lt;/p&gt; &lt;p&gt;&#123;&#123;today | date : &#x27;y-MM-d H:m:s&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt;&#123;&#123;[1,2,3,4,5,6,7] | limitTo:5&#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123;[1,2,3,4,5,6,7] | limitTo:-5&#125;&#125;&lt;/p&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt;&#123;&#123;data.message | lowercase&#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123;data.message | uppercase&#125;&#125;&lt;/p&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt;&#123;&#123; data.city | filter : &#x27;上海&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123; data.city | filter : &#x27;name&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123; data.city | filter : &#123;py:&#x27;g&#x27;&#125; &#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123; data.city | filter : checkName &#125;&#125;&lt;/p&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;!-- 默认顺序是 正序 asc a~z --&gt; &lt;p&gt;&#123;&#123; data.city | orderBy : &#x27;py&#x27;&#125;&#125; &lt;/p&gt; &lt;!-- 默认顺序是 反序 desc z~a --&gt; &lt;p&gt;&#123;&#123; data.city | orderBy : &#x27;-py&#x27;&#125;&#125;&lt;/p&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;/div&gt; 执行结果： ---------------------------------------------------------123,456,78912,345.679---------------------------------------------------------$999,999.00rmb999,999.00---------------------------------------------------------default:&quot;2017-09-09T05:51:07.808Z&quot;medium: Sep 9, 2017 1:51:07 PMshort:9/9/17 1:51 PMfullDate:Saturday, September 9, 2017longDate:September 9, 2017mediumDate:Sep 9, 2017shortDate:9/9/17mediumTime:1:51:07 PMshortTime:1:51 PM---------------------------------------------------------year: 2017 17 2017month: 9 09 Sep Septemberday: 9 Day in month 09 Day in week Saturday Sathour: 13 13 01 1minute: 51 51second: 07 7 .8082017-09-9 13:51:7---------------------------------------------------------[1,2,3,4,5][3,4,5,6,7]---------------------------------------------------------hello worldHELLO WORLD---------------------------------------------------------[&#123;&quot;name&quot;:&quot;上海11212&quot;,&quot;py&quot;:&quot;shanghai&quot;&#125;][][&#123;&quot;name&quot;:&quot;上海11212&quot;,&quot;py&quot;:&quot;shanghai&quot;&#125;,&#123;&quot;name&quot;:&quot;北京&quot;,&quot;py&quot;:&quot;beijing&quot;&#125;][&#123;&quot;name&quot;:&quot;上海11212&quot;,&quot;py&quot;:&quot;shanghai&quot;&#125;,&#123;&quot;name&quot;:&quot;四川&quot;,&quot;py&quot;:&quot;sichuan&quot;&#125;]---------------------------------------------------------[&#123;&quot;name&quot;:&quot;北京&quot;,&quot;py&quot;:&quot;beijing&quot;&#125;,&#123;&quot;name&quot;:&quot;上海11212&quot;,&quot;py&quot;:&quot;shanghai&quot;&#125;,&#123;&quot;name&quot;:&quot;四川&quot;,&quot;py&quot;:&quot;sichuan&quot;&#125;][&#123;&quot;name&quot;:&quot;四川&quot;,&quot;py&quot;:&quot;sichuan&quot;&#125;,&#123;&quot;name&quot;:&quot;上海11212&quot;,&quot;py&quot;:&quot;shanghai&quot;&#125;,&#123;&quot;name&quot;:&quot;北京&quot;,&quot;py&quot;:&quot;beijing&quot;&#125;]--------------------------------------------------------- 三、自定义过滤器在AngularJS中可以通过以下格式自定义过滤器：（过滤器必须返回一个Function） module.filter(name,filterFactory) 或者 $filterProvider.register(name,filterFactory); 如下通过不同的方式定义两个过滤器filterCity 和 filterAge ： var myApp = angular.module(&#x27;myApp&#x27;, [], function ($filterProvider) &#123; $filterProvider.register(&#x27;filterAge&#x27;, function () &#123; return function (obj) &#123; var newObj = []; angular.forEach(obj, function (o) &#123; if (o.age &gt; 20) &#123; newObj.push(o); &#125; &#125;); return newObj; &#125; &#125;);&#125;)// module.filter.filter(&#x27;filterCity&#x27;,function()&#123; return function(obj)&#123; var newObj = []; angular.forEach(obj, function (o) &#123; if (o.city === &#x27;上海&#x27;) &#123; newObj.push(o); &#125; &#125;); return newObj; &#125;&#125;); 四、参考博客 kittencup 的AngularJS教学视频 http://kittencup.com/","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"AngularJS - 过滤器","slug":"AngularJS-过滤器","permalink":"http://wuzguo.com/blog/tags/AngularJS-%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"name":"Filter","slug":"Filter","permalink":"http://wuzguo.com/blog/tags/Filter/"}],"author":"Zak"},{"title":"Upsource代码审查（评审）流程","slug":"tools/upsource_instructions_review_process","date":"2017-09-04T13:50:01.000Z","updated":"2022-08-01T06:35:23.790Z","comments":true,"path":"2017/09/04/tools/upsource_instructions_review_process.html","link":"","permalink":"http://wuzguo.com/blog/2017/09/04/tools/upsource_instructions_review_process.html","excerpt":"","text":"一、意义在Upsource中，一次代码审查可以包含多个Revision（修订）或者整个分支，当一次审查流程发起后它将成为一个独立的实体且具有唯一ID。允许审查流程中的成员针对特定的代码交换意见或者讨论修改策略。 代码审查具有以下意义： 可以借助团队的力量保持项目代码干净清晰，可读性强，不包含严重的缺陷。 发现其他人的代码中存在的问题，借助代码审查引起他的注意。 当某部分代码有重大改变时，可以通过审查让相关人员保持跟进状态，并根据改变作出相应。 有新成员加入时可以更好的了解代码逻辑修改的原因。 二、各个角色的定义 Author 通常是修订的提交者，代码审查即是来检查和观察他所提交的变更。 Reviewer Reviewer预计将检查作者提交的变更，并留下反馈。他可以讨论具体的行，或者添加一般性注释。他也可以完成审查，接受改变或提高对他们的关注。查看其他审查人员对审查工作流程的更改。 Watcher 预计不会参与审查过程的人，每当有重要的变化时，Watcher就不断更新项目的状态。请参阅由队友介绍的监视更改，以了解观察者的工作流程。 三、审查流程代码的修改者一般作为审查流程的发起者，如果要邀请团队成员其他成员进行代码审查，需要进行以下操作： 将更改内容提交到版本管理系统（Gitlab），在我们当前项目开发流程中这里可能需要发起MR。 创建一个审查流程（可以设置为系统自动发起）。 邀请审查参与者（Reviewer和可选的Watcher）。 讨论代码的问题，审查流程参与者交换意见。 Author解决问题，提交版本修订，并将新的修订添加到现有的评审中。（IDEA的Upsource插件支持自动添加） 当Reviewer接受你的修改后，便可以关闭审查流程。 以下详细讲解如何创建Upsource代码审查。 创建Review 登录upsource服务（http://www.wuzguo.com ），然后选中需要进行审查的项目。 单击项目名称打开，项目主页视图显示按时间顺序列出所有修订： 向下滚动列表，或者通过搜索功能找到对应的修订。 将光标移动到对应的修订条目，将出现一个Review changes链接: 单击Review changes，然后从弹出菜单中选择Create Review。一个新的评审被创建后，它将拥有一个全局唯一的ID显示在左上角（如：AEE-CR-7 ）。 邀请参与者 默认的审查标题与修订提交到VCS的内容相同，可以自定义编辑修改标题。在接下来的步骤中，需要指派至少一个Reviewer进行代码审查。 可以添加多个参与者，在审查流程发起后任何时刻也可以对审查人员就行增改，被添加的用户将通过电子邮件或 Upsource news feed （通知流）得到通知。 设置期限 您可以选择设置审查的最后期限提醒评审人员完成代码审查。（一般情况下管理员设置默认值） 参与讨论你可以发表评论或者向其他参与者发表意见，并回复其他人留下的评论。 Review进展在审稿人的图标下面有一个进度条，可以看到每个审查者的审查进度。鼠标悬停在它上面，看看有多少文件已经被审查者查看。 审查决议当审查人员接受（accept ）或关注（raise concern）变更时，他们的决议用相应的标志表示: 添加新修订如果最初提交的修订没有得到Reviewer的Accept，可以按照Reviewer的要求处理这些问题并提交代码的修订到gitlab，然后可以将一个或几个新的修订附加到原始审核中以获得Accept。 注意：如果增加的修订来自不同的提交者，那么所有这些提交者也会作为Author添加到审查流程中。这样整个讨论和修订的历史都包含在一个审查流程中，并且只要审查流程是打开（Open）的，在不同的修订过程中始终可以看到它。 当一个新的修订添加到审查流程的时候，你只看到那些修订的差异。如果想在评审中显示或隐藏任何额外的修订，可以使用修订选择器快速切换： 要从审查中移除修订，在review视图中单击修订名称旁边的detach图标。 在IDE中打开Review当我们在IDE（如：IDEA）中安装并配置Upsource插件时，可以通过点击这个链接在IDE中对应的tab中查看对应Review所包含的评论和修改并进行评审操作。 关闭评审一旦所有Reviewer都Accept了你的更改，您可以通过单击右侧窗格中的相应按钮来关闭这次评审流程。 Review 状态一个评审流程可以分为三个不同阶段: Pending ：评审流程被创建并且分配人员完成。 Completed： 所有审阅者通过接受（accept ）或关注（raise concern）变更的时候。 Closed ：由作者关闭的，不再需要参与者采取进一步的行动的评审流程。 删除Review不管你是因为失误创建了一个Review，还是想要清理过时的评论，不管它目前的状态是什么，你都可以删除它。 四、审查他人的变更如果你是审查人员，通常需要执行以下操作： 进入评审并检查变更。 留下你的反馈，参与讨论。 有选择地邀请其他参与者。 通过点击 （accept ）或（raise concern）来完成变更的评审。 检查修改当有人邀请你参加审查时，你会收到一封电子邮件的通知或者受到upsource的消息提醒，里面包含了对该评论的直接链接。点击链接可以进入评审页面，面板上的 **Review summary ** （概览）列出了与审查相关的所有事件和评论。 **Overview **选项卡下面显示更改的文件列表。单击文件名可以显示文件的修改内容。 您可以通过按下相应的按钮来进行差异比较或查看整个文件。这些视图可以让你更好的查看代码。按下 ESC按钮可以返回摘要视图（summary view）。 参与讨论你可以发表评论，向其他参与者发表意见，并回复其他人留下的评论。 邀请参与者如果你先让其他人参与评审流程，你可以根据实际情况增加Reviewer或Watcher。 你也可以在发布评论的时候添加Watcher，在文本框中输入@ 便可以选择对应的人员。你可以添加任意数量的参与者，被添加的用户将通过电子邮件和Upsource news feed得到通知。 选择修订当新的修订被添加到评审中，您只会看到这次修订的差异，并且可以继续进行评审，而不必再次检查以前提交的更改。如果您想在评审中显示或隐藏其他额外的修订，使用修订选择器可以快速切换修改。 完成审查如果你对作者的修改感到满意，那你就可以点击Accept。如果你觉得这些问题值得怀疑，那就点击Raise concern 。 五、监视审查过程作为一个观察者，您通常执行以下任务： 进入评审并定位更改。 阅读评论并有选择地参与讨论。 当有人邀请你参加审查时，你会收到一封电子邮件的通知（或者的Upsource的 News Feed），其中包含了对该评论的直接链接。您还将在此评审中更新任何结果更改。点击链接可以进入评审页面。主面板列出了与审查相关的所有事件和评论。 **Overview **选项卡下面显示更改的文件列表。单击文件名可以显示文件的修改内容。 从这里，您可以通过按下相应的按钮来进行代码的比较或查看整个文件。 参与讨论你也可以发表评论或者向其他参与者发表意见，并回复其他人留下的评论。 取消消息推送如果想停止对评论的监视，停止接收消息通知，可以从观察者列表中删除你自己。 如果不想删除自己，可以设置为Mute review（消息免打扰），这样这个评审后续的讨论信息就不会出现在你的News Feed中了。不过链接到评审中还是可以看到的。 六、讨论和评论你可以发表评论，向其他参与者发表意见或者回复其他人的评论。也可以直接在评论区@其他人（不一定是评审参与者）。 有三种不同类型的注释（comment，评论）： 评审评论 评审评论没有与任何特定的文件或代码的任何部分联系在一起，它的主题可能是评审范围内的任何内容。 代码块评论 代码块评论是对一个文件中的一个特定的代码片段的注释。 行评论 行注释与特定的代码行相关联。 提交评论 评审评论 在Review页面上，选择Review summary(默认视图)。在文本框中输入您的消息，并单击Add comment按钮。 行评论 在Review页面中，选择添加行注释的文件： 然后选中对应的你需要添加评论的行。 代码块评论 打开文件然后按住左键选中代码段，然后在弹出的页面中选中”Comment on selection”添加评论。 代码块注释和行注释嵌入在代码中，并以黄色突出显示。 评论的选项将光标置于评论之上，就可以看到评论的选项。 下面介绍各个选项的用途： 编辑这条评论。 链接到该文件中（如果是文件中的评论）该评论的位置。 点击添加对该评论的反应（表情包）。 Resolve 当某次讨论主题达成共识时，点击Resolve 链接标记讨论的问题得到解决（Resolved ）。 回复这条评论。 为讨论添加一个标签。","categories":[{"name":"工具","slug":"tools","permalink":"http://wuzguo.com/blog/categories/tools/"}],"tags":[{"name":"Intellij IDEA","slug":"Intellij-IDEA","permalink":"http://wuzguo.com/blog/tags/Intellij-IDEA/"},{"name":"Upsource","slug":"Upsource","permalink":"http://wuzguo.com/blog/tags/Upsource/"},{"name":"Code Review","slug":"Code-Review","permalink":"http://wuzguo.com/blog/tags/Code-Review/"},{"name":"代码审查","slug":"代码审查","permalink":"http://wuzguo.com/blog/tags/%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5/"}],"author":"Zak"},{"title":"AngularJS的指令","slug":"front/angularjs_directive","date":"2017-09-03T13:25:24.000Z","updated":"2022-08-01T06:33:30.005Z","comments":true,"path":"2017/09/03/front/angularjs_directive.html","link":"","permalink":"http://wuzguo.com/blog/2017/09/03/front/angularjs_directive.html","excerpt":"","text":"一、定义当我们开发应用程序实现业务需求时都需要扩展HTML，在AngularJS中通过指令来实现。AngularJS 内置了很多常用的指令，也可以根据业务需求自定义指令。 二、指令的执行过程 浏览器得到 HTML 字符串内容，解析得到 DOM 结构。 ng 引入，把 DOM 结构扔给 $compile 函数处理 找出 DOM 结构中有变量占位符 匹配找出 DOM 中包含的所有指令引用 把指令关联到 DOM 关联到 DOM 的多个指令按权重排列 执行指令中的 compile 函数（改变 DOM 结构，返回 link 函数） 得到的所有 link 函数组成一个列表作为 $compile 函数的返回 执行 link 函数（连接模板的 scope） 三、内置指令AngularJS 内置了很多常用的指令，下面简单介绍几个常用的指令。 ng-app ng-app 指令初始化一个 AngularJS 应用程序。ng-app 指令用于告诉 AngularJS 应用当前这个元素是根元素。所有 AngularJS 应用都必须要要一个根元素。HTML 文档中只允许有一个 ng-app 指令，如果有多个 ng-app 指令，则只有第一个会被使用。 &lt;element ng-app=&quot;modulename&quot;&gt;... 在 ng-app 根元素上的内容可以包含 AngularJS 代码...&lt;/element&gt; &lt;div ng-app=&quot;myApp&quot; ng-controller=&quot;myCtrl&quot;&gt; &#123;&#123; firstName + &quot; &quot; + lastName &#125;&#125;&lt;/div&gt;&lt;script&gt;var app = angular.module(&quot;myApp&quot;, []);app.controller(&quot;myCtrl&quot;, function($scope) &#123; $scope.firstName = &quot;John&quot;; $scope.lastName = &quot;Doe&quot;;&#125;);&lt;/script&gt; ng-init ng-init 指令初始化应用程序数据。 &lt;div ng-app=&quot;&quot; ng-init=&quot;myText=&#x27;Hello World!&#x27;&quot;&gt;&lt;h1&gt;&#123;&#123;myText&#125;&#125;&lt;/h1&gt; ng-model ng-model 指令把元素值（比如输入域的值）绑定到应用程序。&lt;input&gt; , &lt;select&gt;, &lt;textarea&gt;, 元素支持该指令。 &lt;element ng-model=&quot;name&quot;&gt;&lt;/element&gt; &lt;div ng-app=&quot;myApp&quot; ng-controller=&quot;myCtrl&quot;&gt; &lt;input ng-model=&quot;name&quot;&gt;&lt;/div&gt;&lt;script&gt;var app = angular.module(&#x27;myApp&#x27;, []);app.controller(&#x27;myCtrl&#x27;, function($scope) &#123; $scope.name = &quot;John Doe&quot;;&#125;);&lt;/script&gt; ng-bind ng-bind 指令告诉 AngularJS 使用给定的变量或表达式的值来替换 HTML 元素的内容。 如果给定的变量或表达式修改了，指定替换的 HTML 元素也会修改。所有的 HTML 元素都支持该指令。 &lt;element ng-bind=&quot;expression&quot;&gt;&lt;/element&gt; 或作为 CSS 类： &lt;element class=&quot;ng-bind: expression&quot;&gt;&lt;/element&gt; &lt;div ng-app=&quot;&quot; ng-init=&quot;myText=&#x27;Hello World!&#x27;&quot;&gt;&lt;p ng-bind=&quot;myText&quot;&gt;&lt;/p&gt;&lt;/div&gt; 四、自定义指令除了 AngularJS 内置的指令外，我们还可以创建自定义指令。你可以使用 .directive函数来添加自定义的指令。要调用自定义指令，HTML 元素上需要添加自定义指令名。 使用驼峰法来命名一个指令， runoobDirective ，但在使用它时需要以 - 分割，runoob-directive ： &lt;body ng-app=&quot;myApp&quot;&gt;&lt;runoob-directive&gt;&lt;/runoob-directive&gt;&lt;script&gt;var app = angular.module(&quot;myApp&quot;, []);app.directive(&quot;runoobDirective&quot;, function() &#123; return &#123; template : &quot;&lt;h1&gt;自定义指令!&lt;/h1&gt;&quot; &#125;;&#125;);&lt;/script&gt;&lt;/body&gt; 你可以通过以下方式来调用指令： 元素名： &lt;runoob-directive&gt;&lt;/runoob-directive&gt; 属性： &lt;div runoob-directive&gt;&lt;/div&gt; 类名： &lt;div class=&quot;runoob-directive&quot;&gt;&lt;/div&gt; 注释： &lt;!-- directive: runoob-directive --&gt; 1. 指令的属性 restrict（限制使用） 你可以限制你的指令只能通过特定的方式来调用。通过添加 restrict属性,并设置只值为 A, 来设置指令只能通过属性的方式来调用： var app = angular.module(&quot;myApp&quot;, []);app.directive(&quot;runoobDirective&quot;, function() &#123; return &#123; restrict : &quot;A&quot;, template : &quot;&lt;h1&gt;自定义指令!&lt;/h1&gt;&quot; &#125;;&#125;); restrict 值可以是以下几种: E 作为元素名使用 A 作为属性使用 C 作为类名使用 M 作为注释使用 restrict 默认值为 EA, 即可以通过元素名和属性名来调用指令。 注意：从浏览器的兼容性方面考虑，建议使用A或者E。 template 模板内容，这个内容会根据replace参数的设置替换节点或只替换节点的内容。 replace true ：替换节点，会将指令的标签会替换掉 false（默认值）：则把当前指令追加到所在元素内部。 对应 restrict 属性为元素时（E）在最终结果中是多余的，所有replace通常设置为 true。 priority 设置指令在模版中的执行顺序，顺序是相对于元素上其他执行而言，默认为0，从大到小的顺序依次执行。 terminal 是否以当前指令的权重为结束界限。如果这值设置为 true，则节点中权重小于当前指令的其它指令不会被执行。相同权重的会执行。如如果当前指令的 priority 值为 0， terminal 为true，那么其它priority 值小于0 的指令不会被执行。 templateUrl 当模板没有定义在指令中时，可以通过templateUrl属性链接外部模板，模板不一定是一个完整的HTML文件。 scope 指定指令的作用域。 false 指令直接使用父作用域的属性和方法。 true 指令要创建一个新的作用域，这个作用域继承自我们的父作用域（我们修改指令继承过来的属性时，父作用域中对应的属性值不会发生变化）。 {} 创建的一个新的与父作用域隔离的新的作用域，这使我们在不知道外部环境的情况下，就可以正常工作，不依赖外部环境。 var app = angular.module(&quot;MyApp&quot;, []); app.controller(&quot;MyController&quot;, function ($scope) &#123; $scope.name = &quot;dreamapple&quot;; $scope.age = 20; $scope.changeAge = function () &#123; $scope.age = 0; &#125; &#125;); app.directive(&quot;myDirective&quot;, function () &#123; return &#123; restrict: &quot;AE&quot;, scope: &#123; name: &#x27;@myName&#x27;, age: &#x27;=&#x27;, changeAge: &#x27;&amp;changeMyAge&#x27; &#125;, replace: true, template: &quot;&lt;div class=&#x27;my-directive&#x27;&gt;&quot; + &quot;&lt;h3&gt;-------------------------------&lt;/h3&gt;&quot; + &quot;我的名字是：&lt;span ng-bind=&#x27;name&#x27;&gt;&lt;/span&gt;&quot; + &quot;&lt;br/&gt;&quot; + &quot;我的年龄是：&lt;span ng-bind=&#x27;age&#x27;&gt;&lt;/span&gt;&quot; + &quot;&lt;br/&gt;&quot; + &quot;修改名字：&lt;input type=&#x27;text&#x27; ng-model=&#x27;name&#x27;&gt;&quot; + &quot;&lt;br/&gt;&quot; + &quot;&lt;button ng-click=&#x27;changeAge()&#x27;&gt;修改年龄&lt;/button&gt;&quot; + &quot;&lt;/div&gt;&quot; &#125; &#125;); &lt;div ng-app=&quot;MyApp&quot;&gt; &lt;div class=&quot;container&quot; ng-controller=&quot;MyController&quot;&gt; &lt;div class=&quot;my-info&quot;&gt;我的名字是：&lt;span ng-bind=&quot;name&quot;&gt;&lt;/span&gt; &lt;br/&gt;我的年龄是：&lt;span ng-bind=&quot;age&quot;&gt;&lt;/span&gt; &lt;br/&gt; &lt;/div&gt; &lt;div class=&quot;my-directive&quot; my-directive my-name=&quot;&#123;&#123;name&#125;&#125;&quot; age=&quot;age&quot; change-my-age=&quot;changeAge()&quot;&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 执行结果： 我们使用了隔离的作用域，不代表我们不可以使用父作用域的属性和方法。我们可以通过向scope的&#123;&#125;中传入特殊的前缀标识符（即prefix）来进行数据的绑定，前缀标识符有如下几种： @ 单项数据绑定标识符 使用方法：在元素中使用属性，如： &lt;div my-directive my-name=&quot;&#123;&#123;name&#125;&#125;&quot;&gt;&lt;/div&gt; 注意：属性的名字要用-将两个单词连接，因为是数据的单项绑定所以要通过使用双大括号来绑定数据。 = 双向数据绑定标识符 使用方法：在元素中使用属性，如： &lt;div my-directive age=&quot;age&quot;&gt;&lt;/div&gt; 注意：数据的双向绑定要通过=前缀标识符实现，所以不可以使用双大括号。 &amp; 绑定函数方法的标识符 使用方法：在元素中使用属性，如： &lt;div my-directive change-my-age=&quot;changeAge()&quot;&gt;&lt;/div&gt; 注意：属性的名字要用-将多个个单词连接。 注意：在新创建指令的作用域对象中，使用属性的名字进行绑定时，要使用驼峰命名标准，比如下面的代码。 scope: &#123; // &#x27;myName&#x27; 就是原来元素中的 &#x27;my-name&#x27; 属性 name: &#x27;@myName&#x27;, age: &#x27;=&#x27;, // &#x27;changeMyAge&#x27;就是原来元素中的&#x27;change-my-age&#x27;属性 changeAge: &#x27;&amp;changeMyAge&#x27; &#125; controller他会暴露一个方法其他指令可以注入该方法或者指令（通过指令调用这个方法），利用这个API可以在多个指令之间通过依赖注入进行通信。 controllerAs给controller方法起个别名，方便使用。 transclude设置html页面中使用指令时指定的原始数据在指令内部是否使用，当该属性设置为true时，将指令中的内容渲染到 ng-transclude 指令指定的地方。 如下所示： var myApp = angular.module(&#x27;myApp&#x27;, []).directive(&#x27;customTags&#x27;, function () &#123; return &#123; restrict: &#x27;ECAM&#x27;, template:&#x27;&lt;div&gt;新数据 &lt;span ng-transclude&gt;&lt;/span&gt;&lt;/div&gt;&#x27;, replace: true, transclude:false &#125;&#125;); &lt;div ng-app=&quot;myApp&quot;&gt; &lt;custom-tags&gt;原始数据&lt;/custom-tags&gt;&lt;/div&gt; 渲染的结果： 新数据 原始数据 require 填写当前指令需要依赖的其他指令的名称（或者指令名称的字符串数组），并将其控制器作为link函数的第四个参数传入。 选项 用法 directiveName 通过驼峰法命名法指定指令名称，,Angular将会在自身所提供的指令查找控制器 ^directiveName 在上游的指令链中查找所指定的指令中的控制器 ?directiveName 表示指令是可选的，如果当前指令中没有找到所需要的控制器，不需要抛出异常 2. 指令编译在angularJs应用启动之前，它们是以HTML文本形式存在文本编辑器当中。应用启动会进行编译和链接，作用域会同HTML进行绑定。 编译阶段（complie）： 在编译阶段，AngularJS会遍历整个HTML文档并根据JavaScript中的指令定义来处理页面上声明的指令。当AngularJS调用HTML文档根部的指令时，会遍历其中所有的模板，模板中也可能包含带有模板的指令，一旦对指令和其中的子模板进行遍历或编译，编译后的模板会返回一个叫做模板函数的函数。我们有机会在指令的模板函数被返回前，对编译后的DOM树进行修改。 以下是AngularJS中complie函数的参数： complie：function(tElement, tAttrs, transclude) 注意事项： complie函数用来对模版自身进行转换，仅仅在编译阶段运行一次。 complie中直接返回的函数是postLink，表示link参数需要执行的函数，也可以返回一个对象里面包含preLink和postLink。 当定义complie参数时，将无视link参数，因为complie里返回的就是该指令需要执行的link函数。 链接阶段（link）： 链接函数来将模板与作用域链接起来;负责设置事件监听器，监视数据变化和实时的操作DOM.链接函数是可选的。如果定义了编译函数，它会返回链接函数，因此当两个函数都定义了时，编译函数会重载链接函数。 以下是AngularJS中link函数的参数： link(scope, iElement, iAttrs, controller) 注意事项： link参数代表的是complie返回的postLink。 preLink 表示在编译阶段之后，指令连接到子元素之前运行。 postLink 表示会在所有子元素指令都连接之后才运行。 link函数负责在模型和视图之间进行动态关联，对于每个指令的每个实例，link函数都会执行一次。 var myApp = angular.module(&#x27;myApp&#x27;, []); myApp.directive(&#x27;customTags&#x27;, function () &#123; return &#123; restrict: &#x27;ECAM&#x27;, template: &#x27;&lt;div&gt;&#123;&#123;user.name&#125;&#125;&lt;/div&gt;&#x27;, replace: true, compile: function (tElement, tAttrs, transclude) &#123; tElement.append(angular.element(&#x27;&lt;div&gt;&#123;&#123;user.name&#125;&#125;&#123;&#123;user.count&#125;&#125;&lt;/div&gt;&#x27;)); // 编译阶段... console.log(&#x27;customTags compile 编译阶段...&#x27;); return &#123; // 表示在编译阶段之后，指令连接到子元素之前运行 pre: function preLink(scope, iElement, iAttrs, controller) &#123; console.log(&#x27;customTags preLink..&#x27;) &#125;, // 表示在所有子元素指令都连接之后才运行 post: function postLink(scope, iElement, iAttrs, controller) &#123; iElement.on(&#x27;click&#x27;, function () &#123; scope.$apply(function () &#123; scope.user.name = &#x27;click after&#x27;; scope.user.count = ++i; // 进行一次 脏检查 &#125;); &#125;) console.log(&#x27;customTags all child directive link..&#x27;) &#125; &#125; // 可以直接返回 postLink // return postLink function()&#123; // console.log(&#x27;compile return fun&#x27;); //&#125; &#125;, // 此link表示的就是 postLink link: function () &#123;// iElement.on(&#x27;click&#x27;,function()&#123;// scope.$apply(function()&#123;// scope.user.name = &#x27;click after&#x27;;// scope.user.count = ++i;// // 进行一次 脏检查// &#125;);// &#125;) &#125; &#125; &#125;); &lt;div ng-app=&quot;myApp&quot;&gt; &lt;div ng-controller=&quot;firstController&quot;&gt; &lt;div ng-repeat=&quot;user in users&quot; custom-tags=&quot;&quot;&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 执行结果如下： 3. 使用实例以下通过AngularJS指令实现手风琴式的下拉菜单功能，代码如下： angular.module(&#x27;myApp&#x27;, [])// 数据 .factory(&#x27;Data&#x27;, function () &#123; return [ &#123; title: &#x27;no1&#x27;, content: &#x27;no1-content&#x27; &#125;, &#123; title: &#x27;no2&#x27;, content: &#x27;no2-content&#x27; &#125;, &#123; title: &#x27;no3&#x27;, content: &#x27;no3-content&#x27; &#125; ]; &#125;) // 控制器 .controller(&#x27;firstController&#x27;, [&#x27;$scope&#x27;, &#x27;Data&#x27;, function ($scope, Data) &#123; $scope.data = Data; &#125;]) .directive(&#x27;kittencupGroup&#x27;, function () &#123; return &#123; restrict: &#x27;E&#x27;, replace: true, template: &#x27;&lt;div class=&quot;panel-group&quot; ng-transclude&gt;&lt;/div&gt;&#x27;, transclude: true, controllerAs: &#x27;kittencupGroupContrller&#x27;, controller: function () &#123; this.groups = []; this.closeOtherCollapse = function (nowScope) &#123; angular.forEach(this.groups, function (scope) &#123; if (scope !== nowScope) &#123; scope.isOpen = false; &#125; &#125;) &#125; &#125; &#125; &#125;) .directive(&#x27;kittencupCollapse&#x27;, function () &#123; return &#123; restrict: &#x27;E&#x27;, replace: true, require: &#x27;^kittencupGroup&#x27;, templateUrl: &#x27;app/kittencupCollapse.html&#x27;, scope: &#123; heading: &#x27;@&#x27; &#125;, link: function (scope, element, attrs, kittencupGroupContrller) &#123; scope.isOpen = false; scope.changeOpen = function () &#123; scope.isOpen = !scope.isOpen; kittencupGroupContrller.closeOtherCollapse(scope); &#125; kittencupGroupContrller.groups.push(scope); &#125;, transclude: true &#125; &#125;); &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;../../vendor/bootstrap3/css/bootstrap.min.css&quot;/&gt;&lt;/head&gt;&lt;body&gt;&lt;div ng-app=&quot;myApp&quot;&gt; &lt;div class=&quot;container&quot;&gt; &lt;div ng-controller=&quot;firstController&quot;&gt; &lt;kittencup-group&gt; &lt;kittencup-collapse ng-repeat=&quot;collapse in data&quot; heading=&quot;&#123;&#123;collapse.title&#125;&#125;&quot;&gt; &#123;&#123;collapse.content&#125;&#125; &lt;/kittencup-collapse&gt; &lt;/kittencup-group&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;../../vendor/angular/angularjs.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;app/index.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 模板文件 kittencupCollapse.html 如下： &lt;div class=&quot;panel panel-default&quot;&gt; &lt;div class=&quot;panel-heading&quot; ng-click=&quot;changeOpen()&quot;&gt; &lt;h4 class=&quot;panel-title&quot;&gt; &lt;a href=&quot;#&quot;&gt; &#123;&#123;heading&#125;&#125; &lt;/a&gt; &lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-collapse&quot; ng-class=&quot;&#123;collapse:!isOpen&#125;&quot;&gt; &lt;div class=&quot;panel-body&quot; ng-transclude&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 实现效果： 五、参考博客 一招制敌 - 玩转 AngularJS 指令的 Scope (作用域) https://segmentfault.com/a/1190000002773689 kittencup 的AngularJS教学视频 http://kittencup.com/ 菜鸟教程AngularJS教程指令章节 http://www.runoob.com/angularjs/angularjs-directives.html","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"AngularJS","slug":"AngularJS","permalink":"http://wuzguo.com/blog/tags/AngularJS/"},{"name":"指令","slug":"指令","permalink":"http://wuzguo.com/blog/tags/%E6%8C%87%E4%BB%A4/"},{"name":"directive","slug":"directive","permalink":"http://wuzguo.com/blog/tags/directive/"}],"author":"Zak"},{"title":"Upsource使用说明之代码审查实操","slug":"tools/upsource_instructions_code_review","date":"2017-08-28T13:50:01.000Z","updated":"2022-08-01T06:35:23.834Z","comments":true,"path":"2017/08/28/tools/upsource_instructions_code_review.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/28/tools/upsource_instructions_code_review.html","excerpt":"","text":"Upsource使用说明一、添加项目创建新项目，管理员账号登录Upsource中在选择Hub，创建一个需要进行代码审核的项目，输入项目名称、Key及描述信息。 添加完成后，可以为该项目添加团队成员，设置项目的基本信息等。 然后进入正题添加 Code Review 服务。选择Add Service 中配置源代码的VCS的类型及路径等信息，配置完成后Upsource便会连接配置的VCS链接获取代码提交记录。这里需要设置获取源代码的URL地址以及具有访问权限的账号。Key为SSH的私钥。 服务添加成功后便可点击 upsource 链接进行代码审核阶段。 具有代码审查权限的用户可以针对该项目的每一个提交操作创建一个Review。 这里有一个主意的地方就是 upsource 和版本管理系统（gitlab）各自有自己的账户体系，但是他们之间可以关联，只需要在账户设置中配置一下对应关系就好。 二、使用说明 创建一个Review，创建Review时，可以为每个修改的文件添加行注释和段落注释，以及评论，同时为这个Review添加 Observer（可以是作者本人，也可以说跟这个功能相关的人员，如某个接口修改了参数，需要把用到这个接口的人员都添加进来，这样避免修改了参数其他人不知道）。 添加行级注释。 添加段落注释。 当Reviewer添加审查意见后，对应的开发人员和Observer都能在账户中看到对应的评价信息。这个Review相关的所有人都可以对这个Review进行回应。也可以@某个人。 这样就可以精确到某一行代码进行充分讨论，最后得出修改意见。然后代码的作者修改代码，达到审查者的要求后，审查者可以Accept这个 Review的修改结果，表示接受此次修订，此次审查被视为完成。也可以 Raise Concern 让作者引起关注。审查者和代码作者都可以关闭 Review，可以在项目属性设置中代码作者是否有关闭Review的权限。 相关人员都可以对这个Review 贴上标签。可以标注这个问题是功能性的BUG还是代码的样式问题等。也可以自定义标签在全局项目中使用。 开发人员根据代码审查过程中的讨论结果进行代码修改，然后提交到CVS系统，为了跟踪代码审查的执行结果，相关人员可以将提交的revision （修订）加入到本次审查流程中。 评论支持MarkDown语法，可以添加各种表情包，添加表格等等。 Upsource还支持自定义工作流程中进行自动化，其中包括： 可以设置当有人修改项目中的核心文件时自动发起代码审查操作。 当有人修改的代码或提交消息指定了设置的问题ID时（issue ID）时，自动将当前修订（Revision）添加到对应的Review流程中。 自动分配审核参与者给手动或自动创建的Review。 当所有参与评审者接受更改时关闭审查。 在审查结束时将所有讨论表决为Resolved 状态。 当对以前审查的分支机构进行新的提交时，重新审核并恢复分支跟踪。 如图设置了一个当有人修改 AngularEchartsExamplesApplication.java 这个文件时自动发起一个审查流程。","categories":[{"name":"工具","slug":"tools","permalink":"http://wuzguo.com/blog/categories/tools/"}],"tags":[{"name":"Intellij IDEA","slug":"Intellij-IDEA","permalink":"http://wuzguo.com/blog/tags/Intellij-IDEA/"},{"name":"Upsource","slug":"Upsource","permalink":"http://wuzguo.com/blog/tags/Upsource/"},{"name":"Code Review","slug":"Code-Review","permalink":"http://wuzguo.com/blog/tags/Code-Review/"},{"name":"代码审查","slug":"代码审查","permalink":"http://wuzguo.com/blog/tags/%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5/"}],"author":"Zak"},{"title":"Upsource使用说明之代码审查规则","slug":"tools/upsource_instructions_code_audit_rules","date":"2017-08-27T06:45:23.000Z","updated":"2022-08-01T06:35:23.758Z","comments":true,"path":"2017/08/27/tools/upsource_instructions_code_audit_rules.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/27/tools/upsource_instructions_code_audit_rules.html","excerpt":"","text":"一、代码审查的意义 代码审查简单的描述就是让其他同事来检查你的代码，其目的在于找到开发初期所忽略的错误，从而提高代码的整体质量，降低风险，减少代码隐藏的一些问题。同时通过同事间的合作可以相互学习并取得进步。代码提交者很有可能从该工作中得到反馈，并意识到可能存在的问题和需要改进的部分；而审查者也可以通过阅读他人代码学到新的东西，并找出适用于自己的工作方案。 二、使用Upsource进行代码审查1. Review审查流程 由有权限的同事针对某个或者多个代码提交操作发起一次代码审查流程，然后审查人员添加跟这次审查流程相关的Reviewers和Watchers，这里的 Authors、Reviewers、Watchers 都是复数形式表示都有可能是多个人员。 一次Review也可以添加多个Revisions(修订，可以理解为一次版本提交)。 一般情况下将这次Review中关系关系很密切的人设置为Reviewer，然后这次Review需要知道的人设置为Watcher，假设某个Review是关于某个接口的参数修改，我们可以把接口提供方的调用方都设置为Reviewer，然后跟这个接口关系不够密切但是需要知晓的人设置为Watcher。各个参与人员的角色没有很明确的划分界限，可以根据实际情况来确定。Review的参与人员都有权限回复别人提出的意见，Authors（代码作者）角色的人在这次Review是执行者，负责执行大家讨论的结果。 如果Authors 在IDEA中完成经过讨论的修改方案后，提交代码的时候可以设置Review的ID，表示本次提交是为了解决某个Reivew中提出的问题。 然后在upsource中就可以跟踪到某个相应review的解决结果了。 当 Authors 修改完成代码后， Reviewers 可以 Accept修改结果或者Raise concern ,当所有 Reviewers 都 Accept修改结果的时候，这个Review可以正常关闭，默认情况下 Authors也可以关闭Review，但是可以设置不允许 Authors 关闭。（系统管理员选择Edit Project -&gt; Advanced -&gt; 取消 Allow authors to close reviews的选项） 如果提交的时候忘记忘记勾选 reivew with upsource 选项了可以在提交完成后在Version Control ``Tab页面右击鼠标选择upsource 然后设置到对应的Review即可。 也可以直接在Upsource页面将对应的修订提交加到这个Review中来。 添加进来后修订的时间线将发生变化。 当然也可以Remove某个版本。 也可以直接在IDEA中对代码添加评论，或者将评论添加到Review流程中。 或者按下键盘 ctrl+alt+ 斜杠 （windows下）添加评论。 勾选 Attach to review 可以将评论添加到对应的Review中。 2. 审查分支分支审查包括所选分支的所有当前提交修订以及所有将来的修订，将自动附加到审查。创建方式如下选择某个项目 -&gt; Branches -&gt; 选择需要Review的分支 -&gt; Create branch review. 其他流程跟上面的一样，不再赘述。 3. 评论而不创建Review每当你在同事的代码中发现一些小问题时，想要给他一个提醒或者开始一个非正式的讨论，你可以留下评论而不创建Review。您可以发表评论，并回复他人留下的评论。 4. 添加标签你可以为任何评论添加标签来标注你认为有用的信息或者分类，也可以创建自己的标签在整个项目中使用。 5. 评论评论支持MarkDown语法，可以添加各种表情包，添加图表等等。 6. 关于通知Upsource的通知信息一般包含邮件或者IDEA中的插件推送，如果你觉得消息泛滥有些消息不需要提醒的时候，可以自定义消息通知。 邮件通知在Upsource的账户中设置，根据自己关注的添加适当的触发器。 IDEA的插件通知可以在IDEA中打开设置页面，在 file-&gt; settings-&gt; 搜索“upsource” -&gt;然后根据自己的实际关注的消息进行设置。","categories":[{"name":"工具","slug":"tools","permalink":"http://wuzguo.com/blog/categories/tools/"}],"tags":[{"name":"Intellij IDEA","slug":"Intellij-IDEA","permalink":"http://wuzguo.com/blog/tags/Intellij-IDEA/"},{"name":"Upsource","slug":"Upsource","permalink":"http://wuzguo.com/blog/tags/Upsource/"},{"name":"Code Review","slug":"Code-Review","permalink":"http://wuzguo.com/blog/tags/Code-Review/"},{"name":"代码审查","slug":"代码审查","permalink":"http://wuzguo.com/blog/tags/%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5/"}],"author":"Zak"},{"title":"Upsource使用说明之在IDEA中集成Upsource","slug":"tools/upsource_instructions_integrated_idea","date":"2017-08-27T01:14:56.000Z","updated":"2022-08-01T06:35:23.859Z","comments":true,"path":"2017/08/27/tools/upsource_instructions_integrated_idea.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/27/tools/upsource_instructions_integrated_idea.html","excerpt":"","text":"一、简介Upsource是一款优秀的代码审查工具，UI简洁大方，功能强大，浏览代码格式美观，可以跟IDEA进行无缝集成，完成代码审查工作。 二、集成IDEA 首先在IDEA的插件仓库中下载Upsource插件 在IDEA中打开这个页面的路径是 file -&gt; settings -&gt;plugins -&gt;browse repositories，由于我本机上已经安装了该插件所以显示为 update，安装完成后重启IDEA插件生效。 配置服务器和用户名插件安装完成后会在IDEA的右下角有个Upsource的图标，点击图标可以配置Upsource的服务器信息。 点击图标弹出配置服务器信息，将Server URL 配置为：http://172.16.0.100:2222/，然后点击 Apply，其他配置默认即可。 配置完成后点击Upsource的图标会登录界面，用自己的用户名&#x2F;密码登录到服务器就好了。 登录成功以后，就会在IDEA中显示跟你有关的Review信息，可以直接在News Feed中针对某个Comment进行回复。 后续可以直接点击Upsource按钮切换用户名，切换项目等操作，当需要设置Edit Path Mapping 时，需要设置该项目的本地代码路径以及远程路径。 其中Local root path 设置为现在激活的项目的代码的本地路径，Upsource remote path设置为 / 就好。 后续代码审查人员针对你提交的代码有审核操作的时候，你将自动收到来自Upsource的提示信息。 设置邮件提醒 在浏览器中登录Upsource服务器http://www.wuzguo.com/，在右上角选中Upsource然后再点击右上角的账号图标选择 Notifications 便可以设置。 在账户配置中设置开通邮件提醒，当有关于你的代码审查时会邮件通知你。 然后你将收到类似的邮件。","categories":[{"name":"工具","slug":"tools","permalink":"http://wuzguo.com/blog/categories/tools/"}],"tags":[{"name":"Intellij IDEA","slug":"Intellij-IDEA","permalink":"http://wuzguo.com/blog/tags/Intellij-IDEA/"},{"name":"Upsource","slug":"Upsource","permalink":"http://wuzguo.com/blog/tags/Upsource/"},{"name":"Code Review","slug":"Code-Review","permalink":"http://wuzguo.com/blog/tags/Code-Review/"},{"name":"代码审查","slug":"代码审查","permalink":"http://wuzguo.com/blog/tags/%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5/"}],"author":"Zak"},{"title":"Upsource使用说明之安装配置","slug":"tools/upsource_instructions_installation_settings","date":"2017-08-26T15:03:10.000Z","updated":"2022-08-01T06:35:23.768Z","comments":true,"path":"2017/08/26/tools/upsource_instructions_installation_settings.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/26/tools/upsource_instructions_installation_settings.html","excerpt":"","text":"一、简介Upsource是一款优秀的代码审查工具，UI简洁大方，功能强大，浏览代码格式美观，可以跟IDEA无缝集成完成代码审查工作，目前官方最新版本是2017.2.2197，已经被我破解，有需要的可以联系我。 二、安装 下载合适版本的Upsource安装程序，将其解压到任意磁盘，进入Upsource根目录(下面统称upsource_home)，准备进行安装。 以管理员身份运行打开cmd，切换到目录，执行命令 upsource_home\\bin\\upsource.bat start 当出现 upsource is running 提示时说明启动完成。 启动完成后会打开默认浏览器网址http:&#x2F;&#x2F;机器名:80&#x2F;welcome 页面，注意在windows中默认端口为80，在以前的版本端口默认是8080。这是你会看到如下页面： 当看到以上界面时说明安全正常完成。 三、设置 初始化 点击 Set up 链接，这时我们可以修改访问域名和端口和管理员账号。 ​ 点击 next 按钮设置其他信息，最后点击 finish 按钮静候upsource的初始化操作，这里需要一点时间。 ​ 初始化完成后，用管理员账号登录便可进入欢迎界面。 设置默认值 初始化完成后可以设置upsource 的全局信息设置，如语言，字体，session超时时间等。 添加用户，用户组，项目。 然后给用户设置角色。upsource有五种角色，分别是：Code Viewer、Developer、Observer、Project Amin、System Admin。可以根据用户在项目组中担任的角色赋予相应的角色。一个账号可以有多个角色。","categories":[{"name":"工具","slug":"tools","permalink":"http://wuzguo.com/blog/categories/tools/"}],"tags":[{"name":"Intellij IDEA","slug":"Intellij-IDEA","permalink":"http://wuzguo.com/blog/tags/Intellij-IDEA/"},{"name":"Upsource","slug":"Upsource","permalink":"http://wuzguo.com/blog/tags/Upsource/"},{"name":"Code Review","slug":"Code-Review","permalink":"http://wuzguo.com/blog/tags/Code-Review/"},{"name":"代码审查","slug":"代码审查","permalink":"http://wuzguo.com/blog/tags/%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5/"}],"author":"Zak"},{"title":"AngularJS 整合 ECharts","slug":"front/angularjs_echarts_integration","date":"2017-08-23T13:58:12.000Z","updated":"2022-08-01T06:35:23.698Z","comments":true,"path":"2017/08/23/front/angularjs_echarts_integration.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/23/front/angularjs_echarts_integration.html","excerpt":"","text":"一、背景AngularJS 是Google开发的前端技术框架，为了弥补了HTML在构建应用方面的不足，构建单页面应用程序（Single Page Application）非常有优势。 二、整合 定义AngularJS 的指令（directive） &#x27;use strict&#x27;;define([ &#x27;workspace/workspace.module&#x27;], function (workspaceModule) &#123; // 定义指令 workspaceModule.directive(&#x27;stackedEchart&#x27;, function () &#123; //初始化echarts图表 var echeartOption = function (me) &#123; var axisData = [[&#x27;0点&#x27;, 0, 0, 0, 0, 0], [&#x27;1点&#x27;, 0, 0, 0, 0, 0], [&#x27;2点&#x27;, 0, 0, 0, 0, 0], [&#x27;3点&#x27;, 0, 0, 0, 0, 0], [&#x27;4点&#x27;, 0, 0, 0, 0, 0], [&#x27;5点&#x27;, 0, 0, 0, 0, 0], [&#x27;6点&#x27;, 0, 0, 0, 0, 0], [&#x27;7点&#x27;, 0, 0, 0, 0, 0], [&#x27;8点&#x27;, 0, 0, 0, 0, 0], [&#x27;9点&#x27;, 0, 0, 0, 0, 0], [&#x27;10点&#x27;, 0, 0, 0, 0, 0], [&#x27;11点&#x27;, 0, 0, 0, 0, 0], [&#x27;12点&#x27;, 0, 0, 0, 0, 0], [&#x27;13点&#x27;, 0, 0, 0, 0, 0], [&#x27;14点&#x27;, 0, 0, 0, 0, 0], [&#x27;15点&#x27;, 0, 0, 0, 0, 0], [&#x27;16点&#x27;, 0, 0, 0, 0, 0], [&#x27;17点&#x27;, 0, 0, 0, 0, 0], [&#x27;18点&#x27;, 0, 0, 0, 0, 0], [&#x27;19点&#x27;, 0, 0, 0, 0, 0], [&#x27;20点&#x27;, 0, 0, 0, 0, 0], [&#x27;21点&#x27;, 0, 0, 0, 0, 0], [&#x27;22点&#x27;, 0, 0, 0, 0, 0], [&#x27;23点&#x27;, 0, 0, 0, 0, 0]]; var stackedOption = &#123; /*标题*/ title: &#123; text: me.etext || &#x27;平台注册用户量数据统计&#x27;, subtext: me.esubtext || &#x27;实时数据&#x27;, x: &#x27;left&#x27; &#125;, /*提示框，鼠标悬浮交互时的信息提示*/ color: [&#x27;#3398DB&#x27;], tooltip: &#123; trigger: me.etrigger || &#x27;axis&#x27;, axisPointer: &#123; // 坐标轴指示器，坐标轴触发有效 type: &#x27;shadow&#x27; // 默认为直线，可选为：&#x27;line&#x27; | &#x27;shadow&#x27; &#125; &#125;, legend: &#123; data: me.elegendata || [&#x27;注册量（人）&#x27;] &#125;, /*工具箱，每个图表最多仅有一个工具箱*/ toolbox: &#123; show: true, feature: me.efeature || &#123; mark: &#123; show: false &#125;, dataView: &#123; show: false, readOnly: false &#125;, magicType: &#123; show: true, type: [&#x27;line&#x27;, &#x27;bar&#x27;] &#125;, restore: &#123; show: false &#125; &#125; &#125;, // 坐标网格设置 grid: &#123; left: &#x27;1%&#x27;, right: &#x27;3%&#x27;, bottom: &#x27;2%&#x27;, containLabel: true &#125;, xAxis: me.exAxis || [ &#123; data: (me.exAxisData || axisData).map(function (item) &#123; return item[0]; &#125;), type: me.exAxisType || &#x27;category&#x27; &#125; ], /*直角坐标系中纵轴数组*/ yAxis: me.eyAxis || [ &#123; type: &#x27;value&#x27;, min: 0, minInterval: 1 &#125; ], /*驱动图表生成的数据内容数组*/ series: me.eseries || [ &#123; name: &#x27;PC&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, barWidth: 40, data: (me.exAxisData || axisData ).map(function (item) &#123; return item[1]; &#125;) &#125;, &#123; name: &#x27;触屏&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: (me.exAxisData || axisData ).map(function (item) &#123; return item[2]; &#125;) &#125;, &#123; name: &#x27;IOS&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: (me.exAxisData || axisData ).map(function (item) &#123; return item[3]; &#125;) &#125;, &#123; name: &#x27;微信&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: (me.exAxisData || axisData ).map(function (item) &#123; return item[4]; &#125;) &#125;, &#123; name: &#x27;安卓&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: (me.exAxisData || axisData ).map(function (item) &#123; return item[5]; &#125;), markPoint: &#123; data: [ &#123;type: &#x27;max&#x27;, name: &#x27;最大值&#x27;&#125;, &#123;type: &#x27;min&#x27;, name: &#x27;最小值&#x27;&#125; ] &#125;, markLine: &#123; data: [ &#123;type: &#x27;average&#x27;, name: &#x27;平均值&#x27;&#125; ] &#125; &#125; ] &#125;; // 重置图表 me.echarts.clear(); me.echarts.setOption(stackedOption); &#125;; return &#123; restrict: &#x27;AE&#x27;, replace: false, // 定义指令和模板中使用的作用域的关系： // false: 复用组件具体使用位置所在的作用域 // true: 创建子作用域，该作用域继承组件具体使用位置所在的作用域 // 对象：创建独立作用域，该作用域没有继承原型：插入（@）、数据绑定（=）、表达式（&amp;），在指令定义中会以名-值对的方式来定义这些接口： // scope: &#123; // isolated1: &#x27;@attribute1&#x27;, // isolated2: &#x27;=attribute2&#x27;, // isolated3: &#x27;&amp;attribute3&#x27;, // &#125; // 如果在定义字段值时省略了属性名称，那么就表示该映射属性与对应的独立作用域字段名称完全一致 scope: &#123; source: &#x27;=&#x27; &#125;, template: &#x27;&lt;div&gt;柱状图&lt;/div&gt;&#x27;, controller: [&#x27;$scope&#x27;, &#x27;$element&#x27;, &#x27;$attrs&#x27;, function ($scope, $element, $attrs) &#123; // 指令间数据共享的时候用到 &#125;], // 其他指令需要调用这个指令的时候需要用到 controller 的名称 controllerAs: &#x27;stackedEchartController&#x27;, compile: function ($scope, element, attr) &#123; // 获取图表的渲染位置 var stacked = document.getElementById(&#x27;echarts-stacked&#x27;); // 初始化 var me = me || &#123;&#125;; // 初始化图表 me.echarts = echarts.init(stacked); // 直接返回 link 函数 return function ($scope, element, attr) &#123; $scope.$watch( function ($scope) &#123; return $scope.source; &#125;, function () &#123; var data = $scope.source; if (data &amp;&amp; angular.isArray(data)) &#123; var self = me; self.exAxisData = data; echeartOption(self); &#125; &#125;, true); $scope.resizeDom = function () &#123; return &#123; &#x27;height&#x27;: stacked.offsetHeight, &#x27;width&#x27;: stacked.offsetWidth &#125;; &#125;; $scope.$watch($scope.resizeDom, function () &#123; me.echarts.resize(); &#125;, true); // 渲染图表 echeartOption(me); &#125; &#125; &#125;; &#125;);&#125;); 定义AngularJS 的组件（component） &#x27;use strict&#x27;;define([ &#x27;workspace/workspace.module&#x27;, &#x27;text!echarts/stacked/stacked-histograms-template.html&#x27;, &#x27;echarts/common/http-common-service&#x27;, &#x27;echarts/stacked/stacked-histograms-directive&#x27;, &#x27;css!echarts/stacked/stacked-histograms.css&#x27;], function (workspaceModule, echartsHistogramsTemplate) &#123; workspaceModule.component(&#x27;stackedHistogramsComp&#x27;, &#123; template: echartsHistogramsTemplate, controller: function ($scope) &#123; &#125; &#125;); // 控制器 workspaceModule.controller(&#x27;stackedHistogramsCtrl&#x27;, [&#x27;$scope&#x27;, &#x27;$interval&#x27;, &#x27;$timeout&#x27;, &#x27;httpCommonService&#x27;, function ($scope, $interval, $timeout, httpCommonService) &#123; // 获取数据的函数 var registUser = function () &#123; // 请求服务端数据 httpCommonService.httpRequest(&quot;/echart/statRegistUser&quot;, 3000).then(function (res) &#123; if (res.success &amp;&amp; res.data) &#123; // 给默认值，避免画图时候报错 var exAxisdata = [[&#x27;0点&#x27;, 0, 0, 0, 0, 0], [&#x27;1点&#x27;, 0, 0, 0, 0, 0], [&#x27;2点&#x27;, 0, 0, 0, 0, 0], [&#x27;3点&#x27;, 0, 0, 0, 0, 0], [&#x27;4点&#x27;, 0, 0, 0, 0, 0], [&#x27;5点&#x27;, 0, 0, 0, 0, 0], [&#x27;6点&#x27;, 0, 0, 0, 0, 0], [&#x27;7点&#x27;, 0, 0, 0, 0, 0], [&#x27;8点&#x27;, 0, 0, 0, 0, 0], [&#x27;9点&#x27;, 0, 0, 0, 0, 0], [&#x27;10点&#x27;, 0, 0, 0, 0, 0], [&#x27;11点&#x27;, 0, 0, 0, 0, 0], [&#x27;12点&#x27;, 0, 0, 0, 0, 0], [&#x27;13点&#x27;, 0, 0, 0, 0, 0], [&#x27;14点&#x27;, 0, 0, 0, 0, 0], [&#x27;15点&#x27;, 0, 0, 0, 0, 0], [&#x27;16点&#x27;, 0, 0, 0, 0, 0], [&#x27;17点&#x27;, 0, 0, 0, 0, 0], [&#x27;18点&#x27;, 0, 0, 0, 0, 0], [&#x27;19点&#x27;, 0, 0, 0, 0, 0], [&#x27;20点&#x27;, 0, 0, 0, 0, 0], [&#x27;21点&#x27;, 0, 0, 0, 0, 0], [&#x27;22点&#x27;, 0, 0, 0, 0, 0], [&#x27;23点&#x27;, 0, 0, 0, 0, 0]]; // 字典，这里要跟图表中的各个设备的展示顺序一致，否则数据将不正确 var indexDev = &#123;&#x27;PC&#x27;: 1, &#x27;触屏&#x27;: 2, &#x27;IOS&#x27;: 3, &#x27;微信&#x27;: 4, &#x27;安卓&#x27;: 5&#125;; var retJson = res.data; // console.log(&quot;retJson: &quot; + JSON.stringify(retJson)); for (var key in retJson) &#123; // 将对象转换为数组 var indexData = retJson[key]; for (var k in indexData) &#123; for (var i = 0; i &lt; 24; i++) &#123; if ((k + &quot;点&quot;) == exAxisdata[i][0]) &#123; // 由于数据量太小，这里加上随机数 exAxisdata[k][indexDev[key]] = indexData[k] + parseInt(Math.random() * 1000); &#125; &#125; &#125; &#125; // console.log(JSON.stringify(&quot;exAxisdata: &quot; + exAxisdata)); // 更新数据会触发指令的监听事件 $scope.dataGroup = exAxisdata; &#125; &#125;); &#125;; registUser(); // 每个十秒请求一次 $interval(function () &#123; registUser(); &#125;, 10 * 1000); &#125;]);&#125;); 定义AngularJS 的服务（service） &#x27;use strict&#x27;;define([ &#x27;workspace/workspace.module&#x27;], function (workspaceModule) &#123; // 这里不要注入 $scope，否则会报错: [$injector:unpr] Unknown provider: $scopeProvider &lt;- $scope &lt;- httpCommonService workspaceModule.service(&#x27;httpCommonService&#x27;, [&#x27;$http&#x27;, function ($http) &#123; var doRequest = function (url, timeout) &#123; var req = &#123; method: &#x27;GET&#x27;, url: url, cache: false, timeout: timeout, headers: &#123; &#x27;Content-Type&#x27;: &#x27;application/json&#x27; &#125; &#125; return $http(req).then(function (response) &#123; // console.log(&quot;success: &quot; + JSON.stringify(response)); return response.data; &#125;, function (response) &#123; console.log(&quot;error: &quot; + JSON.stringify(response)); return response.data; &#125;); &#125; return &#123; httpRequest: function (url, timeout) &#123; return doRequest(url, timeout); &#125; &#125;; &#125;]);&#125;); 在对应页面中添加指令 &lt;div class=&quot;echarts-mng-container&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm-12&quot; ng-controller=&quot;stackedHistogramsCtrl&quot;&gt; &lt;div id=&quot;echarts-stacked&quot; stacked-echart data-source=&#x27;dataGroup&#x27; style=&quot;width: 100%; height:856px;&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 运行效果 三、源码请移步到我的GitHUb，地址：https://github.com/wuzguo/angular-echarts-examples 。","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"AngularJS","slug":"AngularJS","permalink":"http://wuzguo.com/blog/tags/AngularJS/"},{"name":"ECharts","slug":"ECharts","permalink":"http://wuzguo.com/blog/tags/ECharts/"}],"author":"Zak"},{"title":"ExtJs整合ECharts","slug":"front/extjs_echarts_integration","date":"2017-08-23T13:27:25.000Z","updated":"2022-08-01T06:35:23.807Z","comments":true,"path":"2017/08/23/front/extjs_echarts_integration.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/23/front/extjs_echarts_integration.html","excerpt":"","text":"一、背景ECharts是百度开源的纯 Javascript 的图表库，可以流畅的运行在 PC 和移动设备上，兼容当前绝大部分浏览器。Ext JS主要用于跨浏览器功能构建Web应用程序提供了丰富的UI， 多用于企业级的管理系统和互联网后台管理系统。当我们在系统中展示各种图表的时候需要用到ECharts。 二、集成 首先定义一个Ext JS的组件，直接上代码。 /** * 堆叠柱状图 * Created by wuzguo on 2017/6/19 */Ext.ns(&#x27;Ext.ux.chart&#x27;);Ext.ux.chart.stackedHistograms = Ext.extend(Ext.Component, &#123; border: false, height: 100,//不能省略，虽然没用 //修改/刷新数据 update: function (exAxisData, eseries) &#123; var me = this; // 清空数据 me.exAxisdata.length = 0; me.eseries.length = 0; me.exAxisdata = exAxisData; me.eseries = eseries; me.echarts.clear(); me.echeartOption(me); &#125;, //初始化box组件 initComponent: function () &#123; var me = this; if (!me.height) &#123; throw new Error(&#x27;图表组件需要设置高度!&#x27;); &#125; me.on(&quot;resize&quot;, function (me, mewidth, meheight) &#123; me.getEl().dom.style.height = meheight + &#x27;px&#x27;; me.echarts = echarts.init(me.getEl().dom); me.echeartOption(me); &#125;); &#125;, //初始化echarts图表 echeartOption: function (me) &#123; var axisdata = [&#x27;0点&#x27;, &#x27;1点&#x27;, &#x27;2点&#x27;, &#x27;3点&#x27;, &#x27;4点&#x27;, &#x27;5点&#x27;, &#x27;6点&#x27;, &#x27;7点&#x27;, &#x27;8点&#x27;, &#x27;9点&#x27;, &#x27;10点&#x27;, &#x27;11点&#x27;, &#x27;12点&#x27;, &#x27;13点&#x27;, &#x27;14点&#x27;, &#x27;15点&#x27;, &#x27;16点&#x27;, &#x27;17点&#x27;, &#x27;18点&#x27;, &#x27;19点&#x27;, &#x27;20点&#x27;, &#x27;21点&#x27;, &#x27;22点&#x27;, &#x27;23点&#x27; ]; var seriesData = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]; var option = &#123; title: &#123; text: me.etext, subtext: me.esubtext &#125;, tooltip: &#123; trigger: me.etrigger || &#x27;axis&#x27;, axisPointer: &#123; // 坐标轴指示器，坐标轴触发有效 type: me.eaxisPointer || &#x27;line&#x27; // 默认为直线，可选为：&#x27;line&#x27; | &#x27;shadow&#x27; &#125; &#125;, legend: &#123; data: me.elegendata &#125;, /*工具箱，每个图表最多仅有一个工具箱*/ toolbox: &#123; show: true, feature: me.efeature || &#123; mark: &#123; show: false &#125;, dataView: &#123; show: false, readOnly: false &#125;, magicType: &#123; show: true, type: [&#x27;line&#x27;, &#x27;bar&#x27;] &#125;, restore: &#123; show: false &#125; &#125; &#125;, // 坐标网格设置 grid: &#123; left: &#x27;3%&#x27;, right: &#x27;4%&#x27;, bottom: &#x27;3%&#x27;, containLabel: true &#125;, xAxis: me.exAxis || [ &#123; data: me.exAxisdata || axisdata, type: me.exAxisType || &#x27;category&#x27; &#125; ], /*直角坐标系中纵轴数组*/ yAxis: me.eyAxis || [ &#123; type: &#x27;value&#x27;, min: &#x27;dataMin&#x27;, minInterval: 1 &#125; ], /*驱动图表生成的数据内容数组*/ series: me.eseries || [ &#123; name: &#x27;PC&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, barWidth: 60, data: seriesData &#125;, &#123; name: &#x27;触屏&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: seriesData &#125;, &#123; name: &#x27;IOS&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: seriesData &#125;, &#123; name: &#x27;微信&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: seriesData &#125;, &#123; name: &#x27;安卓&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: seriesData, markPoint: &#123; data: [ &#123;type: &#x27;max&#x27;, name: &#x27;最大值&#x27;&#125;, &#123;type: &#x27;min&#x27;, name: &#x27;最小值&#x27;&#125; ] &#125;, markLine: &#123; data: [ &#123;type: &#x27;average&#x27;, name: &#x27;平均值&#x27;&#125; ] &#125; &#125; ] &#125;; // 刷新 me.echarts.setOption(option); &#125;&#125;); 创建显示面板，将上面定义的组件初始化进去。 //图表显示面板var stackedEchartspanel = new Ext.ux.chart.stackedHistograms(&#123; etext: &#x27;今日平台注册人数实时监控&#x27;, esubtext: &#x27;实时数据&#x27;, eaxisPointer: &#x27;line&#x27;, exAxisType: &#x27;category&#x27;, elegendata: [&#x27;PC&#x27;, &#x27;触屏&#x27;, &#x27;安卓&#x27;, &#x27;IOS&#x27;, &#x27;微信&#x27;], exAxisdata: [&#x27;0点&#x27;, &#x27;1点&#x27;, &#x27;2点&#x27;, &#x27;3点&#x27;, &#x27;4点&#x27;, &#x27;5点&#x27;, &#x27;6点&#x27;, &#x27;7点&#x27;, &#x27;8点&#x27;, &#x27;9点&#x27;, &#x27;10点&#x27;, &#x27;11点&#x27;, &#x27;12点&#x27;, &#x27;13点&#x27;, &#x27;14点&#x27;, &#x27;15点&#x27;, &#x27;16点&#x27;, &#x27;17点&#x27;, &#x27;18点&#x27;, &#x27;19点&#x27;, &#x27;20点&#x27;, &#x27;21点&#x27;, &#x27;22点&#x27;, &#x27;23点&#x27; ], eseriesname: &#x27;注册量（人）&#x27;, eseries: [ &#123; name: &#x27;PC&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, barWidth: 40, data: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] &#125;, &#123; name: &#x27;触屏&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] &#125;, &#123; name: &#x27;IOS&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] &#125;, &#123; name: &#x27;微信&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] &#125;, &#123; name: &#x27;安卓&#x27;, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, data: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], markPoint: &#123; data: [ &#123;type: &#x27;max&#x27;, name: &#x27;最大值&#x27;&#125;, &#123;type: &#x27;min&#x27;, name: &#x27;最小值&#x27;&#125; ] &#125;, markLine: &#123; data: [ &#123;type: &#x27;average&#x27;, name: &#x27;平均值&#x27;&#125; ] &#125; &#125; ]&#125;); 创建一个容器，将上面的Panel放进去，并且调用后台接口定时更新数据。 /** * Created by wuzguo on 2017/06/14 14:02:23 * 日志监控 -&gt; 注册人数监控 */Ext.onReady(function () &#123; //主界面 var mainPanel = Ext.create(&#x27;Ext.panel.Panel&#x27;, &#123; width: &#x27;100%&#x27;, height: &#x27;100%&#x27;, layout: &#x27;card&#x27;, id: &#x27;mainPanelId&#x27;, bodyStyle: &quot;background-color: white&quot;, border: false, autoScroll: true, plain: true, // items: [echartspanel] items: [stackedEchartspanel] &#125;); // 容器 var viewport = Ext.create(&#x27;Ext.container.Viewport&#x27;, &#123; layout: &#x27;fit&#x27;, id: &#x27;mainFrame&#x27;, border: false, bodyStyle: &quot;background-color: white&quot;, plain: true, autoScroll: true, items: [mainPanel] &#125;); var stackedTask = &#123; run: function () &#123; Ext.Ajax.request(&#123; url: basePath + &#x27;/admin/ucweb/statRegistUser&#x27;, method: &quot;GET&quot;, timeout: 30000, success: function (res) &#123; // console.log(&quot;res: &quot; + JSON.stringify(res)); if (res.status == 200) &#123; var result = res.responseText; var exAxisdata = [&#x27;0点&#x27;, &#x27;1点&#x27;, &#x27;2点&#x27;, &#x27;3点&#x27;, &#x27;4点&#x27;, &#x27;5点&#x27;, &#x27;6点&#x27;, &#x27;7点&#x27;, &#x27;8点&#x27;, &#x27;9点&#x27;, &#x27;10点&#x27;, &#x27;11点&#x27;, &#x27;12点&#x27;, &#x27;13点&#x27;, &#x27;14点&#x27;, &#x27;15点&#x27;, &#x27;16点&#x27;, &#x27;17点&#x27;, &#x27;18点&#x27;, &#x27;19点&#x27;, &#x27;20点&#x27;, &#x27;21点&#x27;, &#x27;22点&#x27;, &#x27;23点&#x27; ]; var eseriesdata = []; var retJson = JSON.parse(result); // console.log(&quot;retJson: &quot; + retJson); if (retJson.success &amp;&amp; retJson.data) &#123; var rest = retJson.data; // console.log(&quot;rest: &quot; + rest); for (var key in rest) &#123; // 将对象转换为数组 var indexData = rest[key]; // 给默认值，防止数据为空 var tempData = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]; for (var k in indexData) &#123; for (var i = 0; i &lt; 24; i++) &#123; if ((k + &quot;点&quot;) == exAxisdata[i]) &#123; // 由于数据量太小，这里加上随机数 tempData[i] = indexData[k] + parseInt(Math.random() * 1000); &#125; &#125; &#125; eseriesdata.push(&#123; name: key, type: &#x27;bar&#x27;, stack: &#x27;注册&#x27;, barWidth: 40, data: tempData &#125;); &#125; // 最上的堆叠块显示最大值，最小值以及平均值 var index = eseriesdata.length - 1; eseriesdata[index].markPoint = &#123; data: [ &#123;type: &#x27;max&#x27;, name: &#x27;最大值&#x27;&#125;, &#123;type: &#x27;min&#x27;, name: &#x27;最小值&#x27;&#125; ] &#125;; eseriesdata[index].markLine = &#123; data: [ &#123;type: &#x27;average&#x27;, name: &#x27;平均值&#x27;&#125; ] &#125; // console.log(&quot;eseriesdata: &quot; + JSON.stringify(eseriesdata)); // 刷新表格中的数据 stackedEchartspanel.update(exAxisdata, eseriesdata); &#125; &#125; &#125; &#125;); &#125;, // 刷新频率（10秒） interval: 10000 &#125; // 启动定时服务去后台查询数据 Ext.TaskManager.start(stackedTask);&#125;); 以上就是关键代码，然后不管你是通过JSP还是HTML将Echarts的库文件引进来，然后就可以正常显示图表了。以下就是展示效果：","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"ECharts","slug":"ECharts","permalink":"http://wuzguo.com/blog/tags/ECharts/"},{"name":"ExtJs","slug":"ExtJs","permalink":"http://wuzguo.com/blog/tags/ExtJs/"}],"author":"Zak"},{"title":"Spark学习笔记之Spark Streaming","slug":"bigdata/spark_studynotes_streaming","date":"2017-08-05T14:35:12.000Z","updated":"2022-08-01T06:35:23.710Z","comments":true,"path":"2017/08/05/bigdata/spark_studynotes_streaming.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/05/bigdata/spark_studynotes_streaming.html","excerpt":"","text":"Spark StreamingSpark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。在“One Stack rule them all”的基础上，还可以使用Spark的其他子框架，如集群学习、图计算等，对流数据进行处理。 Spark Streaming处理的数据流图： Spark的各个子框架，都是基于核心Spark的，Spark Streaming在内部的处理机制是，接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据，最终得到处理后的一批批结果数据。 对应的批数据，在Spark内核对应一个RDD实例，因此，对应流数据的DStream可以看成是一组RDDs，即RDD的一个序列。通俗点理解的话，在流数据分成一批一批后，通过一个先进先出的队列，然后 Spark Engine从该队列中依次取出一个个批数据，把批数据封装成一个RDD，然后进行处理，这是一个典型的生产者消费者模型，对应的就有生产者消费者模型的问题，即如何协调生产速率和消费速率。 术语定义 离散流（discretized stream）或DStream：这是Spark Streaming对内部持续的实时数据流的抽象描述，即我们处理的一个实时数据流，在Spark Streaming中对应于一个DStream 实例。 批数据（batch data）：这是化整为零的第一步，将实时流数据以时间片为单位进行分批，将流处理转化为时间片数据的批处理。随着持续时间的推移，这些处理结果就形成了对应的结果数据流了。 时间片或批处理时间间隔（ batch interval）：这是人为地对流数据进行定量的标准，以时间片作为我们拆分流数据的依据。一个时间片的数据对应一个RDD实例。 窗口长度（window length）：一个窗口覆盖的流数据的时间长度。必须是批处理时间间隔的倍数， 滑动时间间隔：前一个窗口到后一个窗口所经过的时间长度。必须是批处理时间间隔的倍数 Input DStream :一个input DStream是一个特殊的DStream，将Spark Streaming连接到一个外部数据源来读取数据。 运行原理 Streaming架构 SparkStreaming是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kdfka、Flume、Twitter、Zero和TCP 套接字）进行类似Map、Reduce和Join等复杂操作，并将结果保存到外部文件系统、数据库或应用到实时仪表盘。 计算流程：Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark Core，也就是把Spark Streaming的输入数据按照batch size（如1秒）分成一段一段的数据（Discretized Stream），每一段数据都转换成Spark中的RDD（Resilient Distributed Dataset），然后将Spark Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加或者存储到外部设备。下图显示了Spark Streaming的整个流程。 容错性：对于流式计算来说，容错性至关重要。首先我们要明确一下Spark中RDD的容错机制。每一个RDD都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（lineage），所以只要输入数据是可容错的，那么任意一个RDD的分区（Partition）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。 对于Spark Streaming来说，其RDD的传承关系如下图所示，图中的每一个椭圆形表示一个RDD，椭圆形中的每个圆形代表一个RDD中的一个Partition，图中的每一列的多个RDD表示一个DStream（图中有三个DStream），而每一行最后一个RDD则表示每一个Batch Size所产生的中间结果RDD。我们可以看到图中的每一个RDD都是通过lineage相连接的，由于Spark Streaming输入数据可以来自于磁盘，例如HDFS（多份拷贝）或是来自于网络的数据流（Spark Streaming会将网络输入数据的每一个数据流拷贝两份到其他的机器）都能保证容错性，所以RDD中任意的Partition出错，都可以并行地在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（如Storm）的效率更高。 实时性：对于实时性的讨论，会牵涉到流式处理框架的应用场景。Spark Streaming将流式计算分解成多个Spark Job，对于每一段数据的处理都会经过Spark DAG图分解以及Spark的任务集的调度过程。对于目前版本的Spark Streaming而言，其最小的Batch Size的选取在0.5~2秒钟之间（Storm目前最小的延迟是100ms左右），所以Spark Streaming能够满足除对实时性要求非常高（如高频实时交易）之外的所有流式准实时计算场景。 扩展性与吞吐量：Spark目前在EC2上已能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理6GB&#x2F;s的数据量（60M records&#x2F;s），其吞吐量也比流行的Storm高2～5倍，图4是Berkeley利用WordCount和Grep两个用例所做的测试，在Grep这个测试中，Spark Streaming中的每个节点的吞吐量是670k records&#x2F;s，而Storm是115k records&#x2F;s。 编程模型DStream（Discretized Stream）作为Spark Streaming的基础抽象，它代表持续性的数据流。这些数据流既可以通过外部输入源赖获取，也可以通过现有的Dstream的transformation操作来获得。在内部实现上，DStream由一组时间序列上连续的RDD来表示。每个RDD都包含了自己特定时间间隔内的数据流。如图7-3所示。 对DStream中数据的各种操作也是映射到内部的RDD上来进行的，如图7-4所示，对Dtream的操作可以通过RDD的transformation生成新的DStream。这里的执行引擎是Spark。 如何使用Spark Streaming作为构建于Spark之上的应用框架，Spark Streaming承袭了Spark的编程风格，对于已经了解Spark的用户来说能够快速地上手。接下来以Spark Streaming官方提供的WordCount代码为例来介绍Spark Streaming的使用方式。 import org.apache.spark._import org.apache.spark.streaming._import org.apache.spark.streaming.StreamingContext._// Create a local StreamingContext with two working thread and batch interval of 1 second.// The master requires 2 cores to prevent from a starvation scenario.val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;NetworkWordCount&quot;)val ssc = new StreamingContext(conf, Seconds(1))// Create a DStream that will connect to hostname:port, like localhost:9999val lines = ssc.socketTextStream(&quot;localhost&quot;, 9999)// Split each line into wordsval words = lines.flatMap(_.split(&quot; &quot;))import org.apache.spark.streaming.StreamingContext._// Count each word in each batchval pairs = words.map(word =&gt; (word, 1))val wordCounts = pairs.reduceByKey(_ + _)// Print the first ten elements of each RDD generated in this DStream to the consolewordCounts.print()ssc.start() // Start the computationssc.awaitTermination() // Wait for the computation to terminate 创建StreamingContext对象 同Spark初始化需要创建SparkContext对象一样，使用Spark Streaming就需要创建StreamingContext对象。创建StreamingContext对象所需的参数与SparkContext基本一致，包括指明Master，设定名称(如NetworkWordCount)。需要注意的是参数Seconds(1)，Spark Streaming需要指定处理数据的时间间隔，如上例所示的1s，那么Spark Streaming会以1s为时间窗口进行数据处理。此参数需要根据用户的需求和集群的处理能力进行适当的设置； 创建InputDStream如同Storm的Spout，Spark Streaming需要指明数据源。如上例所示的socketTextStream，Spark Streaming以socket连接作为数据源读取数据。当然Spark Streaming支持多种不同的数据源，包括Kafka、 Flume、HDFS&#x2F;S3、Kinesis和Twitter等数据源； 操作DStream对于从数据源得到的DStream，用户可以在其基础上进行各种操作，如上例所示的操作就是一个典型的WordCount执行流程：对于当前时间窗口内从数据源得到的数据首先进行分割，然后利用Map和ReduceByKey方法进行计算，当然最后还有使用print()方法输出结果； 启动Spark Streaming之前所作的所有步骤只是创建了执行流程，程序没有真正连接上数据源，也没有对数据进行任何操作，只是设定好了所有的执行计划，当ssc.start()启动后程序才真正进行所有预期的操作。 至此对于Spark Streaming的如何使用有了一个大概的印象，在后面的章节我们会通过源代码深入探究一下Spark Streaming的执行流程。 DStream的输入源在Spark Streaming中所有的操作都是基于流的，而输入源是这一系列操作的起点。输入 DStreams 和 DStreams 接收的流都代表输入数据流的来源，在Spark Streaming 提供两种内置数据流来源： 基础来源 在 StreamingContext API 中直接可用的来源。例如：文件系统、Socket（套接字）连接和 Akka actors； 高级来源 如 Kafka、Flume、Kinesis、Twitter 等，可以通过额外的实用工具类创建。 基础来源 ​ 在前面分析怎样使用Spark Streaming的例子中我们已看到ssc.socketTextStream()方法，可以通过 TCP 套接字连接，从从文本数据中创建了一个 DStream。除了套接字，StreamingContext 的API还提供了方法从文件和 Akka actors 中创建 DStreams作为输入源。 ​ Spark Streaming提供了streamingContext.fileStream(dataDirectory)方法可以从任何文件系统(如：HDFS、S3、NFS 等）的文件中读取数据，然后创建一个DStream。Spark Streaming 监控 dataDirectory 目录和在该目录下任何文件被创建处理(不支持在嵌套目录下写文件)。需要注意的是：读取的必须是具有相同的数据格式的文件；创建的文件必须在 dataDirectory 目录下，并通过自动移动或重命名成数据目录；文件一旦移动就不能被改变，如果文件被不断追加,新的数据将不会被阅读。对于简单的文本文，可以使用一个简单的方法streamingContext.textFileStream(dataDirectory)来读取数据。 Spark Streaming也可以基于自定义 Actors 的流创建DStream ，通过 Akka actors 接受数据流，使用方法streamingContext.actorStream(actorProps, actor-name)。Spark Streaming使用 streamingContext.queueStream(queueOfRDDs)方法可以创建基于 RDD 队列的DStream，每个RDD 队列将被视为 DStream 中一块数据流进行加工处理。 高级来源 这一类的来源需要外部 non-Spark 库的接口，其中一些有复杂的依赖关系(如 Kafka、Flume)。因此通过这些来源创建 DStreams 需要明确其依赖。例如，如果想创建一个使用 Twitter tweets 的数据的DStream 流，必须按以下步骤来做： 在 SBT 或 Maven工程里添加 spark-streaming-twitter_2.10 依赖。 开发：导入 TwitterUtils 包，通过 TwitterUtils.createStream 方法创建一个DStream。 部署：添加所有依赖的 jar 包(包括依赖的spark-streaming-twitter_2.10 及其依赖)，然后部署应用程序。 需要注意的是，这些高级的来源一般在Spark Shell中不可用，因此基于这些高级来源的应用不能在Spark Shell中进行测试。如果你必须在Spark shell中使用它们，你需要下载相应的Maven工程的Jar依赖并添加到类路径中。 其中一些高级来源如下： Twitter Spark Streaming的TwitterUtils工具类使用Twitter4j，Twitter4J 库支持通过任何方法提供身份验证信息，你可以得到公众的流，或得到基于关键词过滤流。 Flume Spark Streaming可以从Flume中接受数据。 Kafka Spark Streaming可以从Kafka中接受数据。 Kinesis Spark Streaming可以从Kinesis中接受数据。 需要重申的一点是在开始编写自己的 SparkStreaming 程序之前，一定要将高级来源依赖的Jar添加到SBT 或 Maven 项目相应的artifact中。常见的输入源和其对应的Jar包如下图所示。 另外，输入DStream也可以创建自定义的数据源，需要做的就是实现一个用户定义的接收器。 DStream的操作与RDD类似，DStream也提供了自己的一系列操作方法，这些操作可以分成三类：普通的转换操作、窗口转换操作和输出操作。 普通的转换操作 普通的转换操作如下表所示： 转换 描述 map(func) 源 DStream的每个元素通过函数func返回一个新的DStream。 flatMap(func) 类似与map操作，不同的是每个输入元素可以被映射出0或者更多的输出元素。 filter(func) 在源DSTREAM上选择Func函数返回仅为true的元素,最终返回一个新的DSTREAM 。 repartition(numPartitions) 通过输入的参数numPartitions的值来改变DStream的分区大小。 union(otherStream) 返回一个包含源DStream与其他 DStream的元素合并后的新DSTREAM。 count() 对源DStream内部的所含有的RDD的元素数量进行计数，返回一个内部的RDD只包含一个元素的DStreaam。 reduce(func) 使用函数func（有两个参数并返回一个结果）将源DStream 中每个RDD的元素进行聚 合操作,返回一个内部所包含的RDD只有一个元素的新DStream。 countByValue() 计算DStream中每个RDD内的元素出现的频次并返回新的DStream[(K,Long)]，其中K是RDD中元素的类型，Long是元素出现的频次。 reduceByKey(func, [numTasks]) 当一个类型为（K，V）键值对的DStream被调用的时候,返回类型为类型为（K，V）键值对的新 DStream,其中每个键的值V都是使用聚合函数func汇总。注意：默认情况下，使用 Spark的默认并行度提交任务（本地模式下并行度为2，集群模式下位8），可以通过配置numTasks设置不同的并行任务数。 join(otherStream, [numTasks]) 当被调用类型分别为（K，V）和（K，W）键值对的2个DStream时，返回类型为（K，（V，W））键值对的一个新 DSTREAM。 cogroup(otherStream, [numTasks]) 当被调用的两个DStream分别含有(K, V) 和(K, W)键值对时,返回一个(K, Seq[V], Seq[W])类型的新的DStream。 transform(func) 通过对源DStream的每RDD应用RDD-to-RDD函数返回一个新的DStream，这可以用来在DStream做任意RDD操作。 updateStateByKey(func) 返回一个新状态的DStream,其中每个键的状态是根据键的前一个状态和键的新值应用给定函数func后的更新。这个方法可以被用来维持每个键的任何状态数据。 在上面列出的这些操作中，transform()方法和updateStateByKey()方法值得我们深入的探讨一下： transform(func)操作 该transform操作（转换操作）连同其其类似的 transformWith操作允许DStream 上应用任意RDD-to-RDD函数。它可以被应用于未在 DStream API 中暴露任何的RDD操作。例如，在每批次的数据流与另一数据集的连接功能不直接暴露在DStream API 中，但可以轻松地使用transform操作来做到这一点，这使得DStream的功能非常强大。例如，你可以通过连接预先计算的垃圾邮件信息的输入数据流（可能也有Spark生成的），然后基于此做实时数据清理的筛选，如下面官方提供的伪代码所示。事实上，也可以在transform方法中使用机器学习和图形计算的算法。 updateStateByKey操作 该 updateStateByKey 操作可以让你保持任意状态，同时不断有新的信息进行更新。要使用此功能，必须进行两个步骤 ： 定义状态 - 状态可以是任意的数据类型。 定义状态更新函数 - 用一个函数指定如何使用先前的状态和从输入流中获取的新值 更新状态。 让我们用一个例子来说明，假设你要进行文本数据流中单词计数。在这里，正在运行的计数是状态而且它是一个整数。我们定义了更新功能如下： 此函数应用于含有键值对的DStream中（如前面的示例中，在DStream中含有（word，1）键值对）。它会针对里面的每个元素（如wordCount中的word）调用一下更新函数，newValues是最新的值，runningCount是之前的值。 窗口转换操作 Spark Streaming 还提供了窗口的计算，它允许你通过滑动窗口对数据进行转换，窗口转换操作如下： 转换 描述 window(windowLength, slideInterval) 返回一个基于源DStream的窗口批次计算后得到新的DStream。 countByWindow(windowLength,slideInterval) 返回基于滑动窗口的DStream中的元素的数量。 reduceByWindow(func, windowLength,slideInterval) 基于滑动窗口对源DStream中的元素进行聚合操作，得到一个新的DStream。 reduceByKeyAndWindow(func,windowLength,slideInterval, [numTasks]) 基于滑动窗口对（K，V）键值对类型的DStream中的值按K使用聚合函数func进行聚合操作，得到一个新的DStream。 reduceByKeyAndWindow(func,invFunc,windowLength, slideInterval, [numTasks]) 一个更高效的reduceByKkeyAndWindow()的实现版本，先对滑动窗口中新的时间间隔内数据增量聚合并移去最早的与新增数据量的时间间隔内的数据统计量。例如，计算t+4秒这个时刻过去5秒窗口的WordCount，那么我们可以将t+3时刻过去5秒的统计量加上[t+3，t+4]的统计量，在减去[t-2，t-1]的统计量，这种方法可以复用中间三秒的统计量，提高统计的效率。 countByValueAndWindow(windowLength,slideInterval, [numTasks]) 基于滑动窗口计算源DStream中每个RDD内每个元素出现的频次并返回DStream[(K,Long)]，其中K是RDD中元素的类型，Long是元素频次。与countByValue一样，reduce任务的数量可以通过一个可选参数进行配置。 在Spark Streaming中，数据处理是按批进行的，而数据采集是逐条进行的，因此在Spark Streaming中会先设置好批处理间隔（batch duration），当超过批处理间隔的时候就会把采集到的数据汇总起来成为一批数据交给系统去处理。 对于窗口操作而言，在其窗口内部会有N个批处理数据，批处理数据的大小由窗口间隔（window duration）决定，而窗口间隔指的就是窗口的持续时间，在窗口操作中，只有窗口的长度满足了才会触发批数据的处理。除了窗口的长度，窗口操作还有另一个重要的参数就是滑动间隔（slide duration），它指的是经过多长时间窗口滑动一次形成新的窗口，滑动窗口默认情况下和批次间隔的相同，而窗口间隔一般设置的要比它们两个大。在这里必须注意的一点是滑动间隔和窗口间隔的大小一定得设置为批处理间隔的整数倍。 如批处理间隔示意图所示，批处理间隔是1个时间单位，窗口间隔是3个时间单位，滑动间隔是2个时间单位。对于初始的窗口time 1-time 3，只有窗口间隔满足了才触发数据的处理。这里需要注意的一点是，初始的窗口有可能流入的数据没有撑满，但是随着时间的推进，窗口最终会被撑满。当每个2个时间单位，窗口滑动一次后，会有新的数据流入窗口，这时窗口会移去最早的两个时间单位的数据，而与最新的两个时间单位的数据进行汇总形成新的窗口（time3-time5）。 对于窗口操作，批处理间隔、窗口间隔和滑动间隔是非常重要的三个时间概念，是理解窗口操作的关键所在。 输出操作 Spark Streaming允许DStream的数据被输出到外部系统，如数据库或文件系统。由于输出操作实际上使transformation操作后的数据可以通过外部系统被使用，同时输出操作触发所有DStream的transformation操作的实际执行（类似于RDD操作）。以下表列出了目前主要的输出操作： 转换 描述 print() 在Driver中打印出DStream中数据的前10个元素。 saveAsTextFiles(prefix, [suffix]) 将DStream中的内容以文本的形式保存为文本文件，其中每次批处理间隔内产生的文件以prefix-TIME_IN_MS[.suffix]的方式命名。 saveAsObjectFiles(prefix, [suffix]) 将DStream中的内容按对象序列化并且以SequenceFile的格式保存。其中每次批处理间隔内产生的文件以prefix-TIME_IN_MS[.suffix]的方式命名。 saveAsHadoopFiles(prefix, [suffix]) 将DStream中的内容以文本的形式保存为Hadoop文件，其中每次批处理间隔内产生的文件以prefix-TIME_IN_MS[.suffix]的方式命名。 foreachRDD(func) 最基本的输出操作，将func函数应用于DStream中的RDD上，这个操作会输出数据到外部系统，比如保存RDD到文件或者网络数据库等。需要注意的是func函数是在运行该streaming应用的Driver进程里执行的。 dstream.foreachRDD是一个非常强大的输出操作，它允将许数据输出到外部系统。但是 ，如何正确高效地使用这个操作是很重要的，下面展示了如何去避免一些常见的错误。 通常将数据写入到外部系统需要创建一个连接对象（如 TCP连接到远程服务器），并用它来发送数据到远程系统。出于这个目的，开发者可能在不经意间在Spark driver端创建了连接对象，并尝试使用它保存RDD中的记录到Spark worker上，如下面代码： 这是不正确的，这需要连接对象进行序列化并从Driver端发送到Worker上。连接对象很少在不同机器间进行这种操作，此错误可能表现为序列化错误（连接对不可序列化），初始化错误（连接对象在需要在Worker 上进行需要初始化） 等等，正确的解决办法是在 worker上创建的连接对象。 通常情况下，创建一个连接对象有时间和资源开销。因此，创建和销毁的每条记录的连接对象可能招致不必要的资源开销，并显著降低系统整体的吞吐量 。一个更好的解决方案是使用rdd.foreachPartition方法创建一个单独的连接对象，然后使用该连接对象输出的所有RDD分区中的数据到外部系统。 这缓解了创建多条记录连接的开销。最后，还可以进一步通过在多个RDDs&#x2F; batches上重用连接对象进行优化。一个保持连接对象的静态池可以重用在多个批处理的RDD上将其输出到外部系统，从而进一步降低了开销。 需要注意的是，在静态池中的连接应该按需延迟创建，这样可以更有效地把数据发送到外部系统。另外需要要注意的是：DStreams延迟执行的，就像RDD的操作是由actions触发一样。默认情况下，输出操作会按照它们在Streaming应用程序中定义的顺序一个个执行。 容错、持久化和性能调优 容错 DStream基于RDD组成，RDD的容错性依旧有效，我们首先回忆一下SparkRDD的基本特性。 RDD是一个不可变的、确定性的可重复计算的分布式数据集。RDD的某些partition丢失了，可以通过血统（lineage）信息重新计算恢复； 如果RDD任何分区因worker节点故障而丢失，那么这个分区可以从原来依赖的容错数据集中恢复； 由于Spark中所有的数据的转换操作都是基于RDD的，即使集群出现故障，只要输入数据集存在，所有的中间结果都是可以被计算的。 Spark Streaming是可以从HDFS和S3这样的文件系统读取数据的，这种情况下所有的数据都可以被重新计算，不用担心数据的丢失。但是在大多数情况下，Spark Streaming是基于网络来接受数据的，此时为了实现相同的容错处理，在接受网络的数据时会在集群的多个Worker节点间进行数据的复制（默认的复制数是2），这导致产生在出现故障时被处理的两种类型的数据： Data received and replicated ：一旦一个Worker节点失效，系统会从另一份还存在的数据中重新计算。 Data received but buffered for replication ：一旦数据丢失，可以通过RDD之间的依赖关系，从HDFS这样的外部文件系统读取数据。 此外，有两种故障，我们应该关心： Worker节点失效：通过上面的讲解我们知道，这时系统会根据出现故障的数据的类型，选择是从另一个有复制过数据的工作节点上重新计算，还是直接从从外部文件系统读取数据。 Driver（驱动节点）失效 ：如果运行 Spark Streaming应用时驱动节点出现故障，那么很明显的StreamingContext已经丢失，同时在内存中的数据全部丢失。对于这种情况，Spark Streaming应用程序在计算上有一个内在的结构——在每段micro-batch数据周期性地执行同样的Spark计算。这种结构允许把应用的状态（亦称checkpoint）周期性地保存到可靠的存储空间中，并在driver重新启动时恢复该状态。具体做法是在ssc.checkpoint()函数中进行设置，Spark Streaming就会定期把DStream的元信息写入到HDFS中，一旦驱动节点失效，丢失的StreamingContext会通过已经保存的检查点信息进行恢复。 最后我们谈一下Spark Stream的容错在Spark 1.2版本的一些改进： 实时流处理系统必须要能在24&#x2F;7时间内工作，因此它需要具备从各种系统故障中恢复过来的能力。最开始，SparkStreaming就支持从driver和worker故障恢复的能力。然而有些数据源的输入可能在故障恢复以后丢失数据。在Spark1.2版本中，Spark已经在SparkStreaming中对预写日志（也被称为journaling）作了初步支持，改进了恢复机制，并使更多数据源的零数据丢失有了可靠。 对于文件这样的源数据，driver恢复机制足以做到零数据丢失，因为所有的数据都保存在了像HDFS或S3这样的容错文件系统中了。但对于像Kafka和Flume等其它数据源，有些接收到的数据还只缓存在内存中，尚未被处理，它们就有可能会丢失。这是由于Spark应用的分布操作方式引起的。当driver进程失败时，所有在standalone&#x2F;yarn&#x2F;mesos集群运行的executor，连同它们在内存中的所有数据，也同时被终止。对于Spark Streaming来说，从诸如Kafka和Flume的数据源接收到的所有数据，在它们处理完成之前，一直都缓存在executor的内存中。纵然driver重新启动，这些缓存的数据也不能被恢复。为了避免这种数据损失，在Spark1.2发布版本中引进了预写日志（WriteAheadLogs）功能。 预写日志功能的流程是： 一个SparkStreaming应用开始时（也就是driver开始时），相关的StreamingContext使用SparkContext启动接收器成为长驻运行任务。这些接收器接收并保存流数据到Spark内存中以供处理。 接收器通知driver。 接收块中的元数据（metadata）被发送到driver的StreamingContext。这个元数据包括：（a）定位其在executor内存中数据的块referenceid，（b）块数据在日志中的偏移信息（如果启用了）。 用户传送数据的生命周期如下图所示。 类似Kafka这样的系统可以通过复制数据保持可靠性。允许预写日志两次高效地复制同样的数据：一次由Kafka，而另一次由SparkStreaming。Spark未来版本将包含Kafka容错机制的原生支持，从而避免第二个日志。 持久化 与RDD一样，DStream同样也能通过persist()方法将数据流存放在内存中，默认的持久化方式是MEMORY_ONLY_SER，也就是在内存中存放数据同时序列化的方式，这样做的好处是遇到需要多次迭代计算的程序时，速度优势十分的明显。而对于一些基于窗口的操作，如reduceByWindow、reduceByKeyAndWindow，以及基于状态的操作，如updateStateBykey，其默认的持久化策略就是保存在内存中。 对于来自网络的数据源（Kafka、Flume、sockets等），默认的持久化策略是将数据保存在两台机器上，这也是为了容错性而设计的。 另外，对于窗口和有状态的操作必须checkpoint，通过StreamingContext的checkpoint来指定目录，通过 Dtream的checkpoint指定间隔时间，间隔必须是滑动间隔（slide interval）的倍数。 性能调优 优化运行时间 增加并行度 确保使用整个集群的资源，而不是把任务集中在几个特定的节点上。对于包含shuffle的操作，增加其并行度以确保更为充分地使用集群资源； 减少数据序列化，反序列化的负担 Spark Streaming默认将接受到的数据序列化后存储，以减少内存的使用。但是序列化和反序列话需要更多的CPU时间，因此更加高效的序列化方式（Kryo）和自定义的系列化接口可以更高效地使用CPU； 设置合理的batch duration（批处理时间间） 在Spark Streaming中，Job之间有可能存在依赖关系，后面的Job必须确保前面的作业执行结束后才能提交。若前面的Job执行的时间超出了批处理时间间隔，那么后面的Job就无法按时提交，这样就会进一步拖延接下来的Job，造成后续Job的阻塞。因此设置一个合理的批处理间隔以确保作业能够在这个批处理间隔内结束时必须的； 减少因任务提交和分发所带来的负担 通常情况下，Akka框架能够高效地确保任务及时分发，但是当批处理间隔非常小（500ms）时，提交和分发任务的延迟就变得不可接受了。使用Standalone和Coarse-grained Mesos模式通常会比使用Fine-grained Mesos模式有更小的延迟。 优化内存使用 控制batch size（批处理间隔内的数据量） Spark Streaming会把批处理间隔内接收到的所有数据存放在Spark内部的可用内存区域中，因此必须确保当前节点Spark的可用内存中少能容纳这个批处理时间间隔内的所有数据，否则必须增加新的资源以提高集群的处理能力； 及时清理不再使用的数据 前面讲到Spark Streaming会将接受的数据全部存储到内部可用内存区域中，因此对于处理过的不再需要的数据应及时清理，以确保Spark Streaming有富余的可用内存空间。通过设置合理的spark.cleaner.ttl时长来及时清理超时的无用数据，这个参数需要小心设置以免后续操作中所需要的数据被超时错误处理； 观察及适当调整GC策略 GC会影响Job的正常运行，可能延长Job的执行时间，引起一系列不可预料的问题。观察GC的运行情况，采用不同的GC策略以进一步减小内存回收对Job运行的影响。","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://wuzguo.com/blog/categories/bigdata/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://wuzguo.com/blog/tags/Spark/"},{"name":"日志分析","slug":"日志分析","permalink":"http://wuzguo.com/blog/tags/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"name":"Spark Streaming","slug":"Spark-Streaming","permalink":"http://wuzguo.com/blog/tags/Spark-Streaming/"}],"author":"Zak"},{"title":"Spark学习笔记之SparkSQL","slug":"bigdata/spark_studynotes_sparksql","date":"2017-08-05T13:45:11.000Z","updated":"2022-08-01T06:35:23.724Z","comments":true,"path":"2017/08/05/bigdata/spark_studynotes_sparksql.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/05/bigdata/spark_studynotes_sparksql.html","excerpt":"","text":"SparkSQLSparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具，Hive应运而生，它是当时唯一运行在Hadoop上的SQL-on-Hadoop工具。但是MapReduce计算过程中大量的中间磁盘落地过程消耗了大量的I&#x2F;O，降低的运行效率，为了提高SQL-on-Hadoop的效率，大量的SQL-on-Hadoop工具开始产生，其中表现较为突出的是： MapR的Drill Cloudera的Impala Shark 其中Shark是伯克利实验室Spark生态环境的组件之一，它修改了下图所示的右下角的内存管理、物理计划、执行三个模块，并使之能运行在Spark引擎上，从而使得SQL查询的速度得到10-100倍的提升。 类似于关系型数据库，SparkSQL也是语句也是由Projection（a1，a2，a3）、Data Source（tableA）、Filter（condition）组成，分别对应sql查询过程中的Result、Data Source、Operation，也就是说SQL语句按Result–&gt;Data Source–&gt;Operation的次序来描述的。 当执行SparkSQL语句的顺序为： 对读入的SQL语句进行解析（Parse），分辨出SQL语句中哪些词是关键词（如SELECT、FROM、WHERE），哪些是表达式、哪些是Projection、哪些是Data Source等，从而判断SQL语句是否规范； 将SQL语句和数据库的数据字典（列、表、视图等等）进行绑定（Bind），如果相关的Projection、Data Source等都是存在的话，就表示这个SQL语句是可以执行的； 一般的数据库会提供几个执行计划，这些计划一般都有运行统计数据，数据库会在这些计划中选择一个最优计划（Optimize）； 计划执行（Execute），按Operation–&gt;Data Source–&gt;Result的次序来进行的，在执行过程有时候甚至不需要读取物理表就可以返回结果，比如重新运行刚运行过的SQL语句，可能直接从数据库的缓冲池中获取返回结果。 Tree和RuleSparkSQL对SQL语句的处理和关系型数据库对SQL语句的处理采用了类似的方法，首先会将SQL语句进行解析（Parse），然后形成一个Tree，在后续的如绑定、优化等处理过程都是对Tree的操作，而操作的方法是采用Rule，通过模式匹配，对不同类型的节点采用不同的操作。在整个sql语句的处理过程中，Tree和Rule相互配合，完成了解析、绑定（在SparkSQL中称为Analysis）、优化、物理计划等过程，最终生成可以执行的物理计划。 Tree Tree的相关代码定义在sql&#x2F;catalyst&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;sql&#x2F;catalyst&#x2F;trees Logical Plans、Expressions、Physical Operators都可以使用Tree表示 Tree的具体操作是通过TreeNode来实现的 SparkSQL定义了catalyst.trees的日志，通过这个日志可以形象的表示出树的结构 TreeNode可以使用scala的集合操作方法（如foreach, map, flatMap, collect等）进行操作 有了TreeNode，通过Tree中各个TreeNode之间的关系，可以对Tree进行遍历操作，如使用transformDown、transformUp将Rule应用到给定的树段，然后用结果替代旧的树段；也可以使用transformChildrenDown、transformChildrenUp对一个给定的节点进行操作，通过迭代将Rule应用到该节点以及子节点。 TreeNode可以细分成三种类型的Node： UnaryNode 一元节点，即只有一个子节点。如Limit、Filter操作 BinaryNode 二元节点，即有左右子节点的二叉节点。如Jion、Union操作 LeafNode 叶子节点，没有子节点的节点。主要用户命令类操作，如SetCommand Rule Rule的相关代码定义在sql&#x2F;catalyst&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;sql&#x2F;catalyst&#x2F;rules Rule在SparkSQL的Analyzer、Optimizer、SparkPlan等各个组件中都有应用到 Rule是一个抽象类，具体的Rule实现是通过RuleExecutor完成 Rule通过定义batch和batchs，可以简便的、模块化地对Tree进行transform操作 Rule通过定义Once和FixedPoint，可以对Tree进行一次操作或多次操作（如对某些Tree进行多次迭代操作的时候，达到FixedPoint次数迭代或达到前后两次的树结构没变化才停止操作，具体参看RuleExecutor.apply） ​ sqlContext和hiveContext的运行过程 SparkSQL有两个分支，sqlContext和hiveContext，sqlContext现在只支持SQL语法解析器（SQL-92语法）；hiveContext现在支持SQL语法解析器和hivesql语法解析器，默认为hiveSQL语法解析器，用户可以通过配置切换成SQL语法解析器，来运行hiveSQL不支持的语法， sqlContext的运行过程 sqlContext总的一个过程如下图所示： SQL语句经过SqlParse解析成UnresolvedLogicalPlan； 使用analyzer结合数据数据字典（catalog）进行绑定，生成resolvedLogicalPlan； 使用optimizer对resolvedLogicalPlan进行优化，生成optimizedLogicalPlan； 使用SparkPlan将LogicalPlan转换成PhysicalPlan； 使用prepareForExecution()将PhysicalPlan转换成可执行物理计划； 使用execute()执行可执行物理计划； 生成SchemaRDD。 在整个运行过程中涉及到多个SparkSQL的组件，如SqlParse、analyzer、optimizer、SparkPlan等等 hiveContext的运行过程 hiveContext总的一个过程如下图所示： SQL语句经过HiveQl.parseSql解析成Unresolved LogicalPlan，在这个解析过程中对hiveql语句使用getAst()获取AST树，然后再进行解析； 使用analyzer结合数据hive源数据Metastore（新的catalog）进行绑定，生成resolved LogicalPlan； 使用optimizer对resolved LogicalPlan进行优化，生成optimized LogicalPlan，优化前使用了ExtractPythonUdfs(catalog.PreInsertionCasts(catalog.CreateTables(analyzed)))进行预处理； 使用hivePlanner将LogicalPlan转换成PhysicalPlan； 使用prepareForExecution()将PhysicalPlan转换成可执行物理计划； 使用execute()执行可执行物理计划； 执行后，使用map(_.copy)将结果导入SchemaRDD。 catalyst优化器 SparkSQL1.1总体上由四个模块组成：core、catalyst、hive、hive-Thriftserver： core处理数据的输入输出，从不同的数据源获取数据（RDD、Parquet、json等），将查询结果输出成schemaRDD； catalyst处理查询语句的整个处理过程，包括解析、绑定、优化、物理计划等，说其是优化器，还不如说是查询引擎； hive对hive数据的处理 hive-ThriftServer提供CLI和JDBC&#x2F;ODBC接口 在这四个模块中，catalyst处于最核心的部分，其性能优劣将影响整体的性能。由于发展时间尚短，还有很多不足的地方，但其插件式的设计，为未来的发展留下了很大的空间。下面是catalyst的一个设计图： 其中虚线部分是以后版本要实现的功能，实线部分是已经实现的功能。从上图看，catalyst主要的实现组件有： sqlParse，完成sql语句的语法解析功能，目前只提供了一个简单的sql解析器； Analyzer，主要完成绑定工作，将不同来源的Unresolved LogicalPlan和数据元数据（如hive metastore、Schema catalog）进行绑定，生成resolved LogicalPlan； optimizer对resolved LogicalPlan进行优化，生成optimized LogicalPlan； Planner将LogicalPlan转换成PhysicalPlan； CostModel，主要根据过去的性能统计数据，选择最佳的物理执行计划 这些组件的基本实现方法： 先将sql语句通过解析生成Tree，然后在不同阶段使用不同的Rule应用到Tree上，通过转换完成各个组件的功能。 Analyzer使用Analysis Rules，配合数据元数据（如hive metastore、Schema catalog），完善Unresolved LogicalPlan的属性而转换成resolved LogicalPlan； optimizer使用Optimization Rules，对resolved LogicalPlan进行合并、列裁剪、过滤器下推等优化作业而转换成optimized LogicalPlan； lanner使用Planning Strategies，对optimized LogicalPlan SparkSQL CLI CLI（Command-Line Interface，命令行界面）是指可在用户提示符下键入可执行指令的界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后予以执行。Spark CLI指的是使用命令界面直接输入SQL命令，然后发送到Spark集群进行执行，在界面中显示运行过程和最终的结果。 Spark1.1相较于Spark1.0最大的差别就在于Spark1.1增加了Spark SQL CLI和ThriftServer，使得Hive用户还有用惯了命令行的RDBMS数据库管理员较容易地上手，真正意义上进入了SQL时代。","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://wuzguo.com/blog/categories/bigdata/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://wuzguo.com/blog/tags/Spark/"},{"name":"日志分析","slug":"日志分析","permalink":"http://wuzguo.com/blog/tags/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"http://wuzguo.com/blog/tags/SparkSQL/"}],"author":"Zak"},{"title":"Spark学习笔记之RDD","slug":"bigdata/spark_studynotes_rdd","date":"2017-08-04T13:12:34.000Z","updated":"2022-08-01T06:35:23.672Z","comments":true,"path":"2017/08/04/bigdata/spark_studynotes_rdd.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/04/bigdata/spark_studynotes_rdd.html","excerpt":"","text":"RDD术语定义 弹性分布式数据集（RDD）： Resillient Distributed Dataset，Spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作）； 有向无环图（DAG）：Directed Acycle graph，反应RDD之间的依赖关系； 有向无环图调度器（DAG Scheduler）：根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler； 任务调度器（Task Scheduler）：将Taskset提交给worker（集群）运行并回报结果； 窄依赖（Narrow dependency）：子RDD依赖于父RDD中固定的data partition； 宽依赖（Wide Dependency）：子RDD对父RDD中的所有data partition都有依赖。 RDD概念 RDD是Spark的最基本抽象,是对分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象实现。RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作。这对于迭代运算比较常见的机器学习算法, 交互式数据挖掘来说，效率提升非常大。 RDD 最适合那种在数据集上的所有元素都执行相同操作的批处理式应用。在这种情况下， RDD 只需记录血统中每个转换就能还原丢失的数据分区，而无需记录大量的数据操作日志。所以 RDD 不适合那些需要异步、细粒度更新状态的应用 ，比如 Web 应用的存储系统，或增量式的 Web 爬虫等。对于这些应用，使用具有事务更新日志和数据检查点的数据库系统更为高效。 RDD的特点 来源：一种是从持久存储获取数据，另一种是从其他RDD生成 只读：状态不可变，不能修改 分区：支持元素根据 Key 来分区 ( Partitioning ) ，保存到多个结点上，还原时只会重新计算丢失分区的数据，而不会影响整个系统 路径：在 RDD 中叫世族或血统 ( lineage ) ，即 RDD 有充足的信息关于它是如何从其他 RDD 产生而来的 持久化：可以控制存储级别（内存、磁盘等）来进行持久化 操作：丰富的动作 ( Action ) ，如Count、Reduce、Collect和Save 等 RDD基础数据类型 目前有两种类型的基础RDD：并行集合（Parallelized Collections）：接收一个已经存在的Scala集合，然后进行各种并行计算。 Hadoop数据集（Hadoop Datasets）：在一个文件的每条记录上运行函数。只要文件系统是HDFS，或者hadoop支持的任意存储系统即可。这两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。 并行化集合 ​ 并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建的（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。例如，下面的解释器输出，演示了如何从一个数组创建一个并行集合。 例如：val rdd = sc.parallelize(Array(1 to 10)) 根据能启动的executor的数量来进行切分多个slice，每一个slice启动一个Task来进行处理。 val rdd = sc.parallelize(Array(1 to 10), 5) 指定了partition的数量 Hadoop数据集 ​ Spark可以将任何Hadoop所支持的存储资源转化成RDD,如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。 ​ 1. 使用textFile()方法可以将本地文件或HDFS文件转换成RDD ​ 支持整个文件目录读取，文件可以是文本或者压缩文件(如gzip等，自动执行解压缩并加载数据)。如textFile（”file:&#x2F;&#x2F;&#x2F;dfs&#x2F;data”） ​ 支持通配符读取,例如： val rdd1 = sc.textFile(&quot;file:///root/access_log/access_log*.filter&quot;);val rdd2=rdd1.map(_.split(&quot;t&quot;)).filter(_.length==6)rdd2.count()......14/08/20 14:44:48 INFO HadoopRDD: Input split: file:/root/access_log/access_log.20080611.decode.filter:134217728+20705903...... textFile()可选第二个参数slice，默认情况下为每一个block分配一个slice。用户也可以通过slice指定更多的分片，但不能使用少于HDFS block的分片数。 使用wholeTextFiles()读取目录里面的小文件，返回（用户名、内容）对 使用sequenceFileK,V方法可以将SequenceFile转换成RDD。SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。 使用SparkContext.hadoopRDD方法可以将其他任何Hadoop输入类型转化成RDD使用方法。一般来说，HadoopRDD中每一个HDFS block都成为一个RDD分区。 此外，通过Transformation可以将HadoopRDD等转换成FilterRDD(依赖一个父RDD产生）和JoinedRDD（依赖所有父RDD）等。 例子：控制台日志挖掘 假设网站中的一个 WebService 出现错误，我们想要从数以 TB 的 HDFS 日志文件中找到问题的原因，此时我们就可以用 Spark 加载日志文件到一组结点组成集群的 RAM 中，并交互式地进行查询。以下是代码示例： 首先行 1 从 HDFS 文件中创建出一个 RDD ，而行 2 则衍生出一个经过某些条件过滤后的 RDD 。行 3 将这个 RDD errors 缓存到内存中，然而第一个 RDD lines 不会驻留在内存中。这样做很有必要，因为 errors 可能非常小，足以全部装进内存，而原始数据则会非常庞大。经过缓存后，现在就可以反复重用 errors 数据了。我们这里做了两个操作，第一个是统计 errors 中包含 MySQL 字样的总行数，第二个则是取出包含 HDFS 字样的行的第三列时间，并保存成一个集合。 这里要注意的是前面曾经提到过的 Spark 的延迟处理。Spark 调度器会将 filter 和 map 这两个转换保存到管道，然后一起发送给结点去计算。 转换与操作对于RDD可以有两种计算方式：转换（返回值还是一个RDD）与操作（返回值不是一个RDD） 转换(Transformations) (如：map, filter, groupBy, join等)，Transformations操作是Lazy的，也就是说从一个RDD转换生成另一个RDD的操作不是马上执行，Spark在遇到Transformations操作时只会记录需要这样的操作，并不会去执行，需要等到有Actions操作的时候才会真正启动计算过程进行计算。 操作(Actions) (如：count, collect, save等)，Actions操作会返回结果或把RDD数据写到存储系统中。Actions是触发Spark启动计算的动因。 转换 reduce(func) 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行 collect() 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM count() 返回数据集的元素个数 take(n) 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用） first() 返回数据集的第一个元素（类似于take（1） saveAsTextFile(path) 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本 saveAsSequenceFile(path) 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等） foreach(func) 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互 操作 map(func) 返回一个新的分布式数据集，由每个原元素经过func函数转换后组成 filter(func) 返回一个新的数据集，由经过func函数后返回值为true的原元素组成 flatMap(func) 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素） flatMap(func) 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素） sample(withReplacement, frac, seed) 根据给定的随机种子seed，随机抽样出数量为frac的数据 union(otherDataset) 返回一个新的数据集，由原数据集和参数联合而成 groupByKey([numTasks]) 在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task reduceByKey(func, [numTasks]) 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。 join(otherDataset, [numTasks]) 在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集 groupWith(otherDataset, [numTasks]) 在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup cartesian(otherDataset) 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。 flatMap(func) 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素） 依赖类型 在 RDD 中将依赖划分成了两种类型：窄依赖 (Narrow Dependencies) 和宽依赖 (Wide Dependencies) 。 窄依赖是指 父 RDD 的每个分区都只被子 RDD 的一个分区所使用 。 宽依赖就是指父 RDD 的分区被多个子 RDD 的分区所依赖。例如， Map 就是一种窄依赖，而 Join 则会导致宽依赖 ( 除非父 RDD 是 hash-partitioned ，见下图 ) 窄依赖（Narrow Dependencies ） 子RDD 的每个分区依赖于常数个父分区（即与数据规模无关） 输入输出一对一的算子，且结果RDD 的分区结构不变，主要是map 、flatMap 输入输出一对一，但结果RDD 的分区结构发生了变化，如union 、coalesce 从输入中选择部分元素的算子，如filter 、distinct 、subtract 、sample 宽依赖（Wide Dependencies ） 子RDD 的每个分区依赖于所有父RDD 分区 对单个RDD 基于Key 进行重组和reduce，如groupByKey 、reduceByKey ； 对两个RDD 基于Key 进行join 和重组，如join RDD缓存 Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 transformation自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。 Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到： object StorageLevel &#123; val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(false, false, true, false) // Tachyon&#125; 其中，StorageLevel 类的构造器参数如下： class StorageLevel private( private var useDisk_ : Boolean, private var useMemory_ : Boolean, private var useOf Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择： 如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快； 如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问； 尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快； 如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算； 如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法； 在不使用cached RDD的时候，及时使用unpersist方法来释放它。","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://wuzguo.com/blog/categories/bigdata/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://wuzguo.com/blog/tags/Spark/"},{"name":"日志分析","slug":"日志分析","permalink":"http://wuzguo.com/blog/tags/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"name":"RDD","slug":"RDD","permalink":"http://wuzguo.com/blog/tags/RDD/"}],"author":"Zak"},{"title":"Spark学习笔记之基本概念","slug":"bigdata/spark_studynotes_basic_concepts","date":"2017-08-04T12:45:20.000Z","updated":"2022-08-01T06:35:23.871Z","comments":true,"path":"2017/08/04/bigdata/spark_studynotes_basic_concepts.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/04/bigdata/spark_studynotes_basic_concepts.html","excerpt":"","text":"基本概念 Hadoop只提供了Map和Reduce两种操作，Spark提供的数据集操作类型有很多种，大致分为：Transformations和Actions两大类。 Transformations包括Map、Filter、FlatMap、Sample、GroupByKey、ReduceByKey、Union、Join、Cogroup、MapValues、Sort和PartionBy等多种操作类型，同时还提供Count。 Actions包括Collect、Reduce、Lookup和Save等操作。 另外各个处理节点之间的通信模型不再像Hadoop只有Shuffle一种模式，用户可以命名、物化，控制中间结果的存储、分区等。 Spark的适用场景 目前大数据处理场景有以下几个类型： 复杂的批量处理（Batch Data Processing），偏重点在于处理海量数据的能力，至于处理速度可忍受，通常的时间可能是在数十分钟到数小时 基于历史数据的交互式查询（Interactive Query），通常的时间在数十秒到数十分钟之间 基于实时数据流的数据处理（Streaming Data Processing），通常在数百毫秒到数秒之间 目前对以上三种场景需求都有比较成熟的处理框架，第一种情况可以用Hadoop的MapReduce来进行批量海量数据处理，第二种情况可以Impala进行交互式查询，对于第三中情况可以用Storm分布式处理框架处理实时流式数据。以上三者都是比较独立，各自一套维护成本比较高，而Spark的出现能够一站式平台满意以上需求。 通过以上分析，总结Spark场景有以下几个： Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小 由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如web服务的存储或者是增量的web爬虫和索引。就是对于那种增量修改的应用模型不适合 数据量不是特别大，但是要求实时统计分析需求 ​ RDD (Resilient Distributed Dataset，弹性分布式数据集) 的抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的，如果数据集一部分丢失，则可以根据“血统”对它们进行重建，保证了数据的高容错性 Tachyon是一个高容错的分布式文件系统，允许文件以内存的速度在集群框架中进行可靠的共享，就像Spark和 MapReduce那样。通过利用信息继承，内存侵入，Tachyon获得了高性能。 DAG（有向无环图） Spark生态圈以Spark Core为核心，从HDFS、Amazon S3和HBase等持久层读取数据，以MESS、YARN和自身携带的Standalone为资源管理器调度Job完成Spark应用程序的计算。 这些应用程序可以来自于不同的组件，如Spark Shell&#x2F;Spark Submit的批处理、Spark Streaming的实时处理应用、Spark SQL的即席查询、BlinkDB的权衡查询、MLlib&#x2F;MLbase的机器学习、GraphX的图处理和SparkR的数学计算等等。 模型组成 Spark应用程序可分两部分：Driver部分和Executor部分 Driver部分Driver部分主要是对SparkContext进行配置、初始化以及关闭。初始化SparkContext是为了构建Spark应用程序的运行环境，在初始化SparkContext，要先导入一些Spark的类和隐式转换；在Executor部分运行完毕后，需要将SparkContext关闭。 Executor部分Spark应用程序的Executor部分是对数据的处理，数据分三种： 原生数据（包含原生的输入数据和输出数据） 对于输入原生数据，Spark目前提供了两种： ​ Scala集合数据集：如Array(1,2,3,4,5)，Spark使用parallelize方法转换成RDD ​ Hadoop数据集：Spark支持存储在hadoop上的文件和hadoop支持的其他文件系统，如本地文件、HBase、SequenceFile和Hadoop的输入格式。例如Spark使用txtFile方法可以将本地文件或HDFS文件转换成RDD。 对于输出数据，Spark除了支持以上两种数据，还支持scala标量 ​ 生成Scala标量数据，如count（返回RDD中元素的个数）、reduce、fold&#x2F;aggregate；返回几个标量，如take（返回前几个元素）。 ​ 生成Scala集合数据集，如collect（把RDD中的所有元素倒入 Scala集合类型）、lookup（查找对应key的所有值）。 ​ 生成hadoop数据集，如saveAsTextFile、saveAsSequenceFile RDD，RDD提供了四种算子： ​ 输入算子：将原生数据转换成RDD，如parallelize、txtFile等 ​ 转换算子：最主要的算子，是Spark生成DAG图的对象，转换算子并不立即执行，在触发行动算子后再提交给driver处理，生成DAG图 –&gt; Stage –&gt; Task –&gt; Worker执行。 ​ 缓存算子：对于要多次使用的RDD，可以缓冲加快运行速度，对重要数据可以采用多备份缓存。 ​ 行动算子：将运算结果RDD转换成原生数据，如count、reduce、collect、saveAsTextFile等。 共享变量 ​ 在Spark运行时，一个函数传递给RDD内的patition操作时，该函数所用到的变量在每个运算节点上都复制并维护了一份，并且各个节点之间不会相互影响。但是在Spark Application中，可能需要共享一些变量，提供Task或驱动程序使用。Spark提供了两种共享变量： ​ 一、广播变量（Broadcast Variables）：可以缓存到各个节点的共享变量，通常为只读 ​ 广播变量缓存到各个节点的内存中，而不是每个 Task ​ 广播变量被创建后，能在集群中运行的任何函数调用 ​ 广播变量是只读的，不能在被广播后修改 ​ 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本 使用方法： val broadcastVar = sc.broadcast(Array(1, 2, 3)) 二、累计器 ​ 只支持加法操作的变量，可以实现计数器和变量求和。用户可以调用SparkContext.accumulator(v)创建一个初始值为v的累加器，而运行在集群上的Task可以使用“+&#x3D;”操作，但这些任务却不能读取；只有驱动程序才能获取累加器的值。 使用方法： val accum = sc.accumulator(0)sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum + = x)accum.valueval num=sc.parallelize(1 to 100) Spark术语 Spark运行模式 Local 本地模式 常用于本地开发测试，本地还分为local单线程和local-cluster多线程; 运行环境 模式 描述 Standalone 集群模式 典型的Mater&#x2F;slave模式，不过也能看出Master是有单点故障的；Spark支持 ZooKeeper来实现HA On yarn 集群模式 运行在yarn资源管理器框架之上，由yarn负责资源管理，Spark负责任务调度和计算 On mesos 集群模式 运行在mesos资源管理器框架之上，由mesos负责资源管理，Spark负责任务调度和计算 On cloud 集群模式 比如AWS的EC2，使用这个模式能很方便的访问Amazon的S3;Spark支持多种分布式存储系统：HDFS和S3 Spark常用术语 术语 描述 Application Spark的应用程序，包含一个Driver program和若干Executor SparkContext Spark应用程序的入口，负责调度各个运算资源，协调各个Worker Node上的Executor Driver Program 运行Application的main()函数并且创建SparkContext Executor 是为Application运行在Worker node上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上。每个Application都会申请各自的Executor来处理任务 Cluster Manager 在集群上获取资源的外部服务(例如：Standalone、Mesos、Yarn) Worker Node 集群中任何可以运行Application代码的节点，运行一个或多个Executor进程 Task 运行在Executor上的工作单元 Job SparkContext提交的具体Action操作，常和Action对应 Stage 每个Job会被拆分很多组task，每组任务被称为Stage，也称TaskSet RDD 是Resilient distributed datasets的简称，中文为弹性分布式数据集;是Spark最核心的模块和类 DAGScheduler 根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler TaskScheduler 将Taskset提交给Worker node集群运行并返回结果 Transformations 是Spark API的一种类型，Transformation返回值还是一个RDD，所有的Transformation采用的都是懒策略，如果只是将Transformation提交是不会执行计算的 Action 是Spark API的一种类型，Action返回值不是一个RDD，而是一个scala集合；计算只有在Action被提交的时候计算才被触发 Operation 操作，作用于RDD的各种操作分为Transformation和Action","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://wuzguo.com/blog/categories/bigdata/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://wuzguo.com/blog/tags/Spark/"},{"name":"日志分析","slug":"日志分析","permalink":"http://wuzguo.com/blog/tags/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"name":"RDD","slug":"RDD","permalink":"http://wuzguo.com/blog/tags/RDD/"}],"author":"Zak"},{"title":"Elasticsearch集群搭建及管理","slug":"server/elasticsearch_cluster_and_management","date":"2017-08-02T13:13:54.000Z","updated":"2022-08-01T06:35:23.660Z","comments":true,"path":"2017/08/02/server/elasticsearch_cluster_and_management.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/02/server/elasticsearch_cluster_and_management.html","excerpt":"","text":"安装Elasticsearch-head 安装nodejs 安装phantomjs，国内下载地址：https://npm.taobao.org/mirrors/phantomjs 下载elasticsearch-head源码 git clone https://github.com/mobz/elasticsearch-head.gitcd elasticsearch-head/npm install 修改 elasticsearch 配置文件 修改elasticsearch.yml，增加跨域的配置(需要重启es才能生效) http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 修改elasticsearch-head配置 编辑head/Gruntfile.js，修改服务器监听地址，增加hostname属性，将其值设置为 * connect: &#123; hostname: &#x27;*&#x27;, server: &#123; options: &#123; port: 9100, base: &#x27;.&#x27;, keepalive: true &#125; &#125; &#125; 或者 connect: &#123; server: &#123; options: &#123; hostname: &#x27;*&#x27;, port: 9100, base: &#x27;.&#x27;, keepalive: true &#125; &#125;&#125; 编辑head/_site/app.js，修改 head 连接 es 的地址，将 localhost 修改为 es 的 IP 地址： 原配置 this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://localhost:9200&quot;; 将localhost修改为ES的IP地址 this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://YOUR-ES-IP:9200&quot;; 启动elasticsearch-headcd elasticsearch-head/ &amp;&amp; ./node_modules/grunt/bin/grunt server 或者： npm run start 注意： 此时elasticsearch-head为前台启动，如果终端退出，那么elasticsearch-head服务也会随之关闭。 在非elasticsearch-head目录中启动server会失败。因为grunt需要读取目录下的Gruntfile.js。 搭建集群注意事项 拷贝elasticsearch安装包时不要时不要带data目录，否则你永远永远不可能搭建成功。 解决问题的链接 :https://github.com/elastic/elasticsearch/issues/21405 配置文件 配置es的集群名称, es会自动发现在同一网段下的es,如果在同一网段下有多个集群,就可以用这个属性来区分不同的集群 cluster.name: es-hongling-test 节点名称 node.name: hongling-node-0 设置绑定的ip地址还有其它节点和该节点交互的ip地址 network.host: 127.0.0.1 指定http端口,你使用head､kopf等相关插件使用的端口 http.port: 9200 跨域，如果要使用head,那么需要增加新的参数,使head插件可以访问es http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 指定该节点是否有资格被选举成为node node.master: true 指定该节点是否存储索引数据,默认为true node.data: true 设置节点间交互的tcp端口,默认是9300 transport.tcp.port: 9300network.publish_host: 127.0.0.1 这是在因为Centos6不支持SecComp，而ES5.2.0默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动 bootstrap.memory_lock: falsebootstrap.system_call_filter: false 通过配置大多数节点（主节点总数&#x2F; 2 + 1）来防止“分裂大脑”： discovery.zen.minimum_master_nodes: 2 设置集群中master节点的初始列表,可以通过这些节点来自动发现新加入集群的节点 discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1:9300&quot;,&quot;127.0.0.1:9301&quot;,&quot;127.0.0.1:9302&quot;] 集群的管理 格式化返回结果 curl -XGET localhost:9200/twitter?pretty 查看集群state curl -XGET &#x27;http://localhost:9200/_cluster/state 查看集群 health 状态 curl -XGET &#x27;http://localhost:9200/_cluster/health?pretty 查看某个索引的 health 状态 curl -XGET &#x27;http://localhost:9200/_cluster/health/movie 批量加载数据 curl -XPOST &#x27;localhost:9200/bank/account/_bulk?pretty&#x27; --data-binary @accounts.json 查看_type的映射 curl -XGET &#x27;localhost:9200/shakespeare/_mapping/act&#x27;?pretty 插件安装 analysis-ik download or compile download pre-build package from here: https://github.com/medcl/elasticsearch-analysis-ik/releases or compiled from the source: checkout ik version respective to your elasticsearch version git checkout tags/&#123;version&#125;mvn package copy and unzip target&#x2F;releases&#x2F;elasticsearch-analysis-ik-{version}.zip to your-es-root&#x2F;plugins&#x2F;ik restart elasticsearch Java Client 连接ES集群 // on startupSettings settings = Settings.builder() .put(&quot;cluster.name&quot;, &quot;es-hongling-test&quot;).build();TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;127.0.0.1&quot;), 9300)) .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;127.0.0.1&quot;), 9301)) .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;127.0.0.1&quot;), 9302));List&lt;DiscoveryNode&gt; nodeList = client.connectedNodes();for (DiscoveryNode node : nodeList) &#123; System.out.println(&quot; node: &quot; + node.getName());&#125; 注意：使用的IP地址和端口号对应配置文件中的 network.host 和 transport.tcp.port， 否则将保错： Exception in thread &quot;main&quot; NoNodeAvailableException[None of the configured nodes are available: [&#123;#transport#-1&#125;&#123;9ibol1jmRlCVKFW8PjeJbQ&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9200&#125;]]at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:348)at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:246)...... 常见错误及解决方案 聚合查询时报错 Exception in thread &quot;main&quot; Failed to execute phase [dfs], all shards failed; shardFailures &#123;[IHX-j5mQRLeH6DK6pATl-A][logstash-logs][0]: RemoteTransportException[[hongling-node-2][127.0.0.1:9302][indices:data/read/search[phase/dfs]]]; nested: IllegalArgumentException[Fielddata is disabled on text fields by default. Set fielddata=true on [extension] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.]; &#125;&#123;[IHX-j5mQRLeH6DK6pATl-A][logstash-logs][1]: RemoteTransportException[[hongling-node-2][127.0.0.1:9302][indices:data/read/search[phase/dfs]]]; nested: IllegalArgumentException[Fielddata is disabled on text fields by default. Set fielddata=true on [extension] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.]; &#125;&#123;[iHjuxsoZQOiQwSB2Y-rYkw][logstash-logs][2]: RemoteTransportException[[hongling-node-0][127.0.0.1:9300][indices:data/read/search[phase/dfs]]]; nested: IllegalArgumentException[Fielddata is disabled on text fields by default. Set fielddata=true on [extension] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.]; &#125; at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:272) at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:130) at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:241) at org.elasticsearch.action.search.InitialSearchPhase.onShardFailure(InitialSearchPhase.java:87) at org.elasticsearch.action.search.InitialSearchPhase.access$100(InitialSearchPhase.java:47) at org.elasticsearch.action.search.InitialSearchPhase$1.onFailure(InitialSearchPhase.java:155) at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:51) at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1050) at org.elasticsearch.transport.TcpTransport.lambda$handleException$17(TcpTransport.java:1451) at org.elasticsearch.common.util.concurrent.EsExecutors$1.execute(EsExecutors.java:110) 解决方案： 原因： Fielddata is disabled on text fields by default. Set fielddata=true on [your_field_name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. curl -XPUT http://localhost:9202/logstash-logs/_mapping/log -d &#x27;&#123; &quot;log&quot;: &#123; &quot;properties&quot;: &#123; &quot;extension&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125; &#125; &#125; 向已存在的索引中添加自定义filter&#x2F;analyzer，报错 &#123; &quot;error&quot;: &quot;IndexAlreadyExistsException[[symbol] already exists]&quot;, &quot;status&quot;: 400&#125; 解决方案： POST /symbol/_closePUT /symbol/_settings&#123; &quot;settings&quot;: &#123; .... &#125;&#125;POST /symbol/_open 这样就避免了需要重建索引的麻烦。有了新添加的filter和analyzer，就可以根据需要再对types中的mappings进行更新了。","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://wuzguo.com/blog/tags/Elasticsearch/"},{"name":"Elasticsearch-head","slug":"Elasticsearch-head","permalink":"http://wuzguo.com/blog/tags/Elasticsearch-head/"}],"author":"Zak"},{"title":"Elasticsearch搜索引擎的搜索相关知识点","slug":"server/elasticsearch_basic_2","date":"2017-08-01T02:34:13.000Z","updated":"2022-08-01T06:33:29.901Z","comments":true,"path":"2017/08/01/server/elasticsearch_basic_2.html","link":"","permalink":"http://wuzguo.com/blog/2017/08/01/server/elasticsearch_basic_2.html","excerpt":"","text":"搜索相关知识点 Elasticsearch 不只会存储（stores） 文档，为了能被搜索到也会为文档添加索引（indexes） ，这也是为什么我们使用结构化的 JSON 文档，而不是无结构的二进制数据。 搜索（search） 可以做到 在类似于 gender 或者 age 这样的字段 上使用结构化查询，join_date 这样的字段上使用排序，就像SQL的结构化查询一样。 全文检索，找出所有匹配关键字的文档并按照相关性（relevance） 排序后返回结果。 以上二者兼而有之。 很多搜索都是开箱即用的，为了充分挖掘 Elasticsearch 的潜力，你需要理解以下三个概念： 映射（Mapping） 描述数据在每个字段内如何存储 分析（Analysis） 全文是如何处理使之可以被搜索的 领域特定查询语言（Query DSL） Elasticsearch 中强大灵活的查询语言 分析与分线器 分析 包含下面的过程： 首先，将一块文本分成适合于倒排索引的独立的 词条，之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall分析器执行上面的工作。 分析器 实际上是将三个功能封装到了一个包里： 字符过滤器 首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。 分词器 其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。 Token 过滤器 最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。 Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。我们会在 自定义分析器 章节详细讨论。 内置分析器 Elasticsearch还附带了可以直接使用的预包装的分析器。 接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条： &quot;Set the shape to semi-transparent by calling set_trans(5)&quot; 标准分析器 标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set_trans, 5 简单分析器 简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set, trans 空格分析器 空格分析器在空格的地方划分文本。它会产生Set, the, shape, to, semi-transparent, by, calling, set_trans(5) 语言分析器 特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。英语 分词器会产生下面的词条：set, shape, semi, transpar, call, set_tran, 5注意看 transparent、 calling 和 set_trans 已经变为词根格式。 测试分析器 有些时候很难理解分词的过程和实际被存储到索引中的词条，特别是你刚接触Elasticsearch。为了理解发生了什么，你可以使用 analyze API 来看文本是如何被分析的。在消息体里，指定分析器和要分析的文本： GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Text to analyze&quot;&#125; 结果中每个元素代表一个单独的词条： &#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;text&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; token是实际存储到索引中的词条。position指明词条在原始文本中出现的位置。start_offset和end_offset&#96; 指明字符在原始字符串中的位置。 核心简单域类型Elasticsearch 支持 如下简单域类型 字符串: string 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date 当你索引一个包含新域的文档–之前未曾出现– Elasticsearch 会使用 动态映射 ，通过JSON中基本数据类型，尝试猜测域类型，使用如下规则： JSON type 域 type 布尔型: true 或者 false boolean 整数: 123 long 浮点数: 123.45 double 字符串，有效日期: 2014-09-15 date 字符串: foo bar string 自定义域映射 尽管在很多情况下基本域数据类型 已经够用，但你经常需要为单独域自定义映射 ，特别是字符串域。自定义映射允许你执行下面的操作 全文字符串域和精确值字符串域的区别 使用特定语言分析器 优化域以适应部分匹配 指定自定义数据格式 还有更多 域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ： &#123; &quot;number_of_clicks&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;&#125; 默认， string 类型域会被认为包含全文。就是说，它们的值在索引前，会通过 一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。 string 域映射的两个最重要 属性是 index 和 analyzer 。 index index 属性控制怎样索引字符串。它可以是下面三个值： analyzed 首先分析字符串，然后索引它。换句话说，以全文索引这个域。 not_analyzed 索引这个域，所以可以搜索到它，但索引指定的精确值。不对它进行分析。 no Don’t index this field at all 不索引这个域。这个域不会被搜索到。 string 域 index 属性默认是 analyzed 。如果我们想映射这个字段为一个精确值，我们需要设置它为 not_analyzed ： &#123; &quot;tag&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125;&#125; 其他简单类型（例如 long ， double ， date 等）也接受 index 参数，但有意义的值只有 no 和 not_analyzed ， 因为它们永远不会被分析。 analyzer 对于 analyzed 字符串域，用 analyzer 属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用 standard 分析器， 但你可以指定一个内置的分析器替代它，例如 whitespace 、 simple 和 english： &#123; &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;&#125; 在 自定义分析器 ，我们会展示怎样定义和使用自定义分析器。 查询表达式 查询表达式(Query DSL)是一种非常灵活又富有表现力的 查询语言。 Elasticsearch 使用它可以以简单的 JSON 接口来展现 Lucene 功能的绝大部分。 一个查询语句 的典型结构： &#123; QUERY_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125;&#125; 如果是针对某个字段，那么它的结构如下： &#123; QUERY_NAME: &#123; FIELD_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125; &#125;&#125; 举个例子，你可以使用 match 查询语句 来查询 tweet 字段中包含 elasticsearch 的 tweet： &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;elasticsearch&quot; &#125; &#125; 完整的查询请求如下： GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;elasticsearch&quot; &#125; &#125;&#125; 组合多查询 为了构建类似的高级查询，你需要一种能够将多查询组合成单一查询的查询方法。你可以用 bool 查询来实现你的需求。这种查询将多查询组合在一起，成为用户自己想要的布尔查询。它接收以下参数： must 文档 必须 匹配这些条件才能被包含进来。 must_not 文档 必须不 匹配这些条件才能被包含进来。 should 如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分。 filter 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。 由于这是我们看到的第一个包含多个查询的查询，所以有必要讨论一下相关性得分是如何组合的。每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 下面的查询用于查找 title 字段匹配 how to make millions 并且不被标识为 spam 的文档。那些被标识为 starred 或在2014之后的文档，将比另外那些文档拥有更高的排名。如果 两者 都满足，那么它排名将更高： &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125;&#125; ] &#125;&#125; 相关性 每个文档都有相关性评分，用一个正浮点数字段 _score 来表示。_score 的评分越高，相关性越高。 查询语句会为每个文档生成一个 _score 字段。评分的计算方式取决于查询类型 不同的查询语句用于不同的目的： fuzzy 查询会计算与关键词的拼写相似程度，terms 查询会计算 找到的内容与关键词组成部分匹配的百分比，但是通常我们说的 relevance 是我们用来计算全文本字段的值相对于全文本检索词相似程度的算法。 Elasticsearch 的相似度算法 被定义为检索词频率&#x2F;反向文档频率， TF&#x2F;IDF ，包括以下内容： 检索词频率 检索词在该字段出现的频率？出现频率越高，相关性也越高。 字段中出现过 5 次要比只出现过 1 次的相关性高。 反向文档频率 每个检索词在索引中出现的频率？频率越高，相关性越低。检索词出现在多数文档中会比出现在少数文档中的权重更低。 字段长度准则 字段的长度是多少？长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段权重更大。 单个查询可以联合使用 TF&#x2F;IDF 和其他方式，比如短语查询中检索词的距离或模糊查询里的检索词相似度。相关性并不只是全文本检索的专利。也适用于 yes|no 的子句，匹配的子句越多，相关性评分越高。如果多条查询子句被合并为一条复合查询语句 ，比如 bool 查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。 游标查询（可以替代深分页） scroll 查询 可以用来对 Elasticsearch 有效地执行大批量的文档查询，而又不用付出深度分页那种代价。游标查询允许我们 先做查询初始化，然后再批量地拉取结果。 这有点儿像传统数据库中的 cursor 。 索引设置 Elasticsearch 提供了优化好的默认配置。 除非你理解这些配置的作用并且知道为什么要去修改，否则不要随意修改。 下面是两个 最重要的设置： number_of_shards 每个索引的主分片数，默认值是 5 。这个配置在索引创建后不能修改。 number_of_replicas 每个主分片的副本数，默认值是 1 。对于活动的索引库，这个配置可以随时修改。 例如，我们可以创建只有 一个主分片，没有副本的小索引： PUT /my_temp_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot; : 1, &quot;number_of_replicas&quot; : 0 &#125;&#125; 自定义分析器 虽然Elasticsearch带有一些现成的分析器，然而在分析器上Elasticsearch真正的强大之处在于，你可以通过在一个适合你的特定数据的设置之中组合字符过滤器、分词器、词汇单元过滤器来创建自定义的分析器。 在 分析与分析器 我们说过，一个 分析器 就是在一个包里面组合了三种函数的一个包装器， 三种函数按照顺序被执行: 字符过滤器 字符过滤器 用来 整理 一个尚未被分词的字符串。例如，如果我们的文本是HTML格式的，它会包含像 &lt;p&gt; 或者 &lt;div&gt; 这样的HTML标签，这些标签是我们不想索引的。我们可以使用 html清除 字符过滤器 来移除掉所有的HTML标签，并且像把 &amp;Aacute; 转换为相对应的Unicode字符 Á 这样，转换HTML实体。一个分析器可能有0个或者多个字符过滤器。 分词器 一个分析器 必须 有一个唯一的分词器。 分词器把字符串分解成单个词条或者词汇单元。 标准 分析器里使用的 标准 分词器 把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号，然而还有其他不同行为的分词器存在。例如， 关键词 分词器 完整地输出 接收到的同样的字符串，并不做任何分词。 空格分词器 只根据空格分割文本 。 正则 分词器 根据匹配正则表达式来分割文本 。 词单元过滤器 经过分词，作为结果的 词单元流 会按照指定的顺序通过指定的词单元过滤器 。词单元过滤器可以修改、添加或者移除词单元。我们已经提到过 lowercase 和stop 词过滤器 ，但是在 Elasticsearch 里面还有很多可供选择的词单元过滤器。 词干过滤器 把单词 遏制 为 词干。 ascii_folding 过滤器移除变音符，把一个像 &quot;très&quot; 这样的词转换为 &quot;tres&quot; 。 ngram 和 edge_ngram 词单元过滤器 可以产生 适合用于部分匹配或者自动补全的词单元。 在 深入搜索，我们讨论了在哪里使用，以及怎样使用分词器和过滤器。但是首先，我们需要解释一下怎样创建自定义的分析器。 创建一个自定义分析器 和我们之前配置 es_std 分析器一样，我们可以在 analysis 下的相应位置设置字符过滤器、分词器和词单元过滤器: PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; ... custom character filters ... &#125;, &quot;tokenizer&quot;: &#123; ... custom tokenizers ... &#125;, &quot;filter&quot;: &#123; ... custom token filters ... &#125;, &quot;analyzer&quot;: &#123; ... custom analyzers ... &#125; &#125; &#125;&#125; 作为示范，让我们一起来创建一个自定义分析器吧，这个分析器可以做到下面的这些事: 使用 html清除 字符过滤器移除HTML部分。 使用一个自定义的 映射 字符过滤器把 &amp; 替换为 &quot; 和 &quot; ： &quot;char_filter&quot;: &#123; &quot;&amp;_to_and&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;] &#125;&#125; 使用 标准 分词器分词。 小写词条，使用 小写 词过滤器处理。 使用自定义 停止 词过滤器移除自定义的停止词列表中包含的词： &quot;filter&quot;: &#123; &quot;my_stopwords&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] &#125;&#125; 我们的分析器定义用我们之前已经设置好的自定义过滤器组合了已经定义好的分词器和过滤器： &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] &#125;&#125; 汇总起来，完整的 创建索引 请求 看起来应该像这样： PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; &quot;&amp;_to_and&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;] &#125;&#125;, &quot;filter&quot;: &#123; &quot;my_stopwords&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] &#125;&#125;, &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] &#125;&#125; &#125;&#125;&#125; 根对象 映射的最高一层被称为 根对象 ，它可能包含下面几项： 一个 properties 节点，列出了文档中可能包含的每个字段的映射 各种元数据字段，它们都以一个下划线开头，例如 _type 、 _id 和 _source 设置项，控制如何动态处理新的字段，例如 analyzer 、 dynamic_date_formats和 dynamic_templates 其他设置，可以同时应用在根对象和其他 object 类型的字段上，例如 enabled、 dynamic 和 include_in_all 动态映射 当 Elasticsearch 遇到文档中以前 未遇到的字段，它用 dynamic mapping 来确定字段的数据类型并自动把新的字段添加到类型映射。 如果Elasticsearch是作为重要的数据存储，可能就会期望遇到新字段就会抛出异常，这样能及时发现问题。 幸运的是可以用 dynamic 配置来控制这种行为 ，可接受的选项如下： true 动态添加新的字段–缺省 false 忽略新的字段 strict 如果遇到新字段抛出异常 配置参数 dynamic 可以用在根 object 或任何 object 类型的字段上。你可以将 dynamic 的默认值设置为 strict , 而只在指定的内部对象中开启它, 例如： PUT /my_index &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic&quot;: &quot;strict&quot;, &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;string&quot;&#125;, &quot;stash&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;dynamic&quot;: true &#125; &#125; &#125; &#125; &#125; 把 dynamic 设置为 false 一点儿也不会改变 _source 的字段内容。 _source 仍然包含被索引的整个JSON文档。只是新的字段不会被加到映射中也不可搜索。 动态模板 使用 dynamic_templates ，你可以完全控制 新检测生成字段的映射。你甚至可以通过字段名称或数据类型来应用不同的映射。 每个模板都有一个名称， 你可以用来描述这个模板的用途， 一个 mapping 来指定映射应该怎样使用，以及至少一个参数 (如 match) 来定义这个模板适用于哪个字段。 模板按照顺序来检测；第一个匹配的模板会被启用。例如，我们给 string 类型字段定义两个模板： es ：以 _es 结尾的字段名需要使用 spanish 分词器。 en ：所有其他字段使用 english 分词器。 我们将 es 模板放在第一位，因为它比匹配所有字符串字段的 en 模板更特殊： PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;es&quot;: &#123; &quot;match&quot;: &quot;*_es&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;spanish&quot; &#125; &#125;&#125;, &#123; &quot;en&quot;: &#123; &quot;match&quot;: &quot;*&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125; &#125;&#125; ] &#125;&#125;&#125; match_mapping_type 允许你应用模板到特定类型的字段上，就像有标准动态映射规则检测的一样， (例如 string 或 long)。 match 参数只匹配字段名称， path_match 参数匹配字段在对象上的完整路径，所以 address.*.name 将匹配这样的字段： &#123; &quot;address&quot;: &#123; &quot;city&quot;: &#123; &quot;name&quot;: &quot;New York&quot; &#125; &#125;&#125; unmatch 和 path_unmatch将被用于未被匹配的字段。 索引别名和零停机 在前面提到的，重建索引的问题是必须更新应用中的索引名称。 索引别名就是用来解决这个问题的！ 索引 别名 就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何一个需要索引名的API来使用。别名 带给我们极大的灵活性，允许我们做下面这些： 在运行的集群中可以无缝的从一个索引切换到另一个索引 给多个索引分组 (例如， last_three_months) 给索引的一个子集创建 视图 在后面我们会讨论更多关于别名的使用。现在，我们将解释怎样使用别名在零停机下从旧索引切换到新索引。 有两种方式管理别名： _alias 用于单个操作， _aliases 用于执行多个原子级操作。 查询语句提升权重 当然 bool 查询不仅限于组合简单的单个词 match 查询， 它可以组合任意其他的查询，以及其他 bool 查询。 普遍的用法是通过汇总多个独立查询的分数，从而达到为每个文档微调其相关度评分 _score 的目的。 假设想要查询关于 “full-text search（全文搜索）” 的文档， 但我们希望为提及 “Elasticsearch” 或 “Lucene” 的文档给予更高的 权重 ，这里 更高权重 是指如果文档中出现 “Elasticsearch” 或 “Lucene” ，它们会比没有的出现这些词的文档获得更高的相关度评分 _score ，也就是说，它们会出现在结果集的更上面。 一个简单的 bool 查询 允许我们写出如下这种非常复杂的逻辑： GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;full text search&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;Elasticsearch&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;Lucene&quot; &#125;&#125; ] &#125; &#125;&#125; content 字段必须包含 full 、 text 和 search 所有三个词。如果 content 字段也包含 Elasticsearch 或 Lucene ，文档会获得更高的评分 _score 。 should 语句匹配得越多表示文档的相关度越高。目前为止还挺好。 但是如果我们想让包含 Lucene 的有更高的权重，并且包含 Elasticsearch 的语句比 Lucene 的权重更高，该如何处理? 我们可以通过指定 boost 来控制任何查询语句的相对的权重， boost 的默认值为 1 ，大于 1 会提升一个语句的相对权重。所以下面重写之前的查询： GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;full text search&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;Elasticsearch&quot;, &quot;boost&quot;: 3 &#125; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;Lucene&quot;, &quot;boost&quot;: 2 &#125; &#125;&#125; ] &#125; &#125;&#125; 这些语句使用默认的 boost 值 1 。 这条语句更为重要，因为它有最高的 boost 值。 这条语句比使用默认值的更重要，但它的重要性不及 Elasticsearch 语句。 boost 参数被用来提升一个语句的相对权重（ boost 值大于 1 ）或降低相对权重（ boost 值处于 0 到 1 之间），但是这种提升或降低并不是线性的，换句话说，如果一个 boost 值为 2 ，并不能获得两倍的评分 _score 。 相反，新的评分 _score 会在应用权重提升之后被 归一化 ，每种类型的查询都有自己的归一算法，细节超出了本书的范围，所以不作介绍。简单的说，更高的 boost 值为我们带来更高的评分 _score 。 如果不基于 TF&#x2F;IDF 要实现自己的评分模型，我们就需要对权重提升的过程能有更多控制，可以使用 function_score 查询操纵一个文档的权重提升方式而跳过归一化这一步骤。 注意事项深分页 先查后取的过程支持用 from 和 size 参数分页，但是这是 有限制的 。 要记住需要传递信息给协调节点的每个分片必须先创建一个 from + size 长度的队列，协调节点需要根据 number_of_shards * (from + size) 排序文档，来找到被包含在 size 里的文档。 取决于你的文档的大小，分片的数量和你使用的硬件，给 10,000 到 50,000 的结果文档深分页（ 1,000 到 5,000 页）是完全可行的。但是使用足够大的 from 值，排序过程可能会变得非常沉重，使用大量的CPU、内存和带宽。因为这个原因，我们强烈建议你不要使用深分页。 Bouncing Results 想象一下有两个文档有同样值的时间戳字段，搜索结果用 timestamp 字段来排序。 由于搜索请求是在所有有效的分片副本间轮询的，那就有可能发生主分片处理请求时，这两个文档是一种顺序， 而副本分片处理请求时又是另一种顺序。 这就是所谓的 bouncing results 问题: 每次用户刷新页面，搜索结果表现是不同的顺序。 让同一个用户始终使用同一个分片，这样可以避免这种问题， 可以设置 preference 参数为一个特定的任意值比如用户会话ID来解决。 超时问题 通常分片处理完它所有的数据后再把结果返回给协同节点，协同节点把收到的所有结果合并为最终结果。这意味着花费的时间是最慢分片的处理时间加结果合并的时间。如果有一个节点有问题，就会导致所有的响应缓慢。 参数 timeout 告诉 分片允许处理数据的最大时间。如果没有足够的时间处理所有数据，这个分片的结果可以是部分的，甚至是空数据。 搜索的返回结果会用属性 timed_out 标明分片是否返回的是部分结果： ...&quot;timed_out&quot;: true, ... 路由 在 路由一个文档到一个分片中 中, 我们解释过如何定制参数 routing ，它能够在索引时提供来确保相关的文档，比如属于某个用户的文档被存储在某个分片上。 在搜索的时候，不用搜索索引的所有分片，而是通过指定几个 routing 值来限定只搜索几个相关的分片： GET /_search?routing=user_1,user2 这个技术在设计大规模搜索系统时就会派上用场，我们在 扩容设计 中详细讨论它。 搜索类型 缺省的搜索类型是 query_then_fetch 。 在某些情况下，你可能想明确设置 search_type 为 dfs_query_then_fetch 来改善相关性精确度： GET /_search?search_type=dfs_query_then_fetch 搜索类型 dfs_query_then_fetch 有预查询阶段，这个阶段可以从所有相关分片获取词频来计算全局词频。 我们在 被破坏的相关度！ 会再讨论它。 重新索引你的数据 尽管可以增加新的类型到索引中，或者增加新的字段到类型中，但是不能添加新的分析器或者对现有的字段做改动。 如果你那么做的话，结果就是那些已经被索引的数据就不正确， 搜索也不能正常工作。 对现有数据的这类改变最简单的办法就是重新索引：用新的设置创建新的索引并把文档从旧的索引复制到新的索引。 字段 _source 的一个优点是在Elasticsearch中已经有整个文档。你不必从源数据中重建索引，而且那样通常比较慢。 为了有效的重新索引所有在旧的索引中的文档，用 scroll 从旧的索引检索批量文档 ， 然后用 bulk API 把文档推送到新的索引中。 从Elasticsearch v2.3.0开始， Reindex API 被引入。它能够对文档重建索引而不需要任何插件或外部工具。 批量重新索引 同时并行运行多个重建索引任务，但是你显然不希望结果有重叠。正确的做法是按日期或者时间 这样的字段作为过滤条件把大的重建索引分成小的任务： GET /old_index/_search?scroll=1m &#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot;, &quot;lt&quot;: &quot;2014-02-01&quot; &#125; &#125; &#125;, &quot;sort&quot;: [&quot;_doc&quot;], &quot;size&quot;: 1000 &#125; 配置参数禁止自动创建索引 config/elasticsearch.yml 的每个节点下添加下面的配置： action.auto_create_index: false 我们会在之后讨论你怎么用 索引模板 来预配置开启自动创建索引。这在索引日志数据的时候尤其有用：你将日志数据索引在一个以日期结尾命名的索引上，子夜时分，一个预配置的新索引将会自动进行创建。 Groovy 脚本编程 您可以通过设置集群中的所有节点的 config/elasticsearch.yml文件来禁用动态 Groovy 脚本： script.groovy.sandbox.enabled: false 这将关闭 Groovy 沙盒，从而防止动态 Groovy 脚本作为请求的一部分被接受， 或者从特殊的 .scripts 索引中被检索。当然，你仍然可以使用存储在每个节点的 config/scripts/ 目录下的 Groovy 脚本。 避免意外的大量删除数据action.destructive_requires_name: true 这个设置使删除只限于特定名称指向的数据, 而不允许通过指定 _all 或通配符来删除指定索引库。你同样可以通过 Cluster State API 动态的更新这个设置。","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://wuzguo.com/blog/tags/Elasticsearch/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://wuzguo.com/blog/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"},{"name":"ELK","slug":"ELK","permalink":"http://wuzguo.com/blog/tags/ELK/"}],"author":"Zak"},{"title":"Elasticsearch基础知识","slug":"server/elasticsearch_basic_1","date":"2017-07-28T11:20:46.000Z","updated":"2022-08-01T06:33:30.078Z","comments":true,"path":"2017/07/28/server/elasticsearch_basic_1.html","link":"","permalink":"http://wuzguo.com/blog/2017/07/28/server/elasticsearch_basic_1.html","excerpt":"","text":"1. 基本概念索引（名词） 一个 索引 类似于传统关系数据库中的一个 数据库 ，是一个存储关系型文档的地方。 索引 (index) 的复数词为 indices 或 indexes. 索引（动词） 索引一个文档* 就是存储一个文档到一个 索引 （名词）中以便它可以被检索和查询到。这非常类似于 SQL 语句中的 INSERT 关键词，除了文档已存在时新文档会替换旧文档情况之外。 倒排索引 关系型数据库通过增加一个 索引 比如一个 B树（B-tree）索引 到指定的列上，以便提升数据检索速度。Elasticsearch 和 Lucene 使用了一个叫做 倒排索引 的结构来达到相同的目的。 集群健康 Elasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 集群健康 ， 它在 status 字段中展示为 green 、 yellow 或者 red 。status 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下： green 所有的主分片和副本分片都正常运行。 yellow 所有的主分片都正常运行，但不是所有的副本分片都正常运行。（当只有一个结点的时候，状态值为 yellow） red 有主分片没能正常运行。 ###2. 知识点 默认值 一个搜索默认返回十条结果。格式： GET &#x2F;索引库&#x2F;索引类型&#x2F;_search ，例如： GET &#x2F;megacorp&#x2F;employee&#x2F;_search. Elasticsearch 默认按照相关性得分排序，即每个文档跟查询的匹配程度。 ElasticSearch 的主旨是随时可用和按需扩容。 而扩容可以通过购买性能更强大（ 垂直扩容 ，或 纵向扩容 ） 或者数量更多的服务器（ 水平扩容 ，或 横向扩容 ）来实现。 一个运行中的 Elasticsearch 实例称为一个 节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 当一个节点被选举成为 主 节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。 当你在同一台机器上启动了第二个节点时，只要它和第一个节点有同样的 cluster.name 配置，它就会自动发现集群并加入到其中。 但是在不同机器上启动节点的时候，为了加入到同一集群，你需要配置一个可连接到的单播主机列表。 文档元数据 一个文档不仅仅包含它的数据 ，也包含 元数据 —— 有关 文档的信息。 三个必须的元数据元素如下： _index 文档在哪存放 （一个 索引 应该是因共同的特性被分组到一起的文档集合。 例如，你可能存储所有的产品在索引 products 中，而存储所有销售的交易到索引 sales 中。 虽然也允许存储不相关的数据到一个索引中，但这通常看作是一个反模式的做法。） _type 文档表示的对象类别（数据可能在索引中只是松散的组合在一起，但是通常明确定义一些数据中的子分区是很有用的。 例如，所有的产品都放在一个索引中，但是你有许多不同的产品类别，比如 “electronics” 、 “kitchen” 和 “lawn-care”。） _id 文档唯一标识（ID 是一个字符串， 当它和 _index 以及 _type 组合就可以唯一确定 Elasticsearch 中的一个文档。 当你创建一个新的文档，要么提供自己的 _id ，要么让 Elasticsearch 帮你生成。） 取回文档 为了从 Elasticsearch 中检索出文档 ，我们仍然使用相同的 _index , _type , 和 _id ，但是 HTTP 谓词 更改为 GET : GET /website/blog/123?pretty 响应体包括目前已经熟悉了的元数据元素，再加上 _source 字段，这个字段包含我们索引数据时发送给 Elasticsearch 的原始 JSON 文档： &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot; &#125; &#125; GET 请求的响应体包括 &#123;&quot;found&quot;: true&#125; ，这证实了文档已经被找到。 如果我们请求一个不存在的文档，我们仍旧会得到一个 JSON 响应体，但是 found 将会是 false 。 此外， HTTP 响应码将会是 404 Not Found ，而不是 200 OK 。 我们可以通过传递 -i 参数给 curl 命令，该参数 能够显示响应的头部： curl -i -XGET http://localhost:9200/website/blog/124?pretty 显示响应头部的响应体现在类似这样： HTTP/1.1 404 Not FoundContent-Type: application/json; charset=UTF-8Content-Length: 83 返回文档的一部分： &#123; &quot;_index&quot; : &quot;website&quot;, &quot;_type&quot; : &quot;blog&quot;, &quot;_id&quot; : &quot;124&quot;, &quot;found&quot; : false&#125; 默认情况下， GET 请求 会返回整个文档，这个文档正如存储在 _source 字段中的一样。但是也许你只对其中的 title 字段感兴趣。单个字段能用 _source 参数请求得到，多个字段也能使用逗号分隔的列表来指定。 GET /website/blog/123?_source=title,text 该 _source 字段现在包含的只是我们请求的那些字段，并且已经将 date 字段过滤掉了。 &#123; &quot;_index&quot; : &quot;website&quot;, &quot;_type&quot; : &quot;blog&quot;, &quot;_id&quot; : &quot;123&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;title&quot;: &quot;My first blog entry&quot; , &quot;text&quot;: &quot;Just trying this out...&quot; &#125;&#125; 或者，如果你只想得到 _source 字段，不需要任何元数据，你能使用 _source 端点： GET /website/blog/123/_source 那么返回的的内容如下所示： &#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;&#125; 关于冲突 当我们使用 index API 更新文档 ，可以一次性读取原始文档，做我们的修改，然后重新索引 整个文档 。 最近的索引请求将获胜：无论最后哪一个文档被索引，都将被唯一存储在 Elasticsearch 中。如果其他人同时更改这个文档，他们的更改将丢失。 解决冲突（乐观锁） 当我们之前讨论 index ， GET 和 delete 请求时，我们指出每个文档都有一个 _version （版本）号，当文档被修改时版本号递增。 Elasticsearch 使用这个 _version 号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。 我们可以利用 _version 号来确保 应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的 version 号来达到这个目的。 如果该版本不是当前版本号，我们的请求将会失败。 通过外部系统使用版本控制 一个常见的设置是使用其它数据库作为主要的数据存储，使用 Elasticsearch 做数据检索， 这意味着主数据库的所有更改发生时都需要被复制到 Elasticsearch ，如果多个进程负责这一数据同步，你可能遇到类似于之前描述的并发问题。 如果你的主数据库已经有了版本号 — 或一个能作为版本号的字段值比如 timestamp — 那么你就可以在 Elasticsearch 中通过增加 version_type=external 到查询字符串的方式重用这些相同的版本号， 版本号必须是大于零的整数， 且小于 9.2E+18 — 一个 Java 中 long 类型的正值。 外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同， Elasticsearch 不是检查当前 _version 和请求中指定的版本号是否相同， 而是检查当前 _version 是否 小于 指定的版本号。 如果请求成功，外部的版本号作为文档的新 _version 进行存储。 外部版本号不仅在索引和删除请求是可以指定，而且在 创建 新文档时也可以指定。例如，要创建一个新的具有外部版本号 5 的博客文章，我们可以按以下方法进行： PUT /website/blog/2?version=5&amp;version_type=external&#123; &quot;title&quot;: &quot;My first external blog entry&quot;, &quot;text&quot;: &quot;Starting to get the hang of this...&quot;&#125; Groovy 脚本编程 您可以通过设置集群中的所有节点的 config/elasticsearch.yml文件来禁用动态 Groovy 脚本： script.groovy.sandbox.enabled: false 这将关闭 Groovy 沙盒，从而防止动态 Groovy 脚本作为请求的一部分被接受， 或者从特殊的 .scripts 索引中被检索。当然，你仍然可以使用存储在每个节点的 config/scripts/ 目录下的 Groovy 脚本。 取回多个文档 Elasticsearch 的速度已经很快了，但甚至能更快。 将多个请求合并成一个，避免单独处理每个请求花费的网络时延和开销。 如果你需要从 Elasticsearch 检索很多文档，那么使用 multi-get 或者 mget API 来将这些检索请求放在一个请求中，将比逐个文档请求更快地检索到全部文档。 mget API 要求有一个 docs 数组作为参数，每个 元素包含需要检索文档的元数据， 包括 _index 、 _type 和 _id 。如果你想检索一个或者多个特定的字段，那么你可以通过 _source 参数来指定这些字段的名字： GET /_mget&#123; &quot;docs&quot; : [ &#123; &quot;_index&quot; : &quot;website&quot;, &quot;_type&quot; : &quot;blog&quot;, &quot;_id&quot; : 2 &#125;, &#123; &quot;_index&quot; : &quot;website&quot;, &quot;_type&quot; : &quot;pageviews&quot;, &quot;_id&quot; : 1, &quot;_source&quot;: &quot;views&quot; &#125; ]&#125; 该响应体也包含一个 docs 数组 ， 对于每一个在请求中指定的文档，这个数组中都包含有一个对应的响应，且顺序与请求中的顺序相同。 其中的每一个响应都和使用单个 getrequest 请求所得到的响应体相同： &#123; &quot;docs&quot; : [ &#123; &quot;_index&quot; : &quot;website&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_type&quot; : &quot;blog&quot;, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;This is a piece of cake...&quot;, &quot;title&quot; : &quot;My first external blog entry&quot; &#125;, &quot;_version&quot; : 10 &#125;, &#123; &quot;_index&quot; : &quot;website&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_type&quot; : &quot;pageviews&quot;, &quot;found&quot; : true, &quot;_version&quot; : 2, &quot;_source&quot; : &#123; &quot;views&quot; : 2 &#125; &#125; ]&#125; 批量操作： 通过批量索引典型文档，并不断增加批量大小进行尝试。 当性能开始下降，那么你的批量大小就太大了。一个好的办法是开始时将 1,000 到 5,000 个文档作为一个批次, 如果你的文档非常大，那么就减少批量的文档个数。 密切关注你的批量请求的物理大小往往非常有用，一千个 1KB 的文档是完全不同于一千个 1MB 文档所占的物理大小。 一个好的批量大小在开始处理后所占用的物理大小约为 5-15 MB。 分片路由： 当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？ 首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的： shard = hash(routing) % number_of_primary_shards routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。 这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。 所有的文档 API（ get 、 index 、 delete 、 bulk 、 update 以及 mget ）都接受一个叫做 routing 的路由参数 ，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档——例如所有属于同一个用户的文档——都被存储到同一个分片中。我们也会在扩容设计这一章中详细讨论为什么会有这样一种需求。","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://wuzguo.com/blog/tags/Elasticsearch/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://wuzguo.com/blog/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"},{"name":"ELK","slug":"ELK","permalink":"http://wuzguo.com/blog/tags/ELK/"}],"author":"Zak"},{"title":"Java线程","slug":"server/java_muti_thread","date":"2017-07-14T12:04:20.000Z","updated":"2022-08-01T06:35:23.801Z","comments":true,"path":"2017/07/14/server/java_muti_thread.html","link":"","permalink":"http://wuzguo.com/blog/2017/07/14/server/java_muti_thread.html","excerpt":"","text":"一、进程和线程的基本概念 进程 进程是操作系统结构的基础，是一个程序及其数据在处理机上顺序执行时所发生的活动，是程序在一个数据集合上运行的过程，是系统进行资源分配和调度的一个独立单位。 线程 线程是进程中独立运行的子任务，一个进程至少包含一个线程，线程是CPU资源分配的基本单位。 多线程的优点： 充分利用CPU空闲的计算能力，提高程序运行的效率。 如上图所示，CPU完全可以在任务1和任务2之间来回切换，使任务2不必等到10秒后再运行，这样程序的运行效率得到大大提升。 二、线程的实现 线程的实现方式 继承Thread抽象类 缺点：不支持多继承，Java语言只支持单继承。 实现runable接口 实现callable接口（TestCallable.java） 与runable的区别：可以有返回值，并且可以抛出异常。 // callable的基本使用public class TestCallable &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; for (int i = 0; i &lt; 10; i++) &#123; FutureTask&lt;Result&gt; task = new FutureTask&lt;Result&gt;(new Call()); new Thread(task).start(); // 获取运行结果 Result result = task.get(); System.out.println(&quot;result: &quot; + result); &#125; &#125;&#125;class Call implements Callable&lt;Result&gt; &#123; public Result call() throws Exception &#123; System.out.println(&quot;hello world...&quot;); return new Result(new Random(10).nextInt(10), true, &quot;hello world&quot;); &#125;&#125; 执行结果： hello world...result: Result&#123;id=3, success=true, message=&#x27;hello world&#x27;, data=null&#125;hello world...result: Result&#123;id=3, success=true, message=&#x27;hello world&#x27;, data=null&#125; 实例变量和线程安全（TestShareData.java） 自定义线程类中的实例变量针对其他线程可以有共享与不共享之分，这在多个线程之间交互时是一个需要特别注意的地方。 不共享数据的情况 共享数据的情况 共享数据的情况就是多个线程可以访问同一个变量，比如在实现投票功能的程序中，多个线程可以同时处理同一个人的票数。 线程状态切换 Java语言定义了5种线程状态，在任意一个时间点，一个线程只能处于一种状态，这5种状态分别如下： 新建（New）：创建后尚未启动的线程处于这种状态。 运行（Runable）：Runable包括了操作系统线程状态的running和ready，也就是处于此状态的线程有可能正在执行，也有可能正在等着CPU分配时间片。 无限期等待（Waiting）：处于这种状态的线程不会被CPU分配时间，它们需要等待被其他线程显式的唤醒，以下方法会让线程进入无限期的等待状态： 没有设置timeout参数的Object.wait()方法。 没有设置timeout参数的Thread.join()方法。 LockSupport.park()方法。 有限期等待（Timed Waiting）：处于这种状态的线程也不会被分配CPU执行时间，不过无需等待被其他线程显示的唤醒，在一定时间之后它们会被系统自动唤醒。以下方法会让线程进入限期等待状态： Thread.sleep()方法。 设置了timeout参数的Object.wait()方法。 设置了timeout参数的Thread.join()方法。 LockSupport.parkNamos()方法。 LockSupport.parkUntil()方法。 阻塞（Blocked）：线程被阻塞了，线程阻塞状态和等待状态的区别在于： 阻塞状态在等待获取到一个排他锁，这个事件在另外一个线程放弃这个锁的时候发生。 等待状态则在等待一段时间，或者唤醒动作的时候发生。在程序等待进入同步区域的时候，线程进入这种状态。 结束（Terminated）：已终止线程的线程状态，线程已结束执行。 三、线程的使用 currentThread()方法currentThread()方法可返回代码段正在被调用的线程信息。（TestCurrentThread.java） isAlive()方法判断当前线程是否处于活动状态（指已启动并且尚未终止）。（TestIsAlive.java） 另外在使用isAlive()方法时，如果将线程对象以构造方法的方式出递给Thread对象进行start()启动时，运行的结果和前面示例是有差异的。造成这样的差异的原因来自于Thread.currentThread()和this的差异。 sleep()方法让当前“当前正在执行”的线程休眠（暂停执行）指定的毫秒数，这个”正在执行“的线程指this.currentThread()返回的线程。（TestSleep.java） yield()方法yield方法表示是当前线程放弃CPU资源，给其他线程和当前线程同台竞争的机会，至于谁将获取CPU资源还得看缘分。（TestYield.java） join()方法join()方法的作用是让调用join()方法的线程执行完成之后再执行当前线程。（TestJoin.java TestJoinWithParam.java） 在join过程中，如果当前线程对象被打断，则当前线程出现异常。（TestJoinException.java） join()方法和sleep()方法的区别 join()方法内部使用wait(long)方法实现，所以join()方法具有释放锁的特点。（TestJoinSleep.java） sleep()不释放锁。 public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException(&quot;timeout value is negative&quot;); &#125; if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125;&#125; wait()方法wait()方法是使当前执行代码的线程进行等待。在调用wait()方法之前，线程必须获得该对象的对象级别锁（TestWaitNotSync.java），即只能在同步方法或同步块中调用wait()方法，如果调用时没有持有适当的锁，会抛出IllegalMonitorStateException异常。调用wait()方法后，线程持有的锁会主动释放。（TestWaitReleaseLock.java） public class TestWaitNotSync &#123; public static void main(String[] args) throws InterruptedException &#123; String str = new String(); str.wait(); &#125;&#125; 执行结果如下： Exception in thread &quot;main&quot; java.lang.IllegalMonitorStateExceptionat java.lang.Object.wait(Native Method)at java.lang.Object.wait(Object.java:503)at com.github.wuzguo.thread.TestWaitNotSync.main(TestWaitNotSync.java:14) notify()&#x2F;notifyAll()方法唤醒一个或所有呈wait状态的线程。notify()/notifyAll()方法也要在同步方法或同步块中调用，如果调用时没有持有适当的锁，也会抛出IllegalMonitorStateException异常。（TestWaitNotify.java）notify()/notifyAll()方法不释放当前持有的锁。（TestNotifyHoldLock.java） 线程优先级在操作系统中线程可以划分优先级，优先级较高的线程得到的CPU执行时间片就越多（不是优先级越高执行执行时间就越长）。也就是CPU优先执行优先级较高的线程对象中的任务。 设置优先级有助于帮“线程调度器（）”确定下一次选择哪一个线程来优先执行。 /** * The minimum priority that a thread can have. */ public final static int MIN_PRIORITY = 1;/** * The default priority that is assigned to a thread. */ public final static int NORM_PRIORITY = 5; /** * The maximum priority that a thread can have. */ public final static int MAX_PRIORITY = 10; 优先级具有继承性（A线程启动B线程，则B线程的优先级与A的优先级一样）。（TestPriority.java） 需要注意的是Java线程优先级并不是和操作系统的线程优先级一一对应，原因在于Java的线程通过映射到系统的原生线程上实现，所以线程的调度最后还取决于操作系统，Solaris操作系统中有2147483678（2$32$）种优先级，但是windows操作系统中只有七种，如下： Java线程优先级 Windows线程优先级 1（MIN_PRIORITY） THREAD_PRIORITY_LOWEST 2 THREAD_PRIORITY_LOWEST 3 THREAD_PRIORITY_BELOW_NORMAL 4 THREAD_PRIORITY_BELOW_NORMAL 5（NORM_PRIORITY） THREAD_PRIORITY_NORMAL 6 THREAD_PRIORITY_ABOVE_NORMAL 7 THREAD_PRIORITY_ABOVE_NORMAL 8 THREAD_PRIORITY_HIGHEST 9 THREAD_PRIORITY_HIGHEST 10（MAX_PRIORITY） THREAD_PRIORITY_CRITICAL 暂停线程暂停线程意味着此线程还可以恢复运行，在Java多线程中，可以使用suspend() （deprecated）方法暂停线程，使用resume() （deprecated）方法恢复线程的执行。（TestSuspendResume.java） 使用suspend()方法和resume()方法的缺点： 使用不当，容易造成公共同步对象的独占，其他线程线程无法访问公共同步的对象。（TestSuspendResumeDealLock.java） 容易出现线程的暂停而导致数据不同步的问题。（TestSuspendResumeNoSameValue.java） 停止线程停止一个线程意味着在线程处理完任务之前停掉正在做的操作，停止一个线程可以使用Thread.stop()方法，他不是安全的（unsafe）而且已经被作废（deprecated）。大多数停止一个线程的操作都是使用Thread.interrupt()方法，但是这个方法并不会中止正在运行的线程，还需要加入一个判断条件才能停止线程。 （TestInterrupt.java） 在Java中可以用以下三种方法中止正在运行的线程： 使用退出标志，是线程正常退出，也就是当run()方法完成后线程终止。 使用stop()方法强行中止线程，但是不推荐使用，使用它们可能产生不可预料的结果。 使用interrupt()方法。 判断线程不是不是停止状态： this.interrupted()：测试当前（运行this.interrupted()方法的线程）线程是否已经中断。 this.isInterrupted()：测试线程是否已经中断。 （TestInterruptStatus.java） /** * Tests whether the current thread has been interrupted. The * &lt;i&gt;interrupted status&lt;/i&gt; of the thread is cleared by this method. In * other words, if this method were to be called twice in succession, the * second call would return false (unless the current thread were * interrupted again, after the first call had cleared its interrupted * status and before the second call had examined it). * * &lt;p&gt;A thread interruption ignored because a thread was not alive * at the time of the interrupt will be reflected by this method * returning false. * * @return &lt;code&gt;true&lt;/code&gt; if the current thread has been interrupted; * &lt;code&gt;false&lt;/code&gt; otherwise. * @see #isInterrupted() * @revised 6.0 */public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125;/** * Tests whether this thread has been interrupted. The &lt;i&gt;interrupted * status&lt;/i&gt; of the thread is unaffected by this method. * * &lt;p&gt;A thread interruption ignored because a thread was not alive * at the time of the interrupt will be reflected by this method * returning false. * * @return &lt;code&gt;true&lt;/code&gt; if this thread has been interrupted; * &lt;code&gt;false&lt;/code&gt; otherwise. * @see #interrupted() * @revised 6.0 */public boolean isInterrupted() &#123; return isInterrupted(false);&#125; 四、源代码地址博客所使用的源码地址：thread-examples","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"Thread","slug":"Thread","permalink":"http://wuzguo.com/blog/tags/Thread/"},{"name":"多线程","slug":"多线程","permalink":"http://wuzguo.com/blog/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"Runable","slug":"Runable","permalink":"http://wuzguo.com/blog/tags/Runable/"}],"author":"Zak"},{"title":"Java内存模型（一）","slug":"server/Java_memory_model_1","date":"2017-06-29T14:38:40.000Z","updated":"2022-08-01T06:35:23.824Z","comments":true,"path":"2017/06/29/server/Java_memory_model_1.html","link":"","permalink":"http://wuzguo.com/blog/2017/06/29/server/Java_memory_model_1.html","excerpt":"","text":"一、概述Java内存模型即Java Memory Model（JMM），JVM规范试图通过JMM来屏蔽各种硬件和操作系统的内存访问差异，实现让Java程序在各种平台下都能达到一致的内存访问效果，规避像C、C++等主流编程语言直接使用物理硬件和操作系统的内存模型，因为不同平台上内存模型的差异可能导致并发访问的时经常出错，不得不针对不同平台编写不同程序的问题，实现一次编译，到处运行的设计思想。 二、Java与线程并发编程 多任务和高并发是现代计算机系统必备的功能，是衡量计算机处理器的能力重要指标。为了减少在磁盘I&#x2F;O、网络通信、或者数据库访问等耗时操作上造成的计算能力上的资源浪费，多任务处理是其最常用的手段。由于计算机的存储设备与处理器的运算能力之间存在着巨大差距，所以现代计算机系统都不得不加入读写速度尽可能接近处理器运算速度的高速缓存来作为内存与处理器之间的缓冲：将运算需要使用到的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中没这样处理器就无需等待缓慢的内存读写了。 基于高速缓存的存储交互很好地解决了处理器与内存的速度矛盾，但是引入了新的问题：缓存一致性（Cache Coherence）。在多处理器系统中，每个处理器都有自己的高速缓存，而他们又共享同一主存，如下图所示：多个处理器运算任务都涉及同一块主存，需要一种协议（MSI、MESI、MOSI 等）可以保障数据的一致性。 Java虚拟机内存模型中定义的内存访问操作与硬件的缓存访问操作是具有可比性的。 线程的基本概念线程是比进程更轻量级的调度执行单位，线程可以把一个进程的资源分配和执行调度分开，各个线程既可以共享进程资源（内存地址，文件I&#x2F;O等），又可以独立调度（线程是CPU的调度基本单位）。进程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉。所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用多线程模式不能用多进程模式。实现线程主要有三种方式：内核线程实现、用户线程实现 和 使用用户线程加轻量级进程混合实现。 内核线程内核线程（Kernel-Level Thread KLT） 就是直接由操作系统内核支持的线程，这种线程由内核来完成线程切换，内核通过操作调度器（Thread Scheduler）对线程进行调度，并负责将线程的任务映射到处理器上，每个内核线程可以视为内核的分身，这样操作系统就有能力同事处理多件事情，支持多线程的内核就叫做多线程内核（Multi-Threads Kernel）。程序一般不会直接去使用内核线程，而是去使用内核线程的一种高级接口 — 轻量级进程（Light Weight Process LWP）。 轻量级进程轻量级进程就是我们通常意义上的线程，由于每个LWP都与一个特定的内核线程关联，因此每个LWP都是一个独立的线程调度单元。即使有一个LWP在系统调用中阻塞，也不会影响整个进程的执行，但是轻量级进程具有局限性：首先，大多数LWP的操作，如 建立、析构以及同步都需要进行系统调用，系统调用的代价相对较高，需要在user mode和kernel mode中切换。其次，每个LWP都需要有一个内核线程支持，因此LWP要消耗内核资源（内核线程的栈空间），因此一个系统不能支持大量的LWP。 用户线程 LWP虽然本质上属于用户线程，但LWP线程库是建立在内核之上的，LWP的许多操作都要进行系统调用，因此效率不高。而这里的用户线程（User Thread, UT）指的是完全建立在用户空间的线程库，用户线程的建立，同步，销毁，调度完全在用户空间完成，不需要内核的帮助。因此这种线程的操作是极其快速的且低消耗的。 上图是最初的一个用户线程模型，从中可以看出，进程中包含线程，用户线程在用户空间中实现，内核并没有直接对用户线程进程调度，用户线程之间的调度由在用户空间实现的线程库实现，其缺点是：一个用户线程如果阻塞在系统调用中，则整个进程都将会阻塞。 使用用户线程加轻量级进程混合实现用户线程库还是完全建立在用户空间中，因此用户线程的操作还是很廉价，因此可以建立任意多需要的用户线程。操作系统提供了LWP作为用户线程和内核线程之间的桥梁。LWP还是和前面提到的一样，具有内核线程支持，是内核的调度单元，并且用户线程的系统调用要通过LWP，因此进程中某个用户线程的阻塞不会影响整个进程的执行。用户线程库将建立的用户线程关联到LWP上，LWP与用户线程的数量不一定一致。当内核调度到某个LWP上时，此时与该LWP关联的用户线程就被执行。 当多个线程访问一个对象时，如果不用考虑这个线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那这个对象是线程安全的。 线程间的通信通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 线程之间的同步同步是指程序用于控制不同线程之间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。如果编写多线程程序的Java程序员不理解隐式进行的线程之间通信的工作机制，很可能会遇到各种奇怪的内存可见性问题。 线程调度线程调度指系统为线程分配处理器使用权的过程，主要有两种调度方式： 协同式调度（Cooperative Threads-Scheduling）：线程执行时间由线程自身控制，线程把自己的工作执行完成之后才主动通知系统切换到另外的线程。如果一个线程出错会阻塞整个系统的运行。 抢占式调度（Preemptive Threads-Scheduling）：线程的执行时间由系统根据线程的优先级来分配，优先级越高的线程越容易被系统选择执行。如果一个线程出错不会阻塞其他线程。 参考文献 《深入理解Java虚拟机：JVM高级特性与最佳实践》，周志明著 深入理解java内存模型系列文章 全面理解Java内存模型","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://wuzguo.com/blog/tags/JVM/"},{"name":"JMM","slug":"JMM","permalink":"http://wuzguo.com/blog/tags/JMM/"},{"name":"内存模型","slug":"内存模型","permalink":"http://wuzguo.com/blog/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"}],"author":"Zak"},{"title":"Java内存模型（二）","slug":"server/Java_memory_model_2","date":"2017-06-29T14:38:40.000Z","updated":"2022-08-01T06:35:23.610Z","comments":true,"path":"2017/06/29/server/Java_memory_model_2.html","link":"","permalink":"http://wuzguo.com/blog/2017/06/29/server/Java_memory_model_2.html","excerpt":"","text":"三、Java内存模型的抽象主内存和工作内存Java内存模型的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量（指实例字段、静态字段和构成数组对象的元素，但是不包含局部变量与方法参数，因为局部变量与方法参数是线程私有的，不会被共享）的底层细节。Java内存模型中规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存（可以与前面将的处理器的高速缓存类比），线程的工作内存中保存了该线程使用到的变量到主内存副本拷贝，线程对变量的所有操作（读取、赋值）都必须在工作内存中进行，而不能直接读写主内存中的变量。不同线程之间无法直接访问对方工作内存中的变量，线程间变量值的传递均需要在主内存来完成，线程、主内存和工作内存的交互关系如下图所示： 关于主内存与工作内存之间具体的交互协议，即如何从主内存拷贝到工作内存、如何从工作内存同步回主内存之类的实现细节，Java定义了8种操作来完成： 操作 说明 lock(锁定) 作用于主内存的变量，把一个变量标识为一条线程独占的状态 unclock（解锁） 作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定 read（读取） 作用于主内存的变量，把一个变量的值从主内存传输到线程的工作内存，以便随后的load动作使用 load（载入） 作用于工作内存的变量，把read操作从主内存中得到的变量值放入工作内存的变量副本中 use（使用） 作用于工作内存的变量，把工作内存中一个变量的值传递给执行引擎 assign（赋值） 作用于工作内存的变量，把执行引擎接收到的值赋给工作内存的变量 store（存储） 作用于工作内存的变量，把工作内存中一个变量的值传送给主内存中，以便随后的write操作使用 write（写入） 作用于主内存的变量，把store操作从工作内存中得到的变量的值放入主内存的变量中 如果要把一个变量从主内存复制到工作内存，那就要顺序地执行read和load操作，如果要把变量从工作内存同步回主内存，那就要顺序地执行store和write操作，注意Java内存模型只要求上述两个操作必须顺序地执行，而没有保证是连续执行。 内存模型的抽象在Java虚拟机中，所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享。局部变量（Local variables），方法定义参数和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。Java线程之间的通信由Java内存模型（JMM）控制，**JMM 决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读&#x2F;写共享变量的副本。**本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。Java内存模型的抽象示意图如下： 从上图来看，线程A与线程B之间如要通信的话，必须要经历下面2个步骤： 首先，线程A把本地内存A中更新过的共享变量刷新到主内存中去。 然后，线程B到主内存中去读取线程A之前已更新过的共享变量。 下面通过示意图来说明这两个步骤：如上图所示，本地内存A和B有主内存中共享变量x的副本。假设初始时，这三个内存中的x值都为0。线程A在执行时，把更新后的x值（假设值为1）临时存放在自己的本地内存A中。当线程A和线程B需要通信时，线程A首先会把自己本地内存中修改后的x值刷新到主内存中，此时主内存中的x值变为了1。随后，线程B到主内存中去读取线程A更新后的x值，此时线程B的本地内存的x值也变为了1。从整体来看，这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性保证。 四、JVM对Java内存模型的实现 在JVM内部，Java内存模型把内存分成了两部分：线程栈区 和 堆区，下图展示了Java内存模型在JVM中的逻辑视图： JVM中运行的每个线程都拥有自己的线程栈，线程栈包含了当前线程执行的方法调用相关信息，我们也把它称作调用栈。随着代码的不断执行，调用栈会不断变化。线程栈还包含了当前方法的所有本地变量信息。一个线程只能读取自己的线程栈，也就是说，线程中的本地变量对其它线程是不可见的。即使两个线程执行的是同一段代码，它们也会各自在自己的线程栈中创建本地变量，因此，每个线程中的本地变量都会有自己的版本。所有原始类型（boolean、byte、short、char、int、long、float、double）的本地变量都直接保存在线程栈当中，对于它们的值各个线程之间都是独立的。对于原始类型的本地变量，一个线程可以传递一个副本给另一个线程，它们之间是不能共享的。堆区包含了Java应用创建的所有对象信息，不管对象是哪个线程创建的，其中的对象包括原始类型的封装类（如Byte、Integer、Long等）。不管对象是属于一个成员变量还是方法中的本地变量，它都会被存储在堆区。 下图展示了调用栈和本地变量都存储在栈区，对象都存储在堆区：​ 一个本地变量如果是原始类型，那么它会被完全存储到栈区。 一个本地变量也有可能是一个对象的引用，这种情况下，这个本地引用会被存储到栈中，但是对象本身仍然存储在堆区。 ​ 对于一个对象的成员方法，这些方法中包含本地变量，仍需要存储在栈区，即使它们所属的对象在堆区。​ 对于一个对象的成员变量，不管它是原始类型还是包装类型，都会被存储到堆区。 ​ static 类型的变量以及类本身相关信息都会随着类本身存储在堆区。 ​ 堆中的对象可以被多线程共享。如果一个线程获得一个对象的引用，它便可访问这个对象的成员变量。但是对于本地变量，每个线程都会拷贝一份到自己的线程栈中。 ​ 下图展示了上面描述的过程: 不管是什么内存模型，最终还是运行在计算机硬件上的，所以我们有必要了解计算机硬件内存架构，下图就简单描述了当代计算机硬件内存架构： ​ 现代计算机一般都有2个以上CPU，而且每个CPU还有可能包含多个核心。因此，如果我们的应用是多线程的话，这些线程可能会在各个CPU核心中并行运行。当一个CPU需要访问主存时，会先读取一部分主存数据到CPU缓存，进而在读取CPU缓存到寄存器。当CPU需要写数据到主存时，同样会先flush寄存器到CPU缓存，然后再在某些节点把缓存数据flush到主存。 五、Java内存模型和硬件架构之间的桥接​ 正如上面讲到的，Java内存模型和硬件内存架构并不一致。从硬件上看，不管是栈还是堆，大部分数据都会存到主存中，当然一部分栈和堆的数据也有可能会存到CPU寄存器中，如下图所示，Java内存模型和计算机硬件内存架构是一个交叉关系： 当对象和变量存储到计算机的各个内存区域时，必然会面临一些问题，其中最主要的两个问题是： 共享对象对各个线程的可见性​ 当多个线程同时操作同一个共享对象时，如果没有合理的使用 volatile 或 synchronized 、Lock 等，一个线程对共享对象的更新有可能导致其它线程不可见。 ​ 想象一下我们的共享对象存储在主存，一个CPU中的线程读取主存数据到CPU缓存，然后对共享对象做了更改，但CPU缓存中的更改后的对象还没有flush到主存，此时线程对共享对象的更改对其它CPU中的线程是不可见的。最终就是每个线程最终都会拷贝共享对象，而且拷贝的对象位于不同的CPU缓存中。 ​ 如图：左边CPU中运行的线程从主存中拷贝共享对象obj到它的CPU缓存，把对象obj的count变量改为2。但这个变更对运行在右边CPU中的线程不可见，因为这个更改还没有flush到主存中： ​ ​ 要解决共享对象可见性这个问题，我们可以使用 volatile 关键字。 volatile 关键字可以保证变量会直接从主存读取，而对变量的更新也会直接写到主存。**volatile原理是基于CPU内存屏障指令实现的**，后面会讲到。 共享对象的竞争现象​ 如果多个线程共享一个对象，如果它们同时修改这个共享对象，这就产生了竞争现象。 ​ 如下图所示，线程A和线程B共享一个对象obj。假设线程A从主存读取obj.count变量到自己的CPU缓存，同时，线程B也读取了obj.count变量到它的CPU缓存，并且这两个线程都对obj.count做了加1操作。此时obj.count加1操作被执行了两次，不过都在不同的CPU缓存中。 ​ 如果这两个加1操作是串行执行的，那么obj.count变量便会在原始值上加2，最终主存中的obj.count的值会是3。然而下图中两个加1操作是并行的，不管是线程A还是线程B先flush计算结果到主存，最终主存中的obj.count只会增加1次变成2，尽管一共有两次加1操作。 参考文献 《深入理解Java虚拟机：JVM高级特性与最佳实践》，周志明著 深入理解java内存模型系列文章 全面理解Java内存模型","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://wuzguo.com/blog/tags/JVM/"},{"name":"JMM","slug":"JMM","permalink":"http://wuzguo.com/blog/tags/JMM/"},{"name":"内存模型","slug":"内存模型","permalink":"http://wuzguo.com/blog/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"}],"author":"Zak"},{"title":"Java内存模型（三）","slug":"server/Java_memory_model_3","date":"2017-06-29T14:38:40.000Z","updated":"2022-08-01T06:35:23.666Z","comments":true,"path":"2017/06/29/server/Java_memory_model_3.html","link":"","permalink":"http://wuzguo.com/blog/2017/06/29/server/Java_memory_model_3.html","excerpt":"","text":"六、支撑Java内存模型的基础原理指令重排序 在执行程序时，为了提高性能，编译器和处理器会对指令做重排序。但是，JMM确保在不同的编译器和不同的处理器平台之上，通过插入特定类型的Memory Barrier来禁止特定类型的编译器重排序和处理器重排序。重排序分三种类型： 编译器优化的重排序：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序：现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序：由于处理器使用缓存和读&#x2F;写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从java源代码到最终实际执行的指令序列，会分别经历下面三种重排序： ​ 上图中的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。 对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。 对于处理器重排序，JMM的处理器重排序规则会要求java编译器在生成指令序列时，插入特定类型的内存屏障（Memory Barrier）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供了一致的内存可见性保证。 内存屏障（Memory Barrier）​ 上面讲到了通过内存屏障可以禁止特定类型处理器的重排序，从而让程序按我们预想的流程去执行。内存屏障，又称内存栅栏，是一个CPU指令，具有以下作用： 保证特定操作的执行顺序。 影响某些数据的内存可见性。 编译器和CPU能够重排序指令，保证最终相同的结果，尝试优化性能。插入一条 Memory Barrier 指令会告诉编译器和CPU：不管什么指令都不能和这条Memory Barrier指令重排序。 ​ Memory Barrier所做的另外一件事是强制刷出各种CPU cache，如一个Write-Barrier（写入屏障）将刷出所有在Barrier之前写入cache的数据，因此，任何CPU上的线程都能读取到这些数据的最新版本。 ​ 如果一个变量是volatile修饰的，JMM会在写入这个字段之后插进一个Write-Barrier指令，并在读这个字段之前插入一个Read-Barrier指令。这意味着，如果写入一个volatile变量，就可以保证： 一个线程写入变量a后，任何线程访问该变量都会拿到最新值。在写入变量a之前的写入操作，其更新的数据对于其他线程也是可见的。因为Memory Barrier会刷出cache中的所有先前的写入。 ​ 现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器（指多个独立CPU而不是多核）上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的 读&#x2F;写 操作的执行顺序，不一定与内存实际发生的 读&#x2F;写 操作顺序一致。为了具体说明，请看下面示例： Processor A Processor B a &#x3D; 1; &#x2F;&#x2F;A1 b &#x3D; 2; &#x2F;&#x2F;B1 x &#x3D; b; &#x2F;&#x2F;A2 y &#x3D; a; &#x2F;&#x2F;B2 初始状态：a &#x3D; b &#x3D; 0；处理器允许执行后得到结果：x &#x3D; y &#x3D; 0； ​ 假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到x &#x3D; y &#x3D; 0的结果。具体的原因如下图所示： ​ ​ 这里处理器A和处理器B可以同时把共享变量写入自己的写缓冲区（A1，B1），然后从内存中读取另一个共享变量（A2，B2），最后才把自己写缓存区中保存的脏数据刷新到内存中（A3，B3）。当以这种时序执行时，程序就可以得到x = y = 0的结果。 ​ 从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1-&gt;A2，但内存操作实际发生的顺序却是：A2-&gt;A1。此时，处理器A的内存操作顺序被重排序了。这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作重排序。 下面是常见处理器允许的重排序类型的列表： Load-Load Load-Store Store-Store Store-Load 存在数据依赖 sparc-TSO N N N Y N x86 N N N Y N ia64 Y Y Y Y N PowerPC Y Y Y Y N 上表单元格中的 “N” 表示处理器不允许两个操作重排序，“Y” 表示允许重排序。 从上表我们可以看出： 常见的处理器都允许Store-Load重排序。 常见的处理器都不允许对存在数据依赖的操作做重排序。 sparc-TSO和x86拥有相对较强的处理器内存模型，它们仅允许对写-读操作做重排序（因为它们都使用了写缓冲区）。 注： sparc-TSO是指以TSO(Total Store Order)内存模型运行时，sparc处理器的特性。 上表中的x86包括x64及AMD64。 由于ARM处理器的内存模型与PowerPC处理器的内存模型非常类似，本文将忽略它。 数据依赖性后文会专门说明。 为了保证内存可见性，**Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。**JMM把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2; 确保Load1数据的装载，之前于Load2及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2; 确保Store1数据对其他处理器可见（刷新到内存），之前于Store2及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2; 确保Load1数据装载，之前于Store2及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2; 确保Store1数据对其他处理器变得可见（刷新到内存），之前于Load2及所有后续装载指令的装载。StoreLoad Barriers会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 ​ StoreLoad Barriers 是一个 “全能型” 的屏障，它同时具有其他三个屏障的效果。现代的多处理器大都支持该屏障。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（buffer fully flush）。 happens-before（先行发生原则）​ Java语言定义了先行发生原则来辅助volatile和synchronized等来保证内存模型所有操作的有序性，它是判断数据是否存在竞争、线程是否安全的主要依据。 ​ 先行发生原则是Java内存模型中定义的两项操作之间的偏序关系，如果说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A所产生的影响能被操作B观察到，“影响”包括修改了内存中共享变量的值，发送了消息，调用了方法等。 ​ 下面是Java内存模型下一些“天然的”先行发生原则，这些先行发生原则无需任何同步机制协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则中推导出来的话，它们就是没有顺序性保障，虚拟机接可以对它们随意进行重排序。 程序次序原则（Program Order Rule） 在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。 管程锁定规则(Monitor Lock Rule)对某个锁的unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是同一个锁，这里的“后面”是指时间上的先后顺序。 volatile变量规则(Volatile Variable Rule)对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的“后面”同样是指时间上的先后顺序。也就是说，某个线程对volatile变量写入某个值后，能立即被其它线程读取到。 线程启动规则(Thread Start Rule)Thread对象的start方法先行发生于此线程的每个动作。 线程终止规则(Thread Termination Rule)线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.join()方法结束，Thread.isAlive()的返回值等手段检测到线程是否已经终止运行。 线程中断规则(Thread Interruption Rule)对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测到是否有中断发生。 对象终结规则(Thread Termination Rule)一个对象的初始化完成(构造函数执行结束)先行发生于它的finalize()方法的开始。 传递性(Transitivity)如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。其中程序次序规则，管程锁定规则，volatile变量规则，传递性规则经常用来推断先行发生关系。需要注意的是，只有满足以上几条先行发生原则的时间上的先后操作具备顺序可靠性，时间上的先后顺序不能得出先行发生关系，如下示例代码所示： private int value = 0;public void setValue(int value) &#123; this.value=value;&#125;public int getValue() &#123; return value;&#125; ​ 假设存在线程A和线程B，线程A先（时间上的先后）调用了setValue(1)，然后线程B调用了同一个对象的getValue()，那么线程B收到的返回值是不确定的，由于工作内存和主内存同步存在延迟，也由于可能存在重排序现象。 虽然时间上线程A的setValue()操作先于线程B的 getValue() 操作，但是并不能推断出线程A的setValue()操作先行发生于线程B的getValue()操作，如果有这种先行发生关系，那么可以推断出线程B的getValue()操作获得的值。 ​ 如果我们给getValue()方法和setValue()方法添加 synchronized 关键字，就能利用管程锁定规则推断出线程A的setValue操作先行发生于线程B的getValue操作，或者我们也可以将value定义为 volatile 变量，也能利用 volatile 变量规则推断出先行发生关系。 ​ 先行发生关系也不能推断出时间上的先后执行顺序，示例代码如下所示： int i=1;int j=2; ​ 根据程序次序规则，我们可以推断出int i=1的操作先行发生于int j=2的操作，但是int j=2的代码完全有可能先被处理器执行（时间上的先后），这就是重排序，虚拟机规范是允许这种特性存在的，虚拟机可利用这种特性提高性能。 注意： 两个操作之间具有 happens-before 关系，并不意味前一个操作必须要在后一个操作之前执行。仅仅要求前一个操作的执行结果，对于后一个操作是可见的，且前一个操作按顺序排在后一个操作之前。 七、volatile关键字volatile的特性volatile关键字是Java虚拟机提供的最轻量级的同步机制，当一个变量定义为volatile后，他具备两种特性： 保证此变量对所有线程的可见性（当某个线程修改了这个变量的值，其他线程可以立刻得知）。 禁止指令重排序优化。 理解volatile特性的一个好方法是：把对volatile变量的单个读&#x2F;写，看成是使用同一个锁对这些单个读&#x2F;写操作做了同步。下面我们通过具体的示例来说明，请看下面的示例代码： class VolatileFeaturesExample &#123; //使用volatile声明64位的long型变量 volatile long vl = 0L; public void set(long l) &#123; vl = l; //单个volatile变量的写 &#125; public void getAndIncrement () &#123; vl++; //复合（多个）volatile变量的读/写 &#125; public long get() &#123; return vl; //单个volatile变量的读 &#125;&#125; 假设有多个线程分别调用上面程序的三个方法，这个程序在语义上和下面程序等价： class VolatileFeaturesExample &#123; long vl = 0L; // 64位的long型普通变量 //对单个的普通 变量的写用同一个锁同步 public synchronized void set(long l) &#123; vl = l; &#125; public void getAndIncrement () &#123; //普通方法调用 long temp = get(); //调用已同步的读方法 temp += 1L; //普通写操作 set(temp); //调用已同步的写方法 &#125; public synchronized long get() &#123; //对单个的普通变量的读用同一个锁同步 return vl; &#125;&#125; 如上面示例程序所示，对一个volatile变量的单个读&#x2F;写操作，与对一个普通变量的读&#x2F;写操作使用同一个锁（synchronized）来同步，它们之间的执行效果相同。 锁的happens-before规则保证释放锁和获取锁的两个线程之间的内存可见性，这意味着对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 简而言之，volatile变量自身具有下列特性： 可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 原子性：对任意单个volatile变量的读&#x2F;写具有原子性（即使是64位的 long 型 和 double 型变量，只要它是volatile变量，对该变量的读写就将具有原子性），但类似于volatile++这种复合操作不具有原子性。 volatile的写-读建立的happens before关系上面讲的是volatile变量自身的特性，对程序员来说，volatile对线程的内存可见性的影响比volatile自身的特性更为重要，也更需要我们去关注。 从**JSR-133 （Java内存模型与线程规范）**开始，volatile变量的写-读可以实现线程之间的通信。从内存语义的角度来说，volatile与锁有相同的效果： volatile写和锁的释放有相同的内存语义。 volatile读与锁的获取有相同的内存语义。 请看下面使用volatile变量的示例代码： class VolatileExample &#123; int a = 0; volatile boolean flag = false; public void writer() &#123; a = 1; //1 flag = true; //2 &#125; public void reader() &#123; if (flag) &#123; //3 int i = a; //4 …… &#125; &#125;&#125; 假设线程A执行writer() 方法之后，线程B执行 reader() 方法。根据 happens before 规则，这个过程建立的happens before 关系可以分为两类： 根据程序次序规则：1 happens before 2；3 happens before 4。 根据volatile规则：2 happens before 3。 根据happens before 的传递性规则：1 happens before 4。 上述happens before 关系的图形化表现形式如下： ​ 在上图中，每一个箭头链接的两个节点，代表了一个happens before 关系。黑色箭头表示程序顺序规则；橙色箭头表示volatile规则；蓝色箭头表示组合这些规则后提供的happens before保证。这里A线程写一个volatile变量后，B线程读同一个volatile变量。A线程在写volatile变量之前所有可见的共享变量，在B线程读同一个volatile变量后，将立即变得对B线程可见。 volatile写-读的内存语义 volatile写的内存语义如下： 当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。 ​ 以上面示例程序VolatileExample为例，假设线程A首先执行writer()方法，随后线程B执行reader()方法，初始时两个线程的本地内存中的flag和a都是初始状态。下图是线程A执行volatile写后，共享变量的状态示意图： 如上图所示，线程A在写flag变量后，本地内存A中被线程A更新过的两个共享变量的值被刷新到主内存中。此时，本地内存A和主内存中的共享变量的值是一致的。 volatile读的内存语义如下： 当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 下面是线程B读同一个volatile变量后，共享变量的状态示意图： ​ 如上图所示，在读flag变量后，本地内存B已经被置为无效。此时，线程B必须从主内存中读取共享变量。线程B的读取操作将导致本地内存B与主内存中的共享变量的值也变成一致的了。如果我们把volatile写和volatile读这两个步骤综合起来看的话，在读线程B读一个volatile变量后，写线程A在写这个volatile变量之前所有可见的共享变量的值都将立即变得对读线程B可见。 下面对volatile写和volatile读的内存语义做个总结： 线程A写一个volatile变量，实质上是线程A向接下来将要读这个volatile变量的某个线程发出了（其对共享变量所在修改的）消息。 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的（在写这个volatile变量之前对共享变量所做修改的）消息。 线程A写一个volatile变量，随后线程B读这个volatile变量，这个过程实质上是线程A通过主内存向线程B发送消息。 volatile内存语义的实现下面，让我们来看看JMM如何实现volatile写&#x2F;读的内存语义。 前文我们提到过重排序分为编译器重排序和处理器重排序。为了实现volatile内存语义，JMM会分别限制这两种类型的重排序类型。下面是JMM针对编译器制定的volatile重排序规则表： 是否能重排序 第二个操作 第一个操作 普通读&#x2F;写 volatile读 volatile写 普通读&#x2F;写 NO volatile读 NO NO NO volatile写 NO NO 举例来说，第三行最后一个单元格的意思是：在程序顺序中，当第一个操作为普通变量的读或写时，如果第二个操作为volatile写，则编译器不能重排序这两个操作。 从上表我们可以看出： 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能，为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略： 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 上述内存屏障插入策略非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的volatile内存语义。 下面是保守策略下，volatile写插入内存屏障后生成的指令序列示意图： ​ 上图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。 ​ 最后的StoreLoad屏障的作用是避免volatile写与后面可能有的volatile读&#x2F;写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面，是否需要插入一个StoreLoad屏障（如：一个volatile写之后方法立即return）。 ​ 为了保证能正确实现volatile的内存语义，JMM 有两种保守策略可选择：在每个volatile写的后面或在每个volatile读的前面插入一个StoreLoad屏障。 ​ 从整体执行效率的角度考虑，JMM选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里我们可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。 ​ 下面是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图： ​ 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。下面我们通过具体的示例代码来说明： class VolatileBarrierExample &#123; int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() &#123; int i = v1; //第一个volatile读 int j = v2; //第二个volatile读 a = i + j; //普通写 v1 = i + 1; //第一个volatile写 v2 = j * 2; //第二个 volatile写 &#125; //... //其他方法&#125; ​ 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化： ​ 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器常常会在这里插入一个StoreLoad屏障。 ​ 上面的优化是针对任意处理器平台，由于不同的处理器有不同“松紧度”的处理器内存模型，内存屏障的插入还可以根据具体的处理器内存模型继续优化。以x86处理器为例，上图中除最后的StoreLoad屏障外，其它的屏障都会被省略。 前面保守策略下的volatile读和写，在 x86处理器平台可以优化成： volatile 与 synchronized 的比较volatile本质是在告诉JVM当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取。synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。 它们之间的区别可以总结为以下几点： volatile仅能使用在变量级别。synchronized则可以使用在变量、方法、和类级别的。 volatile仅能实现变量的修改可见性，并不能保证原子性。synchronized则可以保证变量的修改可见性和原子性。 volatile不会造成线程的阻塞。synchronized可能会造成线程的阻塞。 volatile标记的变量不会被编译器优化。synchronized标记的变量可以被编译器优化。 关于volatile变量的可见性，经常会被误解，认为以下描述成立：volatile变量对所有线程是立即可见的，对volatile变量所有的写操作都能立即反应到其他线程中，volatile变量在各个线程中的值总是一致的，所以基于volatile变量的运算在并发操作下是安全的。其实volatile变量在各个线程的工作内存中不存在一致性问题，但是Java里面的运算并非原子操作，导致volatile变量的运算在并发下一样不是安全的，我们通过以下代码演示说明： public class VolatileTest &#123; private static volatile int race = 0; private static final int THREADS_COUNT = 20; private static final CountDownLatch latch = new CountDownLatch(THREADS_COUNT); public static void increase() &#123; race++; &#125; public static void main(String[] args) &#123; for (int n = 0; n &lt; THREADS_COUNT; n++) &#123; new Thread(new Runnable() &#123; public void run() &#123; for (int i = 0; i &lt; 10000; i++) &#123; increase(); &#125; latch.countDown(); &#125; &#125;).start(); &#125; try &#123; // 等待所有线程都结束 latch.await(); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;exception: &quot; + e.getMessage()); &#125; System.out.println(&quot;race: &quot; + race); &#125;&#125; 这段代码发起20个线程对volatile修饰的race变量进行10000次自增操作，预期结果应该是200000，但是每次运行得到的结果都不一样。问题就出在自增运算race++中，通过 javap -verbose VolatileTest 反编译代码的class文件发现只有一行代码的increase()方法在Class文件中是由四条指令构成的。 ...... public static void increase(); descriptor: ()V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=0, args_size=0 0: getstatic #2 // Field race:I 3: iconst_1 4: iadd 5: putstatic #2 // Field race:I 8: return LineNumberTable: line 32: 0 line 33: 8 ...... 从字节码层面上就很容易分析出原因：当getstatic指令吧race的值渠道操作栈顶时，volatile关键字保证了race的值在此时是正确的，但是在执行iconst_1、iadd这些指令的时候，其他线程可能已经把race的值加大了，而在操作栈顶的值就变成了过去的数据，所以putstatic指令执行后可能就把较小的race值同步回主内存之中。 参考文献 《深入理解Java虚拟机：JVM高级特性与最佳实践》，周志明著 深入理解java内存模型系列文章 全面理解Java内存模型","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://wuzguo.com/blog/tags/JVM/"},{"name":"JMM","slug":"JMM","permalink":"http://wuzguo.com/blog/tags/JMM/"},{"name":"内存模型","slug":"内存模型","permalink":"http://wuzguo.com/blog/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"}],"author":"Zak"},{"title":"JVM执行引擎（一）","slug":"server/jvm_execution_engine_1","date":"2017-06-23T12:00:25.000Z","updated":"2022-08-01T06:35:23.639Z","comments":true,"path":"2017/06/23/server/jvm_execution_engine_1.html","link":"","permalink":"http://wuzguo.com/blog/2017/06/23/server/jvm_execution_engine_1.html","excerpt":"","text":"一、JVM的结构 执行引擎是Java虚拟机（JVM）最核心的组成部分之一。执行引擎在Class文件的时候可能会有解释执行（通过解释器执行）和编译执行（通过及时编译器产生本地代码执行）两种选择，也可以二者兼备，甚至还可能会包含几个不同级别的编译器执行引擎，但是从外观上看所有的执行引擎都是一致的：输入的是字节码，处理过程是字节码解析的等效过程，输出的是执行结果。 Java虚拟机 = 类加载器（classloader） + 执行引擎（execution engine） + 运行时数据区域（runtime data area） 二、机器级代码指令体系结构（Instruction set architecture, ISA）是计算机系统对机器级程序的行为的抽象，它是指挥机器工作的指示和命令的集合，大多数ISA（如 IA32、X86-64）将程序的行为描述成好像每条指令是按顺序执行的，但是处理器的硬件远比描述的精细复杂，它们可以并发的执行许多指令，但是可以采取措施保证整体行为与ISA指定的顺序执行完全一致。执行程序的过程就是计算机的工作过程。指令通常包括两方面的内容：操作码和操作数，操作码决定要完成的操作，操作数指参加运算的数据及其所在的单元地址。在计算机中，操作要求和操作数地址都由二进制数码表示，分别称作操作码和地址码，整条指令以二进制编码的形式存放在存储器中。 指令集是CPU(或虚拟机)支持的指令的集合，指令集种类和多少与具体的机型有关： CPU指令集存储在CPU内部，对CPU运算进行指导和优化的硬程序。拥有这些指令集，CPU就可以更高效地运行。Intel主要有x86，EM64T，MMX，SSE，SSE2，SSE3，VMX等指令集。AMD主要是x86，x86-64，3D-Now!指令集。 指令集的分类： 精简指令集，即RISC指令集Reduced Instruction Set Computer： 这种指令集的特点是指令数目少，每条指令都采用标准字长、执行时间短、中央处理器的实现细节对于机器级程序是可见的。 复杂指令集，即CISC指令集Complex Instruction Set Computer：在CISC微处理器中，程序的各条指令是按顺序串行执行的，每条指令中的各个操作也是按顺序串行执行的。顺序执行的优点是控制简单，但计算机各部分的利用率不高，执行速度慢。 通俗的理解，RICS指令集是针对CISC指令集中的一些常用指令进行优化设计，放弃了一些复杂的指令，对于复杂的功能，需要通过组合指令来完成。自然，两者的使用场合不一样，对于复杂的系统，CISC更合适，否则，RICS更合适，且低功耗。 常见指令（后续说的指令都指IA32体系指令）： 大多数指令有一个或多个操作数（Operand）,指示出执行一个操作中药引用的源数据值，以及放置结果的位置。操作数可以被分为以下三种类型： 立即数（immediate），也就是常数值。ATT(AT&amp;T)格式的汇编代码中，立即数书写格式为：‘$’ + 标准C表示法表示的整数。 寄存器（register），它表示某个寄存器的内容，对双字操作来说，可以是8个32位寄存器中的一个（如：%eax）,对字操作来说，可以是8个16位寄存器中的一个（如：%ax）,或者对字节操作来说，可以是8个单字节寄存器元素总的一个（如：%al）,用E$a$来表示任意寄存器，用引用R[E$a$]来表示它的值。 存储器（memory）引用，它会根据计算出来的地址（通常称为有效地址）访问某个存储器位置，可以把存储器看成一个很大的字节数组，用符号M$b$[Addr]表示对存储在存储器中从地址Addr开始的b个字节值得引用。 类型 格式 操作数值 名称 立即数 $Imm Imm 立即数寻址 寄存器 E$a$ R[E$a$] 寄存器寻址 存储器 Imm M[Imm] 绝对寻址 存储器 (E$a$) M[R[E$a$]] 间接寻址 存储器 Imm(E$b$) M[Imm+R[E$b$]] （基址+偏移量）寻址 存储器 (E$b$,E$i$) M[R[Eb]+R[E$i$]] 变址寻址 存储器 Imm(E$b$,E$i$) M[Imm+R[E$b$]+R[E$i$]] 变址寻址 存储器 (,E$i$,$s$) M[R[E$i$]*$s$] 比例变址寻址 存储器 Imm(,E$i$,$s$) M[Imm+R[E$i$]*$s$] 比例变址寻址 存储器 (E$b$,E$i$,$s$) M[R[E$b$]+R[E$i$]*$s$] 比例变址寻址 存储器 Imm(E$b$,E$i$,$s$) M[Imm+R[E$b$]+R[E$i$]*$s$] 比例变址寻 数据传送指令将数据从一个位置复制到另一个位置。操作数表示的通用性使得一条简单的数据传送指令能够完成在许多机器中要好几条指令才完成的操作。 指令 效果 描述 MOV S, D D &lt;– S 传送 movb 传送字节 movw 传送字 movl 传送双字 MOVS S, D D &lt;— 符号扩展（S） 传送符号扩展的字节 movsbw 将做了符号扩展的字节传送到字 movsbl 将做了符号扩展的字节传送到双字 movwl 将做了符号扩展的字传送到双字 MOVZ S, D D &lt;—零扩展（S） 传送零扩展的字节 movzbw 将做了零扩展的字节传送到字 movzbl 将做了零扩展的字节传送到双字 movzwl 将做了零扩展的字传送到双字 pushl S R[%esp] &lt;– R[%esp] - 4 ; M[R[%esp]] &lt;– S 将双字压栈 popl D D &lt;– M[R[%esp]] ; R[%esp] &lt;– R[%esp+4] 将双字出栈 算术和逻辑操作 指令 效果 描述 leal S, D D &lt;– &amp;S 加载有效地址 INC D D &lt;– D + 1 加 1 DEC D D &lt;– D - 1 减 1 NEG D D &lt;– -D 取负 NOT D D &lt;– ~D 取补 ADD S, D D &lt;– D + S 加 SUB S, D D &lt;– D - S 减 IMUL S, D D &lt;– D * S 乘 XOR S, D D &lt;– D ^ S 异或 OR S, D D &lt;– D ①S 或 AND S, D D &lt;– D &amp; S 与 SAL k, D D &lt;– D &lt;&lt; k 左移 SHL k, D D &lt;– D &lt;&lt; k 左移（等同于SAL） SAR k, D D &lt;– D &gt;&gt;$A$k 算术右移 SHR k, D D &lt;– D &gt;&gt; $L$k 逻辑右移 **注意： ① 处的符号是 &#x2F; , 跟markdown有冲突 ** JVM指令集JVM的指令由一个字节长度的（指令总数不超过256个）、代表着某种特定操作含义的数字（Opcode,操作码）以及跟随其后的零至多个代表操作所需参数（Operands,操作数）而构成，由于Java虚拟机曹勇面向操作数栈而不是寄存器的架构，所有大多数的指令都不包含操作数，只有一个操作码： 与数据类型相关的指令 对于大部分与数据类型相关的字节码指令，它们的操作码助记符中大部分都有特殊的字符来表明专门为哪种类型服务：i代表int类型的数据操作，l代表long,s代表short,b代表byte,c代表char,f代表float,d代表double，a代表reference。 Opcode byte short int long float double char reference Tstore istore lstore fstore dstore astore Tinc iinc Taload baload saload iaload laload faload daload aaload Tastore bastore sastore iastore lastore fstore dastore aastore Tadd iadd ladd fadd dadd Tsub isub lsub fsub dsub Tmul imul lmul fmul dmul Tdiv idiv ldiv fdiv ddiv Trem irem lrem frem drem Tneg ineg lneg fneg dneg Tshl ishl lshl Tshr ishr lshr Tushr iushr lushr Tand iand lor Tor ior Txor ixor lxor i2T i2b i2s i2l i2f i2d l2T l2i l2f l2d f2T f2i f2l f2d d2T d2i d2l d2f Tcmp lcmp Tcmpl fcmpl dcmpl Tcmpg fcmpg dcmpg if_TcmpOP if_icmpOP if_acmpOP Treturn ireturn lreturn freturn dreturn areturn 加载和存储指令 加载和存储指令用于将数据在栈帧中的局部变量表和操作数栈之间来回传输，这类指令包括如下内容： 将一个局部变量加载到操作栈：iload、iload_&lt;n&gt;、lload、lload_&lt;n&gt;、fload、fload_&lt;n&gt;、dload、dload_&lt;n&gt;、aload、aload_&lt;n&gt;。 将一个数值从操作数栈存储到局部变量表：istore、istore_&lt;n&gt;、lstore、lstore_&lt;n&gt;、fstore、fstore_&lt;n&gt;、dstore、dstore__&lt;n&gt;、astore、astore_&lt;n&gt;。 将一个常量加载到操作数栈：bipush、sipush、idc、idc_w、idc2_w、aconst_null、iconst_ml、iconst_&lt;i&gt;、lconst_&lt;l&gt;、fconst_&lt;f&gt;、dconst_&lt;d&gt;。 扩充局部变量表的访问索引的指令：wide。 运算指令： 运算或算术指令用于对两个操作数栈上的值进行某种特定运算，并把结果重新存入到操作栈顶，大体上算术指令可以分为两种：对整形数据进行运算的指令与对浮点型数据进行运算的指令，无论是哪种算术指令，都使用Java虚拟机的数据类型，由于没有直接支持byte、short、char、和boolean类型的算术指令，对于这类数据的运算，应使用int类型的指令代替。 加法指令：iadd、ladd、fadd、dadd。 减法指令：isub、lsub、fsub、dsub。 乘法指令：imul、lmul、fmul、dmul。 除法指令：idiv、idiv、fdiv、ddiv。 求入指令：irem、lrem、frem、drem。 取反指令：ineg、lneg、fneg、dneg。 位移指令：ishl、ishr、iushr、lshl、lshr、lushr。 按位或指令：ior、lor。 按位与指令：iand、land。 按位异或指令：ixor、lxor。 局部变量自增指令：iinc。 比较指令：dcmpg、dcmpl、fcmpg、fcmpl、lcmp。 类型转换指令： 类型转换指令可以将两种不同的数值类型相互转换，这些转换操作将一般用于实现用户代码总的显式类型转换操作，或者用来处理字节码指令中数据类型相关指令无法与数据类型一一对应的问题。Java虚拟机直接支持（即转换时无需显式的转换指令）以下数值类型的宽化类型转换（即小范围类型向大范围类型的安全转换）。 int类型到long、float或者double类型。 long类型到float、double类型。 float类型到double类型。 相对的，处理窄化类型转换时，必须显式地使用转换指令完成，这些指令包括：i2b、i2c、i2s、l2i、f2i、f2l、d2i、d2l和d2f。窄化类型转换可能导致转换结果产生不同的正负号，不同的数量级的情况，转换过程很可能导致数值精度的丢失。 对象创建和访问指令 虽然类实例和数组都是对象，但Java虚拟机对类实例和数组的创建与操作使用了不同的字节码指令。对象创建后就可以通过对象访问指令获取对象实例或者数组实例中的字段或者数组元素，这些指令如下： 创建类实例的指令：new。 创建数组的指令：newarray、anewarray、multianewarray。 访问类字段（static字段）和实例字段（非static字段）的指令：getfield、putfiled、getstatic、putstatic。 把一个数组元素加载到操作数栈的指令：baload、caload、saload、iaload、laload、faload、daload、aaload。 将一个操作数栈的值存储到数组元素中的指令：bastore、castore、sastore、iastore、fastore、dastore、aastore。 取数组长度的指令：arraylength。 检查类实例类型的指令：instanceof、checkcast。 操作数栈管理指令 将操作数栈的栈顶一个或两个元素出栈：pop、pop2。 复制栈顶一个或两个数值并将复制值或双份的复制值重新压入栈顶：dup、dup2、dup_x1、dup2_x1、dup_x2、dup2_x2。 将栈最顶端的两个数值互换：swap。 控制转移指令 控制转移指令可以让Java虚拟机有条件或无条件地从指定的位置指令而不是控制转移指令的下一条指令继续执行程序，从概念上理解，可以认为控制转移指令就是在有条件或无条件的修改PC寄存器的值。 条件分支：ifeq、iflt、ifle、ifne、ifgt、ifnull、ifnonnull、if_icmpeq、if_icmpne、if_icmplt、if_icmpgt、if_icmple、if_icmpge、if_acmpeq和if_acmpne。 复合条件分支：tableswitch、lookupswitch。 无条件分支：goto、goto_w、jsr、jsr_w、ret。 方法调用和返回指令 invokevirtual指令用于调用对象的实例方法，根据对象的实例类型进行分派（虚方法分派），这也是Java语言中最常见的方法分派方式。 invokeinterface指令用于调用接口方法，他会在运行时搜索一个实现了这个接口方法的对象，找出合适的方法进行调用。 invokespecial指令用于调用一些需要特殊处理的方法，包括实例初始化方法，私有方法和父类方法。 invokestatic指令用于调用类方法（static方法）。 invokedynamic指令用于在运行时动态解析出调用点限定符所引用的方法并调用该方法。 异常处理指令： 在Java程序中显示抛出异常的操作（throw语句）都由athrow指令来实现，除了用throw语句显式抛出异常的情况之外，Java虚拟机规范还规定了许多运行时异常会在其他Java虚拟机指令监测到异常状况时自动抛出。 同步指令 Java虚拟机可以支持方法级的同步和方法内部一段指令序列的同步，这两种同步的结构都是使用管程（Monitor）来支持的。","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://wuzguo.com/blog/tags/JVM/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://wuzguo.com/blog/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"执行引擎","slug":"执行引擎","permalink":"http://wuzguo.com/blog/tags/%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E/"}],"author":"Zak"},{"title":"JVM执行引擎（二）","slug":"server/jvm_execution_engine_2","date":"2017-06-23T12:00:25.000Z","updated":"2022-08-01T06:35:23.829Z","comments":true,"path":"2017/06/23/server/jvm_execution_engine_2.html","link":"","permalink":"http://wuzguo.com/blog/2017/06/23/server/jvm_execution_engine_2.html","excerpt":"","text":"三、基于栈的字节码指令集执行过程运行时栈帧结构栈帧是用于支持虚拟机进行方法调用和方法执行的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素。栈帧存储了方法的局部变量表，操作数栈、动态连接和方法返回地址等信息。每一个方法从调用到开始至执行完成的过程都对应着一个栈帧在虚拟机里面从入栈到出栈的过程。每一个栈帧都包括了局部变量表、操作数栈、动态连接、方法返回地址和一些额外的附加信息。 局部变量表 局部变量表（Local Variable Table）是一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量。在Java程序编译为Class文件时，就在方法的Code属性的max_locals数据项中确定了该方法所需要分配的局部变量表的最大容量。局部变量表的容量以变量槽（Variable Slot）为最小单位，一个槽可以存放一个32位以内的数据类型（boolean、byte、char、short、int、float、reference或returnAddress类型）。 在执行方法时，虚拟机是使用局部变量表完成参数值到参数变量列表的传递过程的，如果执行的是实例方法（非static的方法），那么局部变量的表中第0位索引的Slot默认是用于传递方法所属对象实例的引用，在方法中可以通过关键字“this”来访问到这个隐含的参数，其余参数安装参数表的顺序排列，占用从1开始的局部变量Slot，参数表分配完毕后，再根据方法体内部定义的变量顺序和作用域分配其余的Slot。 ​ ​ (上图调用的是实例方法，下图调用的是静态方法) 操作数栈 操作数栈（Operand Stack）也叫操作栈，它是一个后入先出的栈（Last In First Out LIFO）。同局部变量表一样，操作数栈的最大深度也在编译的时候写入到了Codes属性的max_stacks数据项中，操作数栈的每一个元素可以是任意Java数据类型，包括long和double。32位数据类型所占的栈容量为1,64位数据类型所占的栈容量为2。在执行方法的时候，操作数栈的深度都不会超过在max_stacks数据项设定的最大值。 动态连接 每一个栈帧都包含一个指向运行时常量池中该栈帧所属的引用，持有这个引用是为了支持方法调用过程中的动态连接（Dynamic Linking）。Class文件的常量池中存在大量的符号引用，字节码中的方法调用指令就以常量池中指向方法的符号引用作为参数，这些符号引用一部分会在类加载阶段或者第一次使用的时候就转换为直接引用，这种转化称为静态解析，另一部分将在每一次运行期间转化为直接引用，这部分称为动态连接。 方法返回地址 当方法开始执行后，只有两种方式可以退出这个方法： 执行引擎遇到任意一个方法返回的字节码指令，这个时候可能有返回值传递给上层调用者。是否有返回值和返回值类型将根据遇到何种方法返回指令来决定，这种退出的方式称为正常完成出口。 在方法执行过程中遇到了异常，并且这种异常没有在方法体内得到处理就会导致方法退出，这退出方式称为异常完成出口。 无论采用何种退出方式，在方法退出之后，都需要返回到方法被调用的位置，程序才能继续执行。方法退出的过程实际上就等同于吧当前栈帧出栈，因此退出时可能执行的操作有：恢复上层方法的局部变量表和操作数栈，把返回值（如果有的话）压入调用者栈帧的操作数栈，调用PC计数器的值以指向方法调用指令后面的指令等。 附加信息 虚拟机规范允许具体的虚拟机实现增加一个规范里没有的描述的信息到栈帧之中，例如与调试相关的信息，这部分完全取决于具体的虚拟机实现。 基于栈的解释器执行过程 Java编译器输出的指令流，基本上是一种基于栈的指令集架构（Instruction Set Architecture, ISA）,指令流中的指令大部分都是零地址指令，它们依赖操作数栈进行工作。与之相对的另一套常用的指令集架构是基于寄存器的指令集，最典型的就是x86的二地址指令集（现在主流PC机中直接支持的指令集）。 以下举例演示基于栈的解释器执行过程： public class CallFunc &#123; public static void main(String[] args) &#123; int a = 10; int b = 20; int c = 30; int d = calc(a, b, c); System.out.println(&quot;d: &quot; + d); &#125; public static int calc(int a, int b, int c) &#123; return a + b - c; &#125;&#125; 通过 javap -verbose CallFunc 命令输出Class文件字节码指令： ......public static void main(java.lang.String[]);descriptor: ([Ljava/lang/String;)Vflags: ACC_PUBLIC, ACC_STATICCode: stack=3, locals=5, args_size=1 0: bipush 10 2: istore_1 3: bipush 20 5: istore_2 6: bipush 30 8: istore_3 9: iload_1 10: iload_2 11: iload_3 12: invokestatic #2 // Method calc:(III)I 15: istore 4 17: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 20: new #4 // class java/lang/StringBuilder 23: dup // 复制栈顶数值，并且复制值进栈 24: invokespecial #5 // Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V 27: ldc #6 // String d: // 将int、float或String型常量值从常量池中推送至栈顶 29: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 32: iload 4 34: invokevirtual #8 // Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; 37: invokevirtual #9 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 40: invokevirtual #10 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 43: return LineNumberTable: line 24: 0 line 25: 3 line 26: 6 line 27: 9 line 28: 17 line 29: 43 LocalVariableTable: Start Length Slot Name Signature 0 44 0 args [Ljava/lang/String; 3 41 1 a I 6 38 2 b I 9 35 3 c I 17 27 4 d I public static int calc(int, int, int);descriptor: (III)Iflags: ACC_PUBLIC, ACC_STATICCode: stack=2, locals=3, args_size=3 0: iload_0 1: iload_1 2: iadd 3: iload_2 4: isub 5: ireturn LineNumberTable: line 32: 0 LocalVariableTable: Start Length Slot Name Signature 0 6 0 a I 0 6 1 b I 0 6 2 c I ...... 根据字节码可以看出，main方法需要深度为3的操作数栈（Stack&#x3D;3）和5个Slot的局部变量空间（Locals&#x3D;5）。下面使用图片来描述上面的字节码代码执行过程中的代码、操作数栈和局部变量表的变化情况。 执行偏移地址为0的指令的情况 上图展示了执行偏移地址为0的指令的情况，bipush指令的作用是将单字节的整型常量值（-128~127）推入操作数栈顶，后跟一个参数，指明推送的常量值，这里是10。 执行偏移地址为1的指令的情况 上图则是执行偏移地址为1的指令，istore_1指令的作用是将操作数栈顶的整型值出栈并存放到第1个局部变量Slot中。后面四条指令（3、5、6、8）都是做同样的事情，也就是在对应代码中把变量a、b、c赋值为10、20、30。后面四条指令的图就不重复画了。执行偏移地址为9的指令的情况 执行完偏移地址为9、10、11的指令后 执行偏移地址为12的指令 invokestatic #2 其中 #2 = // com/github/wuzguo/CallFunc.calc:(III)I，所以是调用static的calc方法。前面已经说过每次方法调用都会有新的栈帧产生，所以线程的栈帧结构如下： 根据字节码可以看出，calc方法需要深度为2的操作数栈（Stack&#x3D;2）和3个Slot的局部变量空间（Locals&#x3D;3）。下面使用图片来描述上面的字节码代码执行过程中的代码、操作数栈和局部变量表的变化情况。 执行偏移地址为0和1的指令将局部变量表对应index的值载入操作数栈中，然后执行偏移地址为2的指令，将相加的结果再次存入操作数栈中，操作数栈和局部变量表的变化情况如下： 然后执行偏移地址为3、4的指令，将局部变量表中index==2的值载入操作数栈然后做减法操作，结束后栈帧的变化情况如下： 接着执行 5: ireturn指令，退出当前栈帧，栈帧被弹出当前线程栈，然后接着执行main方法的偏移 地址为15的指令，将calc方法运算结果保存到main方法栈帧的index==4的局部变量表，mian方法栈帧的变化情况如下： 然后按类似的方式执行main方法栈帧中偏移地址17至43的指令，将运算结果打印至控制台，然后退出程序。 四、基于寄存器的指令集执行过程寄存器CPU中临时存数据的结构。一个IA32的CPU包含一组8个存储32位值得寄存器，这些寄存器用来存储整数数据和指针。在多数情况下前6个寄存器可以看成通用寄存器，对他们的使用没有限制。最后两个寄存器（%ebp和%esp）保存着指向程序栈中重要位置的指针。只有根据栈管理的标准惯例才能修改这两个寄存器的值。 以下以简单演示的C语言函数调用过程来演示基于寄存器的指令执行过程 #include &lt;stdio.h&gt;int calc( int *xp, int *yp) &#123; int x = *xp; int y = *yp; return x+y;&#125;int main(int argc, char *argv[])&#123; int x = 10; int y = 20; int sum = calc(&amp;x, &amp;y); printf(&quot;sum is %d\\n&quot;, sum); return sum;&#125; 通过GCC运行编译器 gcc -m32 -O0 -S hello.c 产生汇编代码（AT&amp;T）（汇编代码非常接近于机器代码，并非机器代码），如下： .file &quot;hello.c&quot; .text .globl _calc .def _calc; .scl 2; .type 32; .endef_calc:0 pushl %ebp1 movl %esp, %ebp2 subl $16, %esp3 movl 8(%ebp), %eax4 movl (%eax), %eax5 movl %eax, -4(%ebp)6 movl 12(%ebp), %eax7 movl (%eax), %eax8 movl %eax, -8(%ebp)9 movl -4(%ebp), %edx10 movl -8(%ebp), %eax11 addl %edx, %eax12 leave13 ret .def ___main; .scl 2; .type 32; .endef .section .rdata,&quot;dr&quot;LC0: .ascii &quot;sum is %d\\12\\0&quot; .text .globl _main .def _main; .scl 2; .type 32; .endef_main:0 pushl %ebp1 movl %esp, %ebp2 andl $-16, %esp//按位&amp;,内存地址对齐(0xfffffff0)3 subl $32, %esp//x86编程指导方针确定任何函数所占用栈空间为16字节的整数倍4 call ___main//GNU标准库的子程序，作用是初始化gcc所需的内容5 movl $10, 24(%esp)6 movl $20, 20(%esp)7 leal 20(%esp), %eax8 movl %eax, 4(%esp)9 leal 24(%esp), %eax10 movl %eax, (%esp)11 call _calc12 movl %eax, 28(%esp)13 movl 28(%esp), %eax14 movl %eax, 4(%esp)15 movl $LC0, (%esp)16 call _printf17 movl 28(%esp), %eax18 leave19 ret .ident &quot;GCC: (GNU) 7.1.0&quot; .def _printf; .scl 2; .type 32; .endef 图解CPU执行过程 初始状态假设ebp寄存器执行内存地址的512处，esp寄存器指向内存地址的544处。 之所以将ebp压栈是为了保存上一个栈帧的起始位置，当执行完序号（0、1）的指令后，栈中的结构如下： 然后序号（2、3、4）操作对其内存，并为当前栈帧分配32个字节的空间，操作结束后栈结构如下： 序号为（5、6）的指令将立即数10和20压入寄存器esp指向的位置偏移24字节和20字节的地方，leal指令是指将寄存器的值加上立即数然后存放到对应的寄存器，如： leal 20(%esp), %eax 是将寄存器esp指向的内存地址加上20然后取值，然后存放在寄存器eax中，所以执行完序号（5、6、7、8、9、10）后栈的结构如下： call指令的效果是将返回地址入栈（push），并跳转到被调用函数的起始处。返回地址是在程序中紧跟在call后面的那条指令的地址，这样当被调用函数返回时，执行会从此处继续，ret指令从栈中弹出（pop）地址，并跳转到这个位置。所以 call _calc 就是调用calc函数并将函数地址入栈，执行完成后结果如下： 然后是执行calc函数序号为（0、1、2）指令将上一个函数（mian）的栈帧开始位置入栈并分配内存地址，操作完成后对应的栈帧结构如下： 序号为（3、4、5、6、7、8）的操作就是通过栈帧中存放的内存地址，获取变量10和20。 操作movl 8(%ebp), %eax 表示将ebp指向的内存地址加8然后取值（484）然后放入eax寄存器。 操作movl (%eax), %eax 表示将eax指向的内存地址的值（20）然后放入eax寄存器。操作完成后结构如下： 操作（9、10、11）将10和20相加，并将结果存放在eax中。然后执行leave（为返回操作准备栈）指令和ret（从调用函数返回）指令。然后返回到main方法序号为12的指令，此时esp寄存器地址还原指向464，结构如下： 后续操作类似，不再详述。","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://wuzguo.com/blog/tags/JVM/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://wuzguo.com/blog/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"执行引擎","slug":"执行引擎","permalink":"http://wuzguo.com/blog/tags/%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E/"}],"author":"Zak"},{"title":"JVM类加载器及双亲委派模型","slug":"server/jvm_loader_delegation_mechanism","date":"2017-05-16T13:23:25.000Z","updated":"2022-08-01T06:35:23.655Z","comments":true,"path":"2017/05/16/server/jvm_loader_delegation_mechanism.html","link":"","permalink":"http://wuzguo.com/blog/2017/05/16/server/jvm_loader_delegation_mechanism.html","excerpt":"","text":"一、JVM的构成Java 虚拟机（Java virtual machine，JVM）是运行 Java 程序必不可少的机制，它实现Java语言最重要的特征：平台无关性，即编译后的 Java 程序指令并不直接在硬件系统的 CPU 上执行，而是由 JVM 执行。 JVM屏蔽了与具体平台相关的信息，使Java语言编译程序只需要生成在JVM上运行的目标字节码（.class）文件，就可以在多种平台上不加修改地运行。（一次编译，到处运行） classloader 把硬盘上的class 文件加载到JVM中的运行时数据区域,，但是它不负责这个类文件能否执行，而这个是执行引擎负责的。执行引擎在执行字节码时，把字节码解释成具体平台上的机器指令执行。 下图是JAVA虚拟机的结构图，每个Java虚拟机都有一个类装载子系统，它根据给定的全限定名来装入类型（类或接口）。同样，每个Java虚拟机都有一个执行引擎，它负责执行那些包含在被装载类的方法中的指令。 &#x3D;&#x3D;Java虚拟机&#x3D; 类加载器（classloader） + 执行引擎（execution engine） + 运行时数据区域 （runtime data area）&#x3D;&#x3D; 二、什么是ClassLoader大家都知道，当我们写好一个Java程序之后，不是管是CS还是BS应用，都是由若干个.class文件组织而成的一个完整的Java应用程序，当程序在运行时，即会调用该程序的一个入口函数来调用系统的相关功能，而这些功能都被封装在不同的class文件当中，所以经常要从这个class文件中要调用另外一个class文件中的方法，如果另外一个文件不存在的，则会引发系统异常。而程序在启动的时候，并不会一次性加载程序所要用的所有class文件，而是根据程序的需要，通过Java的类加载机制（ClassLoader）来动态加载某个class文件到内存当中的，从而只有class文件被载入到了内存之后，才能被其它class所引用。所以ClassLoader就是用来动态加载class文件到内存当中用的。 类加载器 classloader 是具有层次结构的，也就是父子关系。其中，Bootstrap 是所有类加载器的父亲。如下图所示： public class ClassLoaderTest &#123; public static void main(String[] args) &#123; ClassLoader loader = ClassLoaderTest.class.getClassLoader(); while (loader != null) &#123; System.out.println(loader.getClass().getName()); loader = loader.getParent(); &#125; System.out.println(loader); &#125;&#125; sun.misc.Launcher$AppClassLoadersun.misc.Launcher$ExtClassLoadernull 三、JVM默认提供的三个ClassLoader BootStrap ClassLoader：称为启动类加载器（根类加载器），是Java类加载层次中最顶层的类加载器，负责加载JDK中的核心类库，如：rt.jar、resources.jar、charsets.jar等，可通过如下程序获得该类加载器从哪些地方加载了相关的jar或class文件： public class ClassLoaderTest &#123; public static void main(String[] args) &#123; URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i &lt; urls.length; i++) &#123; System.out.println(urls[i].toExternalForm()); &#125; &#125;&#125; 控制台打印的结果： file:/D:/Java/jdk1.7.0_80/jre/lib/resources.jarfile:/D:/Java/jdk1.7.0_80/jre/lib/rt.jarfile:/D:/Java/jdk1.7.0_80/jre/lib/sunrsasign.jarfile:/D:/Java/jdk1.7.0_80/jre/lib/jsse.jarfile:/D:/Java/jdk1.7.0_80/jre/lib/jce.jarfile:/D:/Java/jdk1.7.0_80/jre/lib/charsets.jarfile:/D:/Java/jdk1.7.0_80/jre/lib/jfr.jarfile:/D:/Java/jdk1.7.0_80/jre/classes 其实上述结果也是通过查找sun.boot.class.path这个系统属性所得知的。 public class ClassLoaderTest &#123; public static void main(String[] args) &#123; // Java的入口程序sun.misc.Launcher中定义 final String strPath = System.getProperty(&quot;sun.boot.class.path&quot;); File[] dirs; if (strPath != null) &#123; StringTokenizer tokenizer = new StringTokenizer(strPath, File.pathSeparator); int count = tokenizer.countTokens(); dirs = new File[count]; for (int i = 0; i &lt; count; i++) &#123; dirs[i] = new File(tokenizer.nextToken()); &#125; &#125; else &#123; dirs = new File[0]; &#125; for (File f : dirs) &#123; System.out.println(f.getAbsolutePath()); &#125; &#125;&#125; Extension ClassLoader：称为扩展类加载器，负责加载Java的扩展类库，默认加载JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext&#x2F;目下的所有jar。 public class ClassLoaderTest &#123; public static void main(String[] args) &#123; // Java的入口程序sun.misc.Launcher中定义 final String strPath = System.getProperty(&quot;java.ext.dirs&quot;); File[] dirs; if (strPath != null) &#123; StringTokenizer tokenizer = new StringTokenizer(strPath, File.pathSeparator); int count = tokenizer.countTokens(); dirs = new File[count]; for (int i = 0; i &lt; count; i++) &#123; dirs[i] = new File(tokenizer.nextToken()); &#125; &#125; else &#123; dirs = new File[0]; &#125; for (File f : dirs) &#123; System.out.println(f.getAbsolutePath()); &#125; &#125;&#125; 控制台打印的结果： D:\\Java\\jdk1.7.0_80\\jre\\lib\\extC:\\WINDOWS\\Sun\\Java\\lib\\ext App ClassLoader：称为系统类加载器，负责加载应用程序classpath目录下的所有jar和class文件。 public class ClassLoaderTest &#123; public static void main(String[] args) &#123; // Java的入口程序sun.misc.Launcher中定义 final String strPath = System.getProperty(&quot;java.class.path&quot;); File[] dirs; if (strPath != null) &#123; StringTokenizer tokenizer = new StringTokenizer(strPath, File.pathSeparator); int count = tokenizer.countTokens(); dirs = new File[count]; for (int i = 0; i &lt; count; i++) &#123; dirs[i] = new File(tokenizer.nextToken()); &#125; &#125; else &#123; dirs = new File[0]; &#125; for (File f : dirs) &#123; System.out.println(f.getAbsolutePath()); &#125; &#125;&#125; 控制台打印的结果： D:\\Java\\jdk1.7.0_80\\jre\\lib\\charsets.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\deploy.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\ext\\access-bridge-64.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\ext\\dnsns.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\ext\\jaccess.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\ext\\localedata.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\ext\\sunec.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\ext\\sunjce_provider.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\ext\\sunmscapi.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\ext\\zipfs.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\javaws.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\jce.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\jfr.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\jfxrt.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\jsse.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\management-agent.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\plugin.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\resources.jarD:\\Java\\jdk1.7.0_80\\jre\\lib\\rt.jar**F:\\Intellij\\classLoaderTest\\target\\classes****D:\\Program Files\\JetBrains\\IntelliJ IDEA 2017.1\\lib\\idea_rt.jar** 注意事项： 除了Java默认提供的三个ClassLoader之外，用户还可以根据需要定义自已的ClassLoader，而这些自定义的ClassLoader都必须继承自java.lang.ClassLoader类。 Extension ClassLoader和App ClassLoader也继承自ClassLoader类。但是Bootstrap ClassLoader不继承自ClassLoader，它不是一个普通的Java类，底层由操作系统的本地语言编写（C&#x2F;C++&#x2F;VB），已嵌入到了JVM内核当中，当JVM启动后，Bootstrap ClassLoader也随着启动，负责加载完核心类库后，并构造Extension ClassLoader和App ClassLoader类加载器。 public class ClassLoaderTest &#123; public static void main(String[] args) &#123; // System类 java.lang.System 位于rt.jar包中 // 打印结果： null System.out.println(System.class.getClassLoader()); &#125;&#125; 自定义类加载器的加载过程也符合双起委派机制，可以通过XXX.class.getClassLoader().getClass().getName()方法获取XXX类的类加载器。 public class ClassLoaderTest &#123; public static void main(String[] args) &#123; System.out.println(ClassLoaderTest.class.getClassLoader().getClass().getName()); &#125;&#125; sun.misc.Launcher$AppClassLoader 四、类加载器的工作机制类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 类使用方式，Java程序对类的使用方式可分为两种： 主动使用 被动使用 所有的Java虚拟机实现必须在每个类或接口被Java程序”首次主动使用”时才初始化他们，主动使用的场景有以下六种： 创建类的实例 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（如 Class.forName(&quot;com.github.wuzguo.Test&quot;)） 初始化一个类的子类 Java虚拟机启动时被标明为启动类的类（Java Test， 或者包含main方法的类） 除了以上六种情况，其他使用Java类的方式都被看作是对类的被动使用，都不会导致类的初始化。 类的加载方式，加载.class文件的方式有以下几种： 从本地系统中直接加载 通过网络下载.class文件 从zip，jar等归档文件中加载.class文件 从专有数据库中提取.class文件 将Java源文件动态编译为.class文件 类的加载流程，类装载器子系统除了要定位和导入二进制class文件外，还必须负责验证被导入类的正确性，为类变量分配并初始化内存，以及帮助解析符号引用。这些动作必须严格按以下顺序进行： 装载：查找并装载类型的二进制数据，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误（LinkageError错误）如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误 连接：指向验证、准备、以及解析（可选）类被加载后，就进入连接阶段。连接就是将已经读入到内存的类的二进制数据合并到虚拟机的运行时环境中去。 验证：确保被导入类型的正确性。类的验证主要包括以下内容： 类文件的结构检查：确保类文件遵从Java类文件的固定格式。 语义检查：确保类本身符合Java语言的语法规定，比如验证final类型的类有没有子类，final类型的方法有没有被覆盖。 字节码验证：确保字节码可以被Java虚拟机安全地执行。字节码流代表Java方法（包含静态方法和实例方法），他是由被称作操作码的单字节指令组成的序列，每一个操作码后都跟着一个或多个操作数。字节码验证步骤会检查每个操作码是否合法，即是否有着合法的操作数。 二进制兼容性的验证：确保相互引用的类之间协调一致，例如：在worker类的gotowork方法中会调用car类的run方法，java虚拟机在验证worker类时，会检查在方法区是否存在car类的run方法，假如不存在（当worker类和car类不兼容，就会出现这种问题），就会抛出NoSuchMethodError错误。 准备：为类变量分配内存，并将其初始化为默认值 解析：把类型中的符号引用转换为直接引用。 初始化：把类变量（成员变量）初始化为正确初始值。 每个JAVA虚拟机实现都必须有一个启动类装载器，它知道怎么装载受信任的类。每个类装载器都有自己的命名空间，其中维护着由它装载的类型。所以一个Java程序可以多次装载具有同一个全限定名的多个类型。这样一个类型的全限定名就不足以确定在一个Java虚拟机中的唯一性。因此，当多个类装载器都装载了同名的类型时，为了惟一地标识该类型，还要在类型名称前加上装载该类型（指出它所位于的命名空间）的类装载器标识。 五、双亲委派模型（Parent Delegation Model）类的加载过程采用双亲委托机制，这种机制能更好的保证 Java 平台的安全。该模型要求除了顶层的Bootstrap class loader启动类加载器外，其余的类加载器都应当有自己的父类加载器。子类加载器和父类加载器不是以继承（Inheritance）的关系来实现，而是通过组合（Composition）关系来复用父加载器的代码。每个类加载器都有自己的命名空间（由该加载器及所有父类加载器所加载的类组成，在同一个命名空间中，不会出现类的完整名字（包括类的包名）相同的两个类；在不同的命名空间中，有可能会出现类的完整名字（包括类的包名）相同的两个类。 双亲委派模型的工作过程为： 前 ClassLoader 首先从自己已经加载的类中查询是否此类已经加载，如果已经加载则直接返回原来已经加载的类。每个类加载器都有自己的加载缓存，当一个类被加载了以后就会放入缓存，等下次加载的时候就可以直接返回了。 当前 classLoader 的缓存中没有找到被加载的类的时候，委托父类加载器去加载，父类加载器采用同样的策略，首先查看自己的缓存，然后委托父类的父类去加载，一直到 bootstrap ClassLoader。 当所有的父类加载器都没有加载的时候，再由当前的类加载器加载，并将其放入它自己的缓存中，以便下次有加载请求的时候直接返回。 如下图所示： ​ loader2首先从自己的命名空间查找Sample类是否已经被加载，如果已经加载就直接返回代表Sample类的class对象的引用。如果Sample类还没有被加载。loader2首先请求loader1代为加载，loader1再请求系统类加载器代为加载，系统类加载器再请求扩展类加载器代为加载，扩展类加载器再请求根类加载器代为加载。若根类加载器和扩展类加载器都不能加载，则系统类加载器尝试加载，若能加载成功，则将Sample类所对应的class对象的引用返回给loader1，loader1再将引用返回给loader2，从而成功将Sample类加载进虚拟机。若系统类加载器不能加载Sample类，则loader1尝试加载sample类，若loader1也不能成功加载，则loader2尝试加载。若所有的父加载器及loader2本身都不能加载，则抛出ClassNotFoundException异常。 以下是ClassLoader抽象类的代码片段： protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 使用这种模型来组织类加载器之间的关系的好处： 避免重复加载，当父加载器已经加载了该类的时候，子类加载器就没有必要子再加载一次。 考虑到安全因素，我们试想一下，如果不使用这种委托模式，那我们就可以随时使用自定义的String来动态替代java核心api中定义的类型，这样会存在非常大的安全隐患，而双亲委托的方式，就可以避免这种情况，因为String已经在启动时就被引导类加载器（Bootstrcp ClassLoader）加载，所以用户自定义的ClassLoader永远也无法加载一个自己写的String，除非你改变JDK中ClassLoader搜索类的默认算法。 六、自定义类加载器运行自定义加载器相关的代码，深入理解双亲委派模型。 请查看GitHub源代码：clazzLoader 七、类的卸载当类被加载、连接和初始化后，它的生命周期就开始了，当代码该类的Class对象不再被引用，Class对象就会结束生命周期，该类在方法区的数据也会被卸载，从而结束该类的生命周期。由java虚拟机自带的加载器加载的类在虚拟机的生命周期总始终不会被卸载，java虚拟机本身会始终引用这些类加载器，而这些类加载器则会始终引用它们所加载的类的Class对象，因此这些Class对象始终是可触及的。由用户自定义的类加载器所加载的类是可以被卸载的。","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://wuzguo.com/blog/tags/JVM/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://wuzguo.com/blog/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"author":"Zak"},{"title":"构建MongoDB HA集群：副本集和分片","slug":"db/mongodb_slice","date":"2017-02-16T13:10:24.000Z","updated":"2022-08-01T06:35:23.736Z","comments":true,"path":"2017/02/16/db/mongodb_slice.html","link":"","permalink":"http://wuzguo.com/blog/2017/02/16/db/mongodb_slice.html","excerpt":"","text":"MongoDB复制是将数据同步在多个服务器的过程。 复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。 复制还允许您从硬件故障和服务中断中恢复数据。 复制原理mongodb的复制至少需要两个节点。其中一个是主节点，负责处理客户端请求，其余的都是从节点，负责复制主节点上的数据。 mongodb各个节点常见的搭配方式为：一主一从、一主多从。 主节点记录在其上的所有操作oplog，从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据与主节点一致。 MongoDB复制结构图如下所示： 以上结构图总，客户端总主节点读取数据，在客户端写入数据到主节点是， 主节点与从节点进行数据交互保障数据的一致性。 副本集特征： N 个节点的集群 任何节点可作为主节点 所有写入操作都在主节点上 自动故障转移 自动恢复 副本集设置在本教程中我们使用同一个MongoDB来做MongoDB主从的实验， 操作步骤如下： 1、关闭正在运行的MongoDB服务器。 现在我们通过指定 –replSet 选项来启动mongoDB。–replSet 基本语法格式如下： mongod --port &quot;PORT&quot; --dbpath &quot;YOUR_DB_DATA_PATH&quot; --replSet &quot;REPLICA_SET_INSTANCE_NAME&quot; 实例mongod --port 27017 --dbpath &quot;D:\\set up\\mongodb\\data&quot; --replSet rs0 以上实例会启动一个名为rs0的MongoDB实例，其端口号为27017。 启动后打开命令提示框并连接上mongoDB服务。 在Mongo客户端使用命令rs.initiate()来启动一个新的副本集。 我们可以使用rs.conf()来查看副本集的配置 查看副本集状态使用 rs.status() 命令 副本集添加成员添加副本集的成员，我们需要使用多条服务器来启动mongo服务。进入Mongo客户端，并使用rs.add()方法来添加副本集的成员。 语法rs.add() 命令基本语法格式如下： &gt;rs.add(HOST_NAME:PORT) 实例假设你已经启动了一个名为mongod1.net，端口号为27017的Mongo服务。 在客户端命令窗口使用rs.add() 命令将其添加到副本集中，命令如下所示： &gt;rs.add(&quot;mongod1.net:27017&quot;)&gt; MongoDB中你只能通过主节点将Mongo服务添加到副本集中， 判断当前运行的Mongo服务是否为主节点可以使用命令db.isMaster() 。 MongoDB的副本集与我们常见的主从有所不同，主从在主机宕机后所有服务将停止，而副本集在主机宕机后，副本会接管主节点成为主节点，不会出现宕机的情况。 分片原理在Mongodb里面存在另一种集群，就是分片技术,可以满足MongoDB数据量大量增长的需求。 当MongoDB存储海量的数据时，一台机器可能不足以存储数据，也可能不足以提供可接受的读写吞吐量。这时，我们就可以通过在多台机器上分割数据，使得数据库系统能存储和处理更多的数据。 下图展示了在MongoDB中使用分片集群结构分布： 上图中主要有如下所述三个主要组件： **Shard:**用于存储实际的数据块，实际生产环境中一个shard server角色可由几台机器组个一个replica set承担，防止主机单点故障 **Config Server:**mongod实例，存储了整个 ClusterMetadata，其中包括 chunk信息。 **Query Routers:**前端路由，客户端由此接入，且让整个集群看上去像单一数据库，前端应用可以透明使用。 分片实例分片结构端口分布如下： Shard Server 1：10001Shard Server 2：10002Shard Server 3：10003Config Server ：11000Route Process：10000 步骤一：启动Shard Server [root@100 /]# mkdir -p /www/mongoDB/shard/s0[root@100 /]# mkdir -p /www/mongoDB/shard/s1[root@100 /]# mkdir -p /www/mongoDB/shard/s2[root@100 /]# mkdir -p /www/mongoDB/shard/s3[root@100 /]# mkdir -p /www/mongoDB/shard/log[root@100 /]# /usr/local/mongoDB/bin/mongod --port 10001 --dbpath=/www/mongoDB/shard/s0 --logpath=/www/mongoDB/shard/log/s0.log --logappend --fork....[root@100 /]# /usr/local/mongoDB/bin/mongod --port 10003 --dbpath=/www/mongoDB/shard/s2 --logpath=/www/mongoDB/shard/log/s3.log --logappend --fork 步骤二： 启动Config Server [root@100 /]# mkdir -p /www/mongoDB/shard/config[root@100 /]# /usr/local/mongoDB/bin/mongod --port 11000 --dbpath=/www/mongoDB/shard/config --logpath=/www/mongoDB/shard/log/config.log --logappend --fork 注意：这里我们完全可以像启动普通mongodb服务一样启动，不需要添加—shardsvr和configsvr参数。因为这两个参数的作用就是改变启动端口的，所以我们自行指定了端口就可以。 步骤三： 启动Route Process /usr/local/mongoDB/bin/mongos --port 10000 --configdb localhost:11000 --fork --logpath=/www/mongoDB/shard/log/route.log --chunkSize 500 mongos启动参数中，chunkSize这一项是用来指定chunk的大小的，单位是MB，默认大小为200MB. 步骤四： 配置Sharding 接下来，我们使用MongoDB Shell登录到mongos，添加Shard节点 [root@100 shard]# /usr/local/mongoDB/bin/mongo admin --port 40000MongoDB shell version: 2.0.7connecting to: 127.0.0.1:10000/adminmongos&gt; db.runCommand(&#123; addshard:&quot;localhost:10001&quot; &#125;)&#123; &quot;shardAdded&quot; : &quot;shard0000&quot;, &quot;ok&quot; : 1 &#125;......mongos&gt; db.runCommand(&#123; addshard:&quot;localhost:10003&quot; &#125;)&#123; &quot;shardAdded&quot; : &quot;shard0002&quot;, &quot;ok&quot; : 1 &#125;mongos&gt; db.runCommand(&#123; enablesharding:&quot;test&quot; &#125;) #设置分片存储的数据库&#123; &quot;ok&quot; : 1 &#125;mongos&gt; db.runCommand(&#123; shardcollection: &quot;test.log&quot;, key: &#123; id:1,time:1&#125;&#125;)&#123; &quot;collectionsharded&quot; : &quot;test.log&quot;, &quot;ok&quot; : 1 &#125; 步骤五： 程序代码内无需太大更改，直接按照连接普通的mongo数据库那样，将数据库连接接入接口10000 ##分片和副本集结合 以下文档引用至： 首先确定各个组件的数量，mongos 3个， config server 3个，数据分3片 shard server 3个，每个shard 有一个副本一个仲裁也就是 3 * 2 &#x3D; 6 个，总共需要部署15个实例。这些实例可以部署在独立机器也可以部署在一台机器，我们这里测试资源有限，只准备了 3台机器，在同一台机器只要端口不同就可以，看一下物理部署图： 架构搭好了，安装软件。 1、准备机器，IP分别设置为： 192.168.0.136、192.168.0.137、192.168.0.138。 2、分别在每台机器上建立mongodb分片对应测试文件夹。 #存放mongodb数据文件mkdir -p /data/mongodbtest#进入mongodb文件夹cd /data/mongodbtest 3、下载mongodb的安装程序包 wget http://fastdl.mongodb.org/linux/mongodb-linux-x86_64-2.4.8.tgz#解压下载的压缩包tar xvzf mongodb-linux-x86_64-2.4.8.tgz 4、分别在每台机器建立mongos 、config 、 shard1 、shard2、shard3 五个目录。因为mongos不存储数据，只需要建立日志文件目录即可。 #建立mongos目录mkdir -p /data/mongodbtest/mongos/log#建立config server 数据文件存放目录mkdir -p /data/mongodbtest/config/data#建立config server 日志文件存放目录mkdir -p /data/mongodbtest/config/log#建立config server 日志文件存放目录mkdir -p /data/mongodbtest/mongos/log#建立shard1 数据文件存放目录mkdir -p /data/mongodbtest/shard1/data#建立shard1 日志文件存放目录mkdir -p /data/mongodbtest/shard1/log#建立shard2 数据文件存放目录mkdir -p /data/mongodbtest/shard2/data#建立shard2 日志文件存放目录mkdir -p /data/mongodbtest/shard2/log#建立shard3 数据文件存放目录mkdir -p /data/mongodbtest/shard3/data#建立shard3 日志文件存放目录mkdir -p /data/mongodbtest/shard3/log 5、规划5个组件对应的端口号，由于一个机器需要同时部署 mongos、config server 、shard1、shard2、shard3，所以需要用端口进行区分。这个端口可以自由定义，在本文 mongos为 20000， config server 为 21000， shard1为 22001 ， shard2为22002， shard3为22003. 6、在每一台服务器分别启动配置服务器。 /data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongod --configsvr --dbpath /data/mongodbtest/config/data --port 21000 --logpath /data/mongodbtest/config/log/config.log --fork 7、在每一台服务器分别启动mongos服务器。 /data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongos --configdb 192.168.0.136:21000,192.168.0.137:21000,192.168.0.138:21000 --port 20000 --logpath /data/mongodbtest/mongos/log/mongos.log --fork ！！！注意，报错： BadValue: configdb supports only replica set connection string 查看MongoDB的帮助发现在3.2版本以后，configdb的方式已经变成了： configReplSet/&lt;cfgsvr1:port1&gt;,&lt;cfgsvr2:port2&gt;,&lt;cfgsvr3:port3&gt; 解决方案： 登录configdb 在mongo中执行： rs.initiate( &#123;_id: &quot;configReplSet&quot;, configsvr: true, members:[ &#123; _id: 0, host: &quot;192.168.190.136:21000&quot;&#125;, &#123; _id: 1, host: &quot;192.168.190.137:21000&quot;&#125;, &#123; _id: 2, host: &quot;192.168.190.138:21000&quot;&#125; ] &#125; ) 8、配置各个分片的副本集。 #在每个机器里分别设置分片1服务器及副本集shard1/data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongod --shardsvr --replSet shard1 --port 22001 --dbpath /data/mongodbtest/shard1/data --logpath /data/mongodbtest/shard1/log/shard1.log --fork --nojournal --oplogSize 10 为了快速启动并节约测试环境存储空间，这里加上 nojournal 是为了关闭日志信息，在我们的测试环境不需要初始化这么大的redo日志。同样设置 oplogsize是为了降低 local 文件的大小，oplog是一个固定长度的 capped collection,它存在于”local”数据库中,用于记录Replica Sets操作日志。注意，这里的设置是为了测试！ #在每个机器里分别设置分片2服务器及副本集shard2/data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongod --shardsvr --replSet shard2 --port 22002 --dbpath /data/mongodbtest/shard2/data --logpath /data/mongodbtest/shard2/log/shard2.log --fork --nojournal --oplogSize 10#在每个机器里分别设置分片3服务器及副本集shard3/data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongod --shardsvr --replSet shard3 --port 22003 --dbpath /data/mongodbtest/shard3/data --logpath /data/mongodbtest/shard3/log/shard3.log --fork --nojournal --oplogSize 10 分别对每个分片配置副本集，深入了解副本集参考本系列前几篇文章。 任意登陆一个机器，比如登陆192.168.0.136，连接mongodb #设置第一个分片副本集/data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongo 127.0.0.1:22001#使用admin数据库use admin#定义副本集配置config = &#123; _id:&quot;shard1&quot;, members:[ &#123;_id:0,host:&quot;192.168.0.136:22001&quot;&#125;, &#123;_id:1,host:&quot;192.168.0.137:22001&quot;&#125;, &#123;_id:2,host:&quot;192.168.0.138:22001&quot;,arbiterOnly:true&#125; ] &#125;#初始化副本集配置rs.initiate(config);#设置第二个分片副本集/data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongo 127.0.0.1:22002#使用admin数据库use admin#定义副本集配置config = &#123; _id:&quot;shard2&quot;, members:[ &#123;_id:0,host:&quot;192.168.0.136:22002&quot;&#125;, &#123;_id:1,host:&quot;192.168.0.137:22002&quot;&#125;, &#123;_id:2,host:&quot;192.168.0.138:22002&quot;,arbiterOnly:true&#125; ] &#125;#初始化副本集配置rs.initiate(config);#设置第三个分片副本集/data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongo 127.0.0.1:22003#使用admin数据库use admin#定义副本集配置config = &#123;_id:&quot;shard3&quot;, members:[ &#123;_id:0,host:&quot;192.168.0.136:22003&quot;&#125;, &#123;_id:1,host:&quot;192.168.0.137:22003&quot;&#125;, &#123;_id:2,host:&quot;192.168.0.138:22003&quot;,arbiterOnly:true&#125; ] &#125;#初始化副本集配置rs.initiate(config); 9、目前搭建了mongodb配置服务器、路由服务器，各个分片服务器，不过应用程序连接到 mongos 路由服务器并不能使用分片机制，还需要在程序里设置分片配置，让分片生效。 #连接到mongos/data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongo 127.0.0.1:20000#使用admin数据库user admin#串联路由服务器与分配副本集1db.runCommand( &#123; addshard : &quot;shard1/192.168.0.136:22001,192.168.0.137:22001,192.168.0.138:22001&quot;&#125;); 如里shard是单台服务器，用 db.runCommand( &#123; addshard : “[: ]” &#125; ) 这样的命令加入，如果shard是副本集，用 db.runCommand( &#123; addshard : “replicaSetName/[:port[,serverhostname2[:port],…]” &#125;); 这样的格式表示 。 #串联路由服务器与分配副本集2db.runCommand( &#123; addshard : &quot;shard2/192.168.0.136:22002,192.168.0.137:22002,192.168.0.138:22002&quot;&#125;);#串联路由服务器与分配副本集3db.runCommand( &#123; addshard : &quot;shard3/192.168.0.136:22003,192.168.0.137:22003,192.168.0.138:22003&quot;&#125;);#查看分片服务器的配置db.runCommand( &#123; listshards : 1 &#125; );#内容输出&#123; &quot;shards&quot; : [ &#123; &quot;_id&quot; : &quot;shard1&quot;, &quot;host&quot; : &quot;shard1/192.168.0.136:22001,192.168.0.137:22001&quot; &#125;, &#123; &quot;_id&quot; : &quot;shard2&quot;, &quot;host&quot; : &quot;shard2/192.168.0.136:22002,192.168.0.137:22002&quot; &#125;, &#123; &quot;_id&quot; : &quot;shard3&quot;, &quot;host&quot; : &quot;shard3/192.168.0.136:22003,192.168.0.137:22003&quot; &#125; ], &quot;ok&quot; : 1&#125; 因为192.168.0.138是每个分片副本集的仲裁节点，所以在上面结果没有列出来。10、目前配置服务、路由服务、分片服务、副本集服务都已经串联起来了，但我们的目的是希望插入数据，数据能够自动分片，就差那么一点点，一点点。。。连接在mongos上，准备让指定的数据库、指定的集合分片生效。 #指定testdb分片生效db.runCommand( &#123; enablesharding :&quot;testdb&quot;&#125;);#指定数据库里需要分片的集合和片键db.runCommand( &#123; shardcollection : &quot;testdb.table1&quot;,key : &#123;id: 1&#125; &#125; ) 我们设置testdb的 table1 表需要分片，根据 id 自动分片到 shard1 ，shard2，shard3 上面去。要这样设置是因为不是所有mongodb 的数据库和表 都需要分片！11、测试分片配置结果。 #连接mongos服务器/data/mongodbtest/mongodb-linux-x86_64-2.4.8/bin/mongo 127.0.0.1:20000#使用testdbuse testdb; #插入测试数据public class MongoShard &#123; public static void main(String args[]) &#123; try &#123; List&lt;ServerAddress&gt; addresses = new ArrayList&lt;ServerAddress&gt;(); ServerAddress address1 = new ServerAddress(&quot;localhost&quot; , 10000); ServerAddress address2 = new ServerAddress(&quot;localhost&quot; , 20000); addresses.add(address1); addresses.add(address2); // 连接到 mongodb 服务 MongoClient mongoClient = new MongoClient(addresses); // 连接到数据库 MongoDatabase mongoDatabase = mongoClient.getDatabase(&quot;testdb&quot;); System.out.println(&quot;Connect to database successfully&quot;); MongoCollection&lt;Document&gt; collection = mongoDatabase.getCollection(&quot;test&quot;); System.out.println(&quot;集合 test 选择成功&quot;); for (int num = 0; num &lt; 100000; num++) &#123; Document document = new Document(&quot;title&quot;, &quot;MYsql&quot;). append(&quot;description&quot;, &quot;database&quot;). append(&quot;likes&quot;, Math.random()). append(&quot;by&quot;, &quot;xxx&quot;). append(&quot;id&quot;, num); collection.insertOne(document); &#125; System.out.println(&quot;#################################&quot;); &#125; catch (Exception e) &#123; System.err.println(e.getClass().getName() + &quot;: &quot; + e.getMessage()); &#125; System.out.println(&quot;+++++++++++++++++++++++++++++++&quot;); &#125;&#125; #查看分片情况如下，部分无关信息省掉了db.table1.stats();&#123; &quot;sharded&quot; : true, &quot;ns&quot; : &quot;testdb.table1&quot;, &quot;count&quot; : 100000, &quot;numExtents&quot; : 13, &quot;size&quot; : 5600000, &quot;storageSize&quot; : 22372352, &quot;totalIndexSize&quot; : 6213760, &quot;indexSizes&quot; : &#123; &quot;_id_&quot; : 3335808, &quot;id_1&quot; : 2877952 &#125;, &quot;avgObjSize&quot; : 56, &quot;nindexes&quot; : 2, &quot;nchunks&quot; : 3, &quot;shards&quot; : &#123; &quot;shard1&quot; : &#123; &quot;ns&quot; : &quot;testdb.table1&quot;, &quot;count&quot; : 42183, &quot;size&quot; : 0, ... &quot;ok&quot; : 1 &#125;, &quot;shard2&quot; : &#123; &quot;ns&quot; : &quot;testdb.table1&quot;, &quot;count&quot; : 38937, &quot;size&quot; : 2180472, ... &quot;ok&quot; : 1 &#125;, &quot;shard3&quot; : &#123; &quot;ns&quot; : &quot;testdb.table1&quot;, &quot;count&quot; :18880, &quot;size&quot; : 3419528, ... &quot;ok&quot; : 1 &#125; &#125;, &quot;ok&quot; : 1&#125; 可以看到数据分到3个分片，各自分片数量为： shard1 “count” : 42183，shard2 “count” : 38937，shard3 “count” : 18880。已经成功了！不过分的好像不是很均匀，所以这个分片还是很有讲究的，后续再深入讨论。12、java程序调用分片集群，因为我们配置了三个mongos作为入口，就算其中哪个入口挂掉了都没关系，使用集群客户端程序如下： public class TestMongoDBShards &#123; public static void main(String[] args) &#123; try &#123; List&lt;ServerAddress&gt; addresses = new ArrayList&lt;ServerAddress&gt;(); ServerAddress address1 = new ServerAddress(&quot;192.168.0.136&quot; , 20000); ServerAddress address2 = new ServerAddress(&quot;192.168.0.137&quot; , 20000); ServerAddress address3 = new ServerAddress(&quot;192.168.0.138&quot; , 20000); addresses.add(address1); addresses.add(address2); addresses.add(address3); MongoClient client = new MongoClient(addresses); DB db = client.getDB( &quot;testdb&quot; ); DBCollection coll = db.getCollection( &quot;table1&quot; ); BasicDBObject object = new BasicDBObject(); object.append( &quot;id&quot; , 1); DBObject dbObject = coll.findOne(object); System. out .println(dbObject); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 整个分片集群搭建完了，思考一下我们这个架构是不是足够好呢？其实还有很多地方需要优化，比如我们把所有的仲裁节点放在一台机器，其余两台机器承担了全部读写操作，但是作为仲裁的192.168.0.138相当空闲。让机器3 192.168.0.138多分担点责任吧！架构可以这样调整，把机器的负载分的更加均衡一点，每个机器既可以作为主节点、副本节点、仲裁节点，这样压力就会均衡很多了，如图： 当然生产环境的数据远远大于当前的测试数据，大规模数据应用情况下我们不可能把全部的节点像这样部署，硬件瓶颈是硬伤，只能扩展机器。要用好mongodb还有很多机制需要调整，不过通过这个东东我们可以快速实现高可用性、高扩展性，所以它还是一个非常不错的Nosql组件。 再看看我们使用的mongodb java 驱动客户端 MongoClient(addresses)，这个可以传入多个mongos 的地址作为mongodb集群的入口，并且可以实现自动故障转移，但是负载均衡做的好不好呢？打开源代码查看： 它的机制是选择一个ping 最快的机器来作为所有请求的入口，如果这台机器挂掉会使用下一台机器。那这样。。。。肯定是不行的！万一出现双十一这样的情况所有请求集中发送到这一台机器，这台机器很有可能挂掉。一但挂掉了，按照它的机制会转移请求到下台机器，但是这个压力总量还是没有减少啊！下一台还是可能崩溃，所以这个架构还有漏洞！不过这个文章已经太长了，后续解决吧。 ##配置文件 以下是个人测试时的使用的配置文件： ###mongos port=10000logpath=D:\\MongoDB\\repset\\rep_a\\mongos\\logs\\rep_a_mongos.log#日志信息在日志文件中累加而不是覆盖logappend=true#配置数据库地址configdb=rsconf/localhost:11000,localhost:21000 ###configdb port=11000dbpath=D:\\MongoDB\\repset\\rep_a\\configdb\\datalogpath=D:\\MongoDB\\repset\\rep_a\\configdb\\logs\\rep_a_configdb.log#日志信息在日志文件中累加而不是覆盖logappend=true#代表启动日志journal=true#表允许通过http方式来访问jsonp格式数据#jsonp=true#RESTFULrest=true#配置数据库configsvr=true#分片replSet=rsconf ###rep_a&#x2F;shard_a port=10001dbpath=D:\\MongoDB\\repset\\rep_a\\shard_a\\datalogpath=D:\\MongoDB\\repset\\rep_a\\shard_a\\logs\\rep_a_shard_a.log#日志信息在日志文件中累加而不是覆盖logappend=true#代表启动日志journal=true#表允许通过http方式来访问jsonp格式数据#jsonp=true#RESTFULrest=true#启动分片shardsvr=true#设置副本集replSet=rep_shard_1 ###rep_a&#x2F;shard_b port=10002dbpath=D:\\MongoDB\\repset\\rep_a\\shard_b\\datalogpath=D:\\MongoDB\\repset\\rep_a\\shard_b\\logs\\rep_a_shard_b.log#日志信息在日志文件中累加而不是覆盖logappend=true#代表启动日志journal=true#表允许通过http方式来访问jsonp格式数据#jsonp=true#RESTFULrest=true#启动分片shardsvr=true#设置副本集replSet=rep_shard_2 ###rep_a&#x2F;shard_c port=10003dbpath=D:\\MongoDB\\repset\\rep_a\\shard_c\\datalogpath=D:\\MongoDB\\repset\\rep_a\\shard_c\\logs\\rep_a_shard_c.log#日志信息在日志文件中累加而不是覆盖logappend=true#代表启动日志journal=true#表允许通过http方式来访问jsonp格式数据#jsonp=true#RESTFULrest=true#启动分片shardsvr=true#设置副本集replSet=rep_shard_3","categories":[{"name":"数据库","slug":"db","permalink":"http://wuzguo.com/blog/categories/db/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://wuzguo.com/blog/tags/MongoDB/"}],"author":"Zak"},{"title":"Shiro学习笔记","slug":"server/shiro","date":"2016-08-01T18:43:12.000Z","updated":"2022-08-01T06:35:23.784Z","comments":true,"path":"2016/08/02/server/shiro.html","link":"","permalink":"http://wuzguo.com/blog/2016/08/02/server/shiro.html","excerpt":"","text":"1.Shiro介绍Apache Shiro是一个强大易用的Java安全框架，提供了认证、授权、加密和会话管理等功能： 认证 用户身份识别，常被称为用户“登录” 授权 访问控制 密码加密 保护或隐藏数据防止被偷窥 会话管理 每用户相关的时间敏感的状态 对于任何一个应用程序，Shiro都可以提供全面的安全管理服务。并且相对于其他安全框架，Shiro要简单的多。 2.Shiro架构首先，来了解一下Shiro的三个核心组件：Subject, SecurityManager 和 Realms. 如下图： Subject：即“当前操作用户”。但是，在Shiro中，Subject这一概念并不仅仅指人，也可以是第三方进程、后台帐户（Daemon Account）或其他类似事物。它仅仅意味着“当前跟软件交互的东西”。但考虑到大多数目的和用途，你可以把它认为是Shiro的“用户”概念。Subject代表了当前用户的安全操作，SecurityManager则管理所有用户的安全操作。 SecurityManager：它是Shiro框架的核心，典型的Facade模式，Shiro通过SecurityManager来管理内部组件实例，并通过它来提供安全管理的各种服务。SecurityManager默认实现结构： Realm：域，Shiro从Realm获取安全数据（如用户、角色、权限），就是说SecurityManager要验证用户身份，那么它需要从Realm获取相应的用户进行比较以确定用户身份是否合法；也需要从Realm得到用户相应的角色&#x2F;权限进行验证用户是否能进行操作；可以把Realm看成DataSource，即安全数据源。从这个意义上讲，Realm实质上是一个安全相关的DAO：它封装了数据源的连接细节，并在需要时将相关数据提供给Shiro。当配置Shiro时，你必须至少指定一个Realm，用于认证和（或）授权。配置多个Realm是可以的，但是至少需要一个。 Shiro内置了可以连接大量安全数据源（又名目录）的Realm，如LDAP、关系数据库（JDBC）、类似INI的文本配置资源以及属性文件等。如果缺省的Realm不能满足需求，你还可以插入代表自定义数据源的自己的Realm实现。 Shiro缺省Realm实现： 实现自己的Realm ......public class UserRealm extends AuthorizingRealm &#123; private static final Logger logger = LoggerFactory.getLogger(UserRealm.class); @Resource private IUserService userService; /** * 清楚授权的缓存，授权缓存以用户对象为键 * * @param principals */ @Override protected void clearCachedAuthorizationInfo(PrincipalCollection principals) &#123; logger.debug(&quot;[UserRealm] clearCachedAuthorizationInfo begin&quot;); Cache cache = this.getAuthorizationCache(); Set&lt;Object&gt; keys = cache.keys(); for (Object object : keys) &#123; logger.debug(&quot;[UserRealm] clearCachedAuthorizationInfo object: &quot; + object); &#125; super.clearCachedAuthorizationInfo(principals); logger.debug(&quot;[UserRealm] clearCachedAuthorizationInfo end&quot;); &#125; /** * 授权，如果不使用缓存，每次访问新页面都会执行这个方法 * * @param principals * @return */ protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; User user = ((User) principals.getPrimaryPrincipal()); String code = user.getCode(); System.out.println(user.getId() + &quot;,&quot; + user.getNickname()); List&lt;Role&gt; roles = userService.listRoleByUserCode(code); List&lt;AuthResource&gt; authResources = userService.listResByUserCode(code); List&lt;String&gt; permissions = new ArrayList&lt;String&gt;(); for (AuthResource resource : authResources) &#123; permissions.add(resource.getUrl()); &#125; SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); // info.setRoles(new HashSet&lt;String&gt;(roles)); info.setStringPermissions(new HashSet&lt;String&gt;(permissions)); return info; &#125; /** * 清除认证的缓存 * * @param principals */ @Override protected void clearCachedAuthenticationInfo(PrincipalCollection principals) &#123; logger.debug(&quot;[UserRealm] clearCachedAuthenticationInfo begin&quot;); Cache cache = this.getAuthenticationCache(); Set&lt;Object&gt; keys = cache.keys(); for (Object object : keys) &#123; logger.debug(&quot;[UserRealm] clearCachedAuthorizationInfo object: &quot; + object); &#125; // 认证的缓存以用户名为键，需要手动处理，否则退出登录时登录信息将不能被清除 User user = ((User) principals.getPrimaryPrincipal()); SimplePrincipalCollection simplePrincipalCollection = new SimplePrincipalCollection(user.getUsername(), this.getName()); super.clearCachedAuthenticationInfo(simplePrincipalCollection); logger.debug(&quot;[UserRealm] clearCachedAuthenticationInfo end&quot;); &#125; /** * 获取认证信息，登录的时候调用 * * @param token * @return * @throws AuthenticationException */ protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; String username = token.getPrincipal().toString(); String password = new String((char[]) token.getCredentials()); // 获取用户信息 User user = userService.doGetUserInfo(username, password); if (user == null) &#123; logger.debug(&quot;[UserRealm] doGetAuthenticationInfo user is null&quot;); throw new AuthenticationException(&quot;username invalid&quot;); &#125; SimpleAuthenticationInfo info = new SimpleAuthenticationInfo(user, user.getSerct(), this.getName()); // 设置Salt值，增强密码破解的难度 info.setCredentialsSalt(ByteSource.Util.bytes(user.getUsername())); return info; &#125;&#125;...... Shiro完整架构图： 3.Shiro认证与授权认证就是验证用户身份的过程。在认证过程中，用户需要提交实体信息(Principals)和凭据信息(Credentials)以检验用户是否合法。最常见的“实体&#x2F;凭证”组合便是“用户名&#x2F;密码”组合。被 Shiro 保护的资源，才会经过认证与授权过程。使用 Shiro 对 URL 进行保护可以参见“与 Spring 集成”章节。用户访问受 Shiro 保护的 URL；例如 http://host/security/action.do，Shiro 首先检查用户是否已经通过认证，如果未通过认证检查，则跳转到登录页面，否则进行授权检查。认证过程需要通过 Realm 来获取用户及密码信息，通常情况我们实现 JDBC Realm，此时用户认证所需要的信息从数据库获取。如果使用了缓存，除第一次外用户信息从缓存获取。认证通过后接受 Shiro 授权检查，授权检查同样需要通过 Realm 获取用户权限信息。Shiro 需要的用户权限信息包括 Role 或 Permission，可以是其中任何一种或同时两者，具体取决于受保护资源的配置。如果用户权限信息未包含 Shiro 需要的 Role 或 Permission，授权不通过。只有授权通过，才可以访问受保护 URL 对应的资源，否则跳转到“未经授权页面”。 3.1 身份认证流程1.首先创建一个SecurityManager工厂2.接着获取SecurityManager并绑定到SecurityUtils，这是一个全局设置，设置一次即可；3.通过SecurityUtils得到Subject，其会自动绑定到当前线程；如果在web环境在请求结束时需要解除绑定；然后获取身份验证的Token，如用户名&#x2F;密码；4.调用subject.login方法进行登录，其会自动委托给SecurityManager.login方法进行登录；5.如果身份验证失败请捕获AuthenticationException或其子类，常见的如： DisabledAccountException（禁用的帐号）、LockedAccountException（锁定的帐号）、UnknownAccountException（错误的帐号）、ExcessiveAttemptsException（登录失败次数过多）、IncorrectCredentialsException （错误的凭证）、ExpiredCredentialsException（过期的凭证）等，具体请查看其继承关系；对于页面的错误消息展示，最好使用如“用户名&#x2F;密码错误”而不是“用户名错误”&#x2F;“密码错误”，防止一些恶意用户非法扫描帐号库；6.最后可以调用subject.logout退出，其会自动委托给SecurityManager.logout方法退出。 从如上代码可总结出身份验证的步骤：1.收集用户身份&#x2F;凭证，即如用户名&#x2F;密码；2.调用Subject.login进行登录，如果失败将得到相应的AuthenticationException异常，根据异常提示用户错误信息；否则登录成功；3.最后调用Subject.logout进行退出操作。 流程图如下： 实现代码如下： @Controller@RequestMapping(&quot;/&quot;)public class LoginController &#123; @RequestMapping(value = &quot;/login&quot;, method = RequestMethod.GET) public String login() &#123; return &quot;login&quot;; &#125; @RequestMapping(value = &quot;/login&quot;, method = RequestMethod.POST) public String login(String username, String password, Model model) &#123; Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken(username, password); try &#123; subject.login(token); &#125; catch (AuthenticationException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 3.2 Authenticator及AuthenticationStrategyAuthenticator的职责是验证用户帐号，是Shiro API中身份验证核心的入口点： public AuthenticationInfo authenticate(AuthenticationToken authenticationToken) throws AuthenticationException; 如果验证成功，将返回AuthenticationInfo验证信息；此信息中包含了身份及凭证；如果验证失败将抛出相应的AuthenticationException实现。 SecurityManager接口继承了Authenticator，另外还有一个ModularRealmAuthenticator实现，其委托给多个Realm进行验证，验证规则通过AuthenticationStrategy接口指定，默认提供的实现： FirstSuccessfulStrategy：只要有一个Realm验证成功即可，只返回第一个Realm身份验证成功的认证信息，其他的忽略；AtLeastOneSuccessfulStrategy：只要有一个Realm验证成功即可，和FirstSuccessfulStrategy不同，返回所有Realm身份验证成功的认证信息；AllSuccessfulStrategy：所有Realm验证成功才算成功，且返回所有Realm身份验证成功的认证信息，如果有一个失败就失败了。 ModularRealmAuthenticator默认使用AtLeastOneSuccessfulStrategy策略。 3.3 授权流程授权，也叫访问控制，即在应用中控制谁能访问哪些资源（如访问页面&#x2F;编辑数据&#x2F;页面操作等）。在授权中需了解的几个关键对象：主体（Subject）、资源（Resource）、权限（Permission）、角色（Role）。 主体，即访问应用的用户，在Shiro中使用Subject代表该用户。用户只有授权后才允许访问相应的资源。资源，在应用中用户可以访问的任何东西，比如访问JSP页面、查看&#x2F;编辑某些数据、访问某个业务方法、打印文本等等都是资源。用户只要授权后才能访问。权限，安全策略中的原子授权单位，通过权限我们可以表示在应用中用户有没有操作某个资源的权力。即权限表示在应用中用户能不能访问某个资源，如：访问用户列表页面，查看&#x2F;新增&#x2F;修改&#x2F;删除用户数据（即很多时候都是CRUD（增查改删）式权限控制），打印文档等等… 如上可以看出，权限代表了用户有没有操作某个资源的权利，即反映在某个资源上的操作允不允许，不反映谁去执行这个操作。所以后续还需要把权限赋予给用户，即定义哪个用户允许在某个资源上做什么操作（权限），Shiro不会去做这件事情，而是由实现人员提供。 角色，角色代表了操作集合，可以理解为权限的集合，一般情况下我们会赋予用户角色而不是权限，即这样用户可以拥有一组权限，赋予权限时比较方便。典型的如：项目经理、技术总监、CTO、开发工程师等都是角色，不同的角色拥有一组不同的权限。隐式角色：即直接通过角色来验证用户有没有操作权限，如在应用中CTO、技术总监、开发工程师可以使用打印机，假设某天不允许开发工程师使用打印机，此时需要从应用中删除相应代码；再如在应用中CTO、技术总监可以查看用户、查看权限；突然有一天不允许技术总监查看用户、查看权限了，需要在相关代码中把技术总监角色从判断逻辑中删除掉；即粒度是以角色为单位进行访问控制的，粒度较粗；如果进行修改可能造成多处代码修改。显示角色：在程序中通过权限控制谁能访问某个资源，角色聚合一组权限集合；这样假设哪个角色不能访问某个资源，只需要从角色代表的权限集合中移除即可；无须修改多处代码；即粒度是以资源&#x2F;实例为单位的；粒度较细。 流程如下：1.首先调用Subject.isPermitted*&#x2F;hasRole接口，其会委托给SecurityManager，而SecurityManager接着会委托给Authorizer；2.Authorizer是真正的授权者，如果我们调用如isPermitted(“user:view”)，其首先会通过PermissionResolver把字符串转换成相应的Permission实例；3.在进行授权之前，其会调用相应的Realm获取Subject相应的角色&#x2F;权限用于匹配传入的角色&#x2F;权限；4.Authorizer会判断Realm的角色&#x2F;权限是否和传入的匹配，如果有多个Realm，会委托给ModularRealmAuthorizer进行循环判断，如果匹配如isPermitted&#x2F;hasRole*会返回true，否则返回false表示授权失败。 ModularRealmAuthorizer进行多Realm匹配流程：1.首先检查相应的Realm是否实现了实现了Authorizer；2.如果实现了Authorizer，那么接着调用其相应的isPermitted*&#x2F;hasRole*接口进行匹配；3.如果有一个Realm匹配那么将返回true，否则返回false。 如果Realm进行授权的话，应该继承AuthorizingRealm，其流程是：1.如果调用hasRole*，则直接获取AuthorizationInfo.getRoles()与传入的角色比较即可；2.首先如果调用如isPermitted(“user:view”)，首先通过PermissionResolver将权限字符串转换成相应的Permission实例，默认使用WildcardPermissionResolver，即转换为通配符的WildcardPermission；3.通过AuthorizationInfo.getObjectPermissions()得到Permission实例集合；通过AuthorizationInfo. getStringPermissions()得到字符串集合并通过PermissionResolver解析为Permission实例；然后获取用户的角色，并通过RolePermissionResolver解析角色对应的权限集合（默认没有实现，可以自己提供）；4.接着调用Permission. implies(Permission p)逐个与传入的权限比较，如果有匹配的则返回true，否则false。 3.4 Authorizer、PermissionResolver及RolePermissionResolverAuthorizer的职责是进行授权（访问控制），是Shiro API中授权核心的入口点，其提供了相应的角色&#x2F;权限判断接口，具体请参考其Javadoc。SecurityManager继承了Authorizer接口，且提供了ModularRealmAuthorizer用于多Realm时的授权匹配。PermissionResolver用于解析权限字符串到Permission实例，而RolePermissionResolver用于根据角色解析相应的权限集合。 我们可以通过如下配置更改Authorizer实现： ......&lt;!--自定义permissionResolver--&gt;&lt;bean id=&quot;urlPermissionResolver&quot; class=&quot;com.github.wzguo.service.shiro.resolver.WebUrlPermissionResolver&quot;/&gt;&lt;bean id=&quot;authorizer&quot; class=&quot;org.apache.shiro.authz.ModularRealmAuthorizer&quot;&gt; &lt;property name=&quot;permissionResolver&quot; ref=&quot;urlPermissionResolver&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt; &lt;property name=&quot;authorizer&quot; ref=&quot;authorizer&quot;/&gt;&lt;/bean&gt;...... 流程图如下： 实现代码如下： @Controller@RequestMapping(value=&quot;user&quot;)public class UserController &#123; @RequestMapping(params = &quot;myjsp&quot;) public String home() &#123; Subject currentUser = SecurityUtils.getSubject(); if(currentUser.isPermitted(&quot;user.do?myjsp&quot;))&#123; return &quot;my&quot;; &#125;else&#123; return &quot;error/noperms&quot;; &#125; &#125; @RequestMapping(params = &quot;notmyjsp&quot;) public String nopermission() &#123; Subject currentUser = SecurityUtils.getSubject(); if(currentUser.isPermitted(&quot;user.do?notmyjsp&quot;))&#123; return &quot;notmyjsp&quot;; &#125;else&#123; return &quot;error/noperms&quot;; &#125; &#125;&#125; 4.拦截器4.1 拦截器介绍Shiro使用了与Servlet一样的Filter接口进行扩展,首先下图是Shiro拦截器的基础类图： NameableFilterNameableFilter给Filter起个名字，如果没有设置默认就是FilterName；还记得之前的如authc吗？当我们组装拦截器链时会根据这个名字找到相应的拦截器实例； OncePerRequestFilterOncePerRequestFilter用于防止多次执行Filter的；也就是说一次请求只会走一次拦截器链；另外提供enabled属性，表示是否开启该拦截器实例，默认enabled&#x3D;true表示开启，如果不想让某个拦截器工作，可以设置为false即可。 ShiroFilterShiroFilter是整个Shiro的入口点，用于拦截需要安全控制的请求进行处理，这个之前已经用过了。 AdviceFilterAdviceFilter提供了AOP风格的支持，类似于SpringMVC中的Interceptor： boolean preHandle(ServletRequest request, ServletResponse response) throws Exception void postHandle(ServletRequest request, ServletResponse response) throws Exception void afterCompletion(ServletRequest request, ServletResponse response, Exception exception) throws Exception; preHandler：类似于AOP中的前置增强；在拦截器链执行之前执行；如果返回true则继续拦截器链；否则中断后续的拦截器链的执行直接返回；进行预处理（如基于表单的身份验证、授权）postHandle：类似于AOP中的后置返回增强；在拦截器链执行完成后执行；进行后处理（如记录执行时间之类的）；afterCompletion：类似于AOP中的后置最终增强；即不管有没有异常都会执行；可以进行清理资源（如接触Subject与线程的绑定之类的）； PathMatchingFilterPathMatchingFilter提供了基于Ant风格的请求路径匹配功能及拦截器参数解析的功能，如“roles[admin,user]”自动根据“，”分割解析到一个路径参数配置并绑定到相应的路径： boolean pathsMatch(String path, ServletRequest request) boolean onPreHandle(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception pathsMatch：该方法用于path与请求路径进行匹配的方法；如果匹配返回true；onPreHandle：在preHandle中，当pathsMatch匹配一个路径后，会调用opPreHandler方法并将路径绑定参数配置传给mappedValue；然后可以在这个方法中进行一些验证（如角色授权），如果验证失败可以返回false中断流程；默认返回true；也就是说子类可以只实现onPreHandle即可，无须实现preHandle。如果没有path与请求路径匹配，默认是通过的（即preHandle返回true）。 AccessControlFilterAccessControlFilter提供了访问控制的基础功能；比如是否允许访问&#x2F;当访问拒绝时如何处理等： abstract boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception; boolean onAccessDenied(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception; abstract boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception; isAccessAllowed：表示是否允许访问；mappedValue就是[urls]配置中拦截器参数部分，如果允许访问返回true，否则false；onAccessDenied：表示当访问拒绝时是否已经处理了；如果返回true表示需要继续处理；如果返回false表示该拦截器实例已经处理了，将直接返回即可。 onPreHandle会自动调用这两个方法决定是否继续处理： boolean onPreHandle(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception &#123; return isAccessAllowed(request, response, mappedValue) || onAccessDenied(request, response, mappedValue); &#125; 另外AccessControlFilter还提供了如下方法用于处理如登录成功后&#x2F;重定向到上一个请求： void setLoginUrl(String loginUrl) //身份验证时使用，默认/login.jsp String getLoginUrl() Subject getSubject(ServletRequest request, ServletResponse response) //获取Subject实例 boolean isLoginRequest(ServletRequest request, ServletResponse response)//当前请求是否是登录请求 void saveRequestAndRedirectToLogin(ServletRequest request, ServletResponse response) throws IOException //将当前请求保存起来并重定向到登录页面 void saveRequest(ServletRequest request) //将请求保存起来，如登录成功后再重定向回该请求 void redirectToLogin(ServletRequest request, ServletResponse response) //重定向到登录页面 比如基于表单的身份验证就需要使用这些功能。 到此基本的拦截器就完事了，如果我们想进行访问访问的控制就可以继承AccessControlFilter；如果我们要添加一些通用数据我们可以直接继承PathMatchingFilter。 4.2 拦截器链Shiro对Servlet容器的FilterChain进行了代理，即ShiroFilter在继续Servlet容器的Filter链的执行之前，通过ProxiedFilterChain对Servlet容器的FilterChain进行了代理；即先走Shiro自己的Filter体系，然后才会委托给Servlet容器的FilterChain进行Servlet容器级别的Filter链执行；Shiro的ProxiedFilterChain执行流程：1、先执行Shiro自己的Filter链；2、再执行Servlet容器的Filter链（即原始的Filter）。而ProxiedFilterChain是通过FilterChainResolver根据配置文件中[urls]部分是否与请求的URL是否匹配解析得到的。 FilterChain getChain(ServletRequest request, ServletResponse response, FilterChain originalChain); 即传入原始的chain得到一个代理的chain。Shiro内部提供了一个路径匹配的FilterChainResolver实现：PathMatchingFilterChainResolver，其根据[urls]中配置的url模式（默认Ant风格）&#x3D;拦截器链和请求的url是否匹配来解析得到配置的拦截器链的；而PathMatchingFilterChainResolver内部通过FilterChainManager维护着拦截器链，比如DefaultFilterChainManager实现维护着url模式与拦截器链的关系。因此我们可以通过FilterChainManager进行动态动态增加url模式与拦截器链的关系。 DefaultFilterChainManager会默认添加org.apache.shiro.web.filter.mgt.DefaultFilter中声明的拦截器： public enum DefaultFilter &#123; anon(AnonymousFilter.class), authc(FormAuthenticationFilter.class), authcBasic(BasicHttpAuthenticationFilter.class), logout(LogoutFilter.class), noSessionCreation(NoSessionCreationFilter.class), perms(PermissionsAuthorizationFilter.class), port(PortFilter.class), rest(HttpMethodPermissionFilter.class), roles(RolesAuthorizationFilter.class), ssl(SslFilter.class), user(UserFilter.class); &#125; 如果要注册自定义拦截器，IniSecurityManagerFactory&#x2F;WebIniSecurityManagerFactory在启动时会自动扫描ini配置文件中的[filters]&#x2F;[main]部分并注册这些拦截器到DefaultFilterChainManager；且创建相应的url模式与其拦截器关系链。如果使用Spring后续章节会介绍如果注册自定义拦截器。 如果想自定义FilterChainResolver，可以通过实现WebEnvironment接口完成： public class MyIniWebEnvironment extends IniWebEnvironment &#123; @Override protected FilterChainResolver createFilterChainResolver() &#123; //在此处扩展自己的FilterChainResolver return super.createFilterChainResolver(); &#125; &#125; FilterChain之间的关系。如果想动态实现url-拦截器的注册，就可以通过实现此处的FilterChainResolver来完成，比如： //1、创建FilterChainResolver PathMatchingFilterChainResolver filterChainResolver = new PathMatchingFilterChainResolver(); //2、创建FilterChainManager DefaultFilterChainManager filterChainManager = new DefaultFilterChainManager(); //3、注册Filter for(DefaultFilter filter : DefaultFilter.values()) &#123; filterChainManager.addFilter( filter.name(), (Filter) ClassUtils.newInstance(filter.getFilterClass())); &#125; //4、注册URL-Filter的映射关系 filterChainManager.addToChain(&quot;/login.jsp&quot;, &quot;authc&quot;); filterChainManager.addToChain(&quot;/unauthorized.jsp&quot;, &quot;anon&quot;); filterChainManager.addToChain(&quot;/**&quot;, &quot;authc&quot;); filterChainManager.addToChain(&quot;/**&quot;, &quot;roles&quot;, &quot;admin&quot;); //5、设置Filter的属性 FormAuthenticationFilter authcFilter = (FormAuthenticationFilter)filterChainManager.getFilter(&quot;authc&quot;); authcFilter.setLoginUrl(&quot;/login.jsp&quot;); RolesAuthorizationFilter rolesFilter = (RolesAuthorizationFilter)filterChainManager.getFilter(&quot;roles&quot;); rolesFilter.setUnauthorizedUrl(&quot;/unauthorized.jsp&quot;); filterChainResolver.setFilterChainManager(filterChainManager); return filterChainResolver; 此处自己去实现注册filter，及url模式与filter之间的映射关系。可以通过定制FilterChainResolver或FilterChainManager来完成诸如动态URL匹配的实现。 然后再web.xml中进行如下配置Environment： &lt;context-param&gt; &lt;param-name&gt;shiroEnvironmentClass&lt;/param-name&gt; &lt;param-value&gt;com.github.zhangkaitao.shiro.chapter8.web.env.MyIniWebEnvironment&lt;/param-value&gt; &lt;/context-param&gt; 4.3 自定义拦截器通过自定义自己的拦截器可以扩展一些功能，诸如动态url-角色&#x2F;权限访问控制的实现、根据Subject身份信息获取用户信息绑定到Request（即设置通用数据）、验证码验证、在线用户信息的保存等等，因为其本质就是一个Filter；所以Filter能做的它就能做。 基于表单登录拦截器 ......public class FormLoginFilter extends PathMatchingFilter &#123; private String loginUrl = &quot;/login.jsp&quot;; private String successUrl = &quot;/&quot;; @Override protected boolean onPreHandle(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception &#123; if(SecurityUtils.getSubject().isAuthenticated()) &#123; return true;//已经登录过 &#125; HttpServletRequest req = (HttpServletRequest) request; HttpServletResponse resp = (HttpServletResponse) response; if(isLoginRequest(req)) &#123; if(&quot;post&quot;.equalsIgnoreCase(req.getMethod())) &#123;//form表单提交 boolean loginSuccess = login(req); //登录 if(loginSuccess) &#123; return redirectToSuccessUrl(req, resp); &#125; &#125; return true;//继续过滤器链 &#125; else &#123;//保存当前地址并重定向到登录界面 saveRequestAndRedirectToLogin(req, resp); return false; &#125; &#125; private boolean redirectToSuccessUrl(HttpServletRequest req, HttpServletResponse resp) throws IOException &#123; WebUtils.redirectToSavedRequest(req, resp, successUrl); return false; &#125; private void saveRequestAndRedirectToLogin(HttpServletRequest req, HttpServletResponse resp) throws IOException &#123; WebUtils.saveRequest(req); WebUtils.issueRedirect(req, resp, loginUrl); &#125; private boolean login(HttpServletRequest req) &#123; String username = req.getParameter(&quot;username&quot;); String password = req.getParameter(&quot;password&quot;); try &#123; SecurityUtils.getSubject().login(new UsernamePasswordToken(username, password)); &#125; catch (Exception e) &#123; req.setAttribute(&quot;shiroLoginFailure&quot;, e.getClass()); return false; &#125; return true; &#125; private boolean isLoginRequest(HttpServletRequest req) &#123; return pathsMatch(loginUrl, WebUtils.getPathWithinApplication(req)); &#125; &#125; ...... onPreHandle主要流程：1、首先判断是否已经登录过了，如果已经登录过了继续拦截器链即可；2、如果没有登录，看看是否是登录请求，如果是get方法的登录页面请求，则继续拦截器链（到请求页面），否则如果是get方法的其他页面请求则保存当前请求并重定向到登录页面；3、如果是post方法的登录页面表单提交请求，则收集用户名&#x2F;密码登录即可，如果失败了保存错误消息到“shiroLoginFailure”并返回到登录页面；4、如果登录成功了，且之前有保存的请求，则重定向到之前的这个请求，否则到默认的成功页面。 任意角色授权拦截器Shiro提供roles拦截器，其验证用户拥有所有角色，没有提供验证用户拥有任意角色的拦截器。 ......public class AnyRolesFilter extends AccessControlFilter &#123; private String unauthorizedUrl = &quot;/unauthorized.jsp&quot;; private String loginUrl = &quot;/login.jsp&quot;; protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception &#123; String[] roles = (String[])mappedValue; if(roles == null) &#123; return true;//如果没有设置角色参数，默认成功 &#125; for(String role : roles) &#123; if(getSubject(request, response).hasRole(role)) &#123; return true; &#125; &#125; return false;//跳到onAccessDenied处理 &#125; @Override protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception &#123; Subject subject = getSubject(request, response); if (subject.getPrincipal() == null) &#123;//表示没有登录，重定向到登录页面 saveRequest(request); WebUtils.issueRedirect(request, response, loginUrl); &#125; else &#123; if (StringUtils.hasText(unauthorizedUrl)) &#123;//如果有未授权页面跳转过去 WebUtils.issueRedirect(request, response, unauthorizedUrl); &#125; else &#123;//否则返回401未授权状态码 WebUtils.toHttp(response).sendError(HttpServletResponse.SC_UNAUTHORIZED); &#125; &#125; return false; &#125; &#125; ...... 流程：1、首先判断用户有没有任意角色，如果没有返回false，将到onAccessDenied进行处理；2、如果用户没有角色，接着判断用户有没有登录，如果没有登录先重定向到登录；3、如果用户没有角色且设置了未授权页面（unauthorizedUrl），那么重定向到未授权页面；否则直接返回401未授权错误码。 4.4 默认拦截器Shiro内置了很多默认的拦截器，比如身份验证、授权等相关的。默认拦截器可以参考org.apache.shiro.web.filter.mgt.DefaultFilter中的枚举拦截器： 拦截器名拦截器类使用场景说明（括号里的表示默认值）authcFormAuthenticationFilter验证基于表单的拦截器,如\"/****=authc\",如果没有登录会跳到相应的登录页面登录:主要属性:usernameParam:表单提交的用户名参数名(username):passwordParam:表单提交的密码参数名(password):rememberMeParam:表单提交的密码参数名(rememberMe):loginUrl:登录页面地址(/login.jsp):successUrl:登录成功后的默认重定向地址:failureKeyAttribute:登录失败后错误信息存储key(shiroLoginFailure)authcBasicBasicHttpAuthenticationFilter验证Basic HTTP身份验证拦截器，主要属性： applicationName：弹出登录框显示的信息（application）Basic HTTP身份验证拦截器,主要属性:applicationName,弹出登录框显示的信息(application)logoutLogoutFilter验证退出拦截器,主要属性:redirectUrl:退出成功后重定向的地址(/)示例\"/logout=logout\"userUserFilter验证用户拦截器,用户已经身份验证/记住我登录的都可:示例/****=useranonAnonymousFilter验证匿名拦截器，即不需要登录即可访问:一般用于静态资源过滤:示例\"/static/****=anon\"rolesRolesAuthorizationFilter授权角色授权拦截器,验证用户是否拥有所有角色:主要属性:loginUrl:登录页面地址(/login.jsp):unauthorizedUrl:未授权后重定向的地址:示例\"/admin/****=roles[admin]\"permsPermissionsAuthorizationFilter授权权限授权拦截器,验证用户是否拥有所有权限:属性和roles一样:示例\"/user/**=perms[\"user:create\"]\"portPortFilter授权端口拦截器,主要属性:port(80):可以通过的端口:示例\"/test= port[80]\",如果用户访问该页面是非80,将自动将请求端口改为80并重定向到该80端口,其他路径/参数等都一样restHttpMethodPermissionFilter授权rest风格拦截器,自动根据请求方法构建权限字符串(GET=read, POST=create,PUT=update,DELETE=delete,HEAD=read,TRACE=read,OPTIONS=read, MKCOL=create)构建权限字符串:示例\"/users=rest[user]\",会自动拼出\"user:read,user:create,user:update,user:delete\"权限字符串进行权限匹配(所有都得匹配,isPermittedAll)sslSslFilter授权SSL拦截器,只有请求协议是https才能通过:否则自动跳转会https端口(443):其他和port拦截器一样SSL拦截器,只有请求协议是https才能通过:否则自动跳转会https端口(443):其他和port拦截器一样noSessionCreationNoSessionCreationFilter其他不创建会话拦截器,调用subject.getSession(false)不会有什么问题,但是如果subject.getSession(true)将抛出 DisabledSessionException异常 另外还提供了一个org.apache.shiro.web.filter.authz.HostFilter，即主机拦截器，比如其提供了属性：authorizedIps：已授权的ip地址，deniedIps：表示拒绝的ip地址；不过目前还没有完全实现，不可用。 5.参考文章跟我学Shiro","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"shiro","slug":"shiro","permalink":"http://wuzguo.com/blog/tags/shiro/"}],"author":"Zak"},{"title":"深入理解Java虚拟机读书笔记","slug":"server/jvm_reading","date":"2016-06-10T17:43:52.000Z","updated":"2022-08-01T06:35:23.731Z","comments":true,"path":"2016/06/11/server/jvm_reading.html","link":"","permalink":"http://wuzguo.com/blog/2016/06/11/server/jvm_reading.html","excerpt":"","text":"最近有幸拜读了周志明先生所著《深入理解Java虚拟机：JVM高级特性与最佳实践》一书，感触颇深，遂写读书笔记一篇以便加深记忆。 全书分五部分，共13章讲解了Java虚拟机的由来到发展成熟的过程，深入分析了Java虚拟机的实现原理、Class文件格式及加载过程、自动回收机制的实现及垃圾回收算法、虚拟机性能调优和高效多并发等方面的知识，语言通俗易懂，深入浅出，实属我等屌丝程序猿学习的佳作。 Java之所以能获得如此广泛的认可，除了它拥有结构严谨、面向对象的编程、好用到哭的API（跟某些非人类能理解的语言相比，如某++）、垃圾回收机制（写过某++的都知道内存溢出有多蛋疼）等特性之外，还有许多不可忽视的优点，比如：摆脱硬件平台的束缚，实现了“一次编写，到处运行”的理想。 传统的编程语言，如C、C++都是跟平台相关的语言，代码编写受平台约束，编译出来的程序只能在特定环境中运行，在多套操作系统或多CPU架构中，需要重新编译甚至重写代码，无形中增加了程序猿的工作难度。而Java语言引进了虚拟机的概率，通过虚拟机层（ “ 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决”）隐藏了底层技术的复杂性以及物理机和操作系统的差异性，这样程序猿们可以把主要精力放在业务实现和代码算法上。 凡事具有两面性，虽然Java虚拟机屏蔽了平台的差异性，但是程序仍旧是要在硬件平台中运行的，始终逃不过兼容硬件带来的性能损耗。传统的编译性语言（对应是解释性语言）直接编译成平台相关的程序运行起来效率肯定优于需要虚拟机做适配的语言。Java虚拟机为了使程序运行速度能达到或者接近直接在硬件平台上运行的编程语言的速度，做了一系列的优化，如：JIT技术、方法内联、逃逸分析等等。 说了这么多废话，下面真正开始讲述Java虚拟机的内容。 目前主流的主流的商业VM（虚拟机）有Sun HotSpot VM、BEA JRockit、IBM J9 VM、每家厂商都说自己的是最好的，不过Sun有开源版的JDK（OpenJDK）、可以在官网上下载，有时间了去看下源码（程序猿会有时间吗？感觉从来都是时间不够用）。注：后续所说的虚拟机都是指Sun HotSpot VM。 Java虚拟机在执行程序的过程中会把管理的内存划分为若干个不同数据区域，各个区域有不同用途，有的区域随着虚拟机进程的启动而存在，有的区域则依赖用户线程的启动和结束而建立和销毁。 方法区和堆是线程共享的，方法区存储已被虚拟机的类加载器（ExtClassLoader、AppClassLoader、…）加载的类信息、产量、静态变量、即时编译器编译后的代码、Class对象（只有HotSpot VM 是这样）等数据。堆（Heap）是Java虚拟机管理的内存中最大的一块。几乎所有的对象实例都分配在这里（不是所有哦），也是垃圾回收器工作的主要战场，因此也被被称作“GC”堆（Garbage Collected Heap）。 由于Java虚拟机的多线程实现四通过线程轮流切换（抢占式线程调度机制，主要的调度机制还有一种是协同式线程调度–即某个线程的任务执行完了才把CPU让给别的线程），在任何时刻，一个处理器内核只能执行一个线程中的指令。程序计数器（PC 寄存器）的作用就是为了当前线程被CPU翻牌（选中）的时候能回到上次执行到的位置，否则又得从头开始。Java虚拟机的栈（Stack）也是线程私有的，他是一种“先进后出、后进先出”的数据结构，描述的是Java方法执行的内存模型， 他的生命周期和线程相同，每个方法执行都会创建一个栈帧（Stack Frame），用于存储局部变量表、操作数栈、动态链接、方法的返回地址等信息。 如果线程请求的栈深度大于虚拟机允许的深度将抛出StackOverflowError的异常（可以解释递归函数为什么容易栈溢出）。如果虚拟机栈可以动态扩展（Windows系统32位用户态线程最大分配内存为2GB），如果无法申请到足够的内存就会抛出OutOfMenoryError异常。 以下是抛出 StackOverflowError 异常的测试代码/*** StackOverflowError VM Args：-Xss128k @author zzm */public class JavaVMStackSOF &#123; private int stackLength = 1; public void stackLeak() &#123; stackLength++; stackLeak(); &#125; public static void main(String[] args) throws Throwable &#123; JavaVMStackSOF oom = new JavaVMStackSOF(); try &#123; oom.stackLeak(); &#125; catch (Throwable e) &#123; System.out.println(&quot;stack length:&quot; + oom.stackLength); throw e; &#125; &#125; &#125; 局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）和对象引用（reference类型吗，指向对象起始地址的引用指针或代表对象的句柄）。long、double类型数据占用2个局部变量空间（Slot）。 ![](/images/201606/4.png) 开始有说到堆（Heap）是Java虚拟机管理的内存中最大的一块，堆中存放了大部分对象，为了有序的管理这些对象和GC的顺利进行（GC算法主要作用于堆中，其实方法区也有GC，不过条件相对苛刻，性价比不高），虚拟机把堆分成了多个块来管理，一般分为 Eden（新生代）、Tenured（老年代）、Permgen（永生代）。 正常情况下 Surivivor1 和 Surivivor2中有一块是空的，垃圾回收算法运行时，会将 Eden区中存活的对象和Surivivor中存活的对象复制到另外一个Surivivor区中（复制算法），然后清理掉Eden区和Surivivor区。常用的垃圾回收算法有标记-清理算法（Mark-Sweep）、复制算法（Copying，多少商业虚拟机都用这种算法回收新生代对象）、标记-整理算法（Mark-Compact，回收老年代的对象）。 HotSpot VM根据以上算法，实现了不同的类型的垃圾回收器，如下图： Serial(串行GC、复制算法 )收集器Serial收集器是一个新生代收集器，单线程执行，使用复制算法。它在进行垃圾收集时，必须暂停其他所有的工作线程(用户线程)。是JVM Client模式下默认的新生代收集器。对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。 ParNew(并行GC、复制算法 )收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样。 Parallel Scavenge(并行GC、复制算法 )收集器Parallel Scavenge收集器也是一个新生代收集器，它也是使用复制算法的收集器，又是并行多线程收集器。Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量。吞吐量= 程序运行时间/(程序运行时间 + 垃圾收集时间)，虚拟机总共运行了100分钟。其中垃圾收集花掉1分钟，那吞吐量就是99%。由于与吞吐量关系密切，Parallel Scavenge收集器也称为“吞吐量优先”收集器。 Serial Old(串行GC、“标记-整理”算法 )收集器Serial Old是Serial收集器的老年代版本，它同样使用一个单线程执行收集，使用“标记-整理”算法。主要使用在Client模式下的虚拟机。 Parallel Old(并行GC、“标记-整理”算法 )收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。自JDK1.6开始提供。 CMS(并发GC、“标记-清除”算法 )收集器CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。CMS收集器是基于“标记-清除”算法实现的，整个收集过程大致分为4个步骤： ①.初始标记(CMS initial mark) ②.并发标记(CMS concurrenr mark) ③.重新标记(CMS remark) ④.并发清除(CMS concurrent sweep) 其中初始标记、重新标记这两个步骤任然需要停顿其他用户线程。初始标记仅仅只是标记出GC ROOTS能直接关联到的对象，速度很快，并发标记阶段是进行GC ROOTS 根搜索算法阶段，会判定对象是否存活。而重新标记阶段则是为了修正并发标记期间，因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间会被初始标记阶段稍长，但比并发标记阶段要短。由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以整体来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。 CMS收集器的优点：并发收集、低停顿。CMS收集器的缺点： CMS收集器对CPU资源非常敏感。在并发阶段，虽然不会导致用户线程停顿，但是会占用CPU资源而导致引用程序变慢，总吞吐量下降。CMS默认启动的回收线程数是：(CPU数量+3) / 4。 CMS收集器无法处理浮动垃圾（在垃圾回收器回收期间产生的垃圾），可能出现“Concurrent Mode Failure“，失败后而导致另一次Full GC的产生。 CMS是基于“标记-清除”算法实现的收集器，使用“标记-清除”算法收集后，会产生大量碎片。空间碎片太多时，将会给对象分配带来很多麻烦，比如说大对象，内存空间找不到连续的空间来分配不得不提前触发一次Full GC。 G1（并发和并行GC、“标记-整理”算法 ）收集器G1(Garbage First)收集器是JDK1.7提供的一个面向服务应用的收集器，G1收集器基于“标记-整理”算法实现，也就是说不会产生内存碎片。还有一个特点之前的收集器进行收集的范围都是整个新生代或老年代，而G1将整个Java堆(包括新生代，老年代)。G1收集器大致可划分为以下几个步骤： ①.初始标记(Initial Marking) ②.并发标记(Concurrenr Marking) ③.最终标记(Final Marking) ④.筛选回收(Live Data Counting and Evacuation)对于追求低停顿的应用，那G1现在可以作为一个可尝试的选择，如果追求吞吐量，那G1并不是特别好的选择。 GC日志首先看一下如下代码：public class PrintGCDetails &#123; public static void main(String[] args) &#123; Object obj = new Object(); System.gc(); System.out.println(); obj = new Object(); obj = new Object(); System.gc(); System.out.println(); &#125; &#125;设置JVM参数为-XX:+PrintGCDetails,执行结果如下:[GC [PSYoungGen: 1019K->568K(28672K)] 1019K->568K(92672K), 0.0529244 secs] [Times: user=0.00 sys=0.00, real=0.06 secs]自定义注解：[GC [新生代: MinorGC前新生代内存使用-&gt;MinorGC后新生代内存使用(新生代总的内存大小)] MinorGC前JVM堆内存使用的大小-&gt;MinorGC后JVM堆内存使用的大小(堆的可用内存大小), MinorGC总耗时] [Times: 用户耗时&#x3D;0.00 系统耗时&#x3D;0.00, 实际耗时&#x3D;0.06 secs] [Full GC [PSYoungGen: 568K->0K(28672K)] [ParOldGen: 0K->478K(64000K)] 568K->478K(92672K) [PSPermGen: 2484K->2483K(21504K)], 0.0178331 secs] [Times: user=0.01 sys=0.00, real=0.02 secs]自定义注解：[Full GC [PSYoungGen: 568K-&gt;0K(28672K)] [老年代: FullGC前老年代内存使用-&gt;FullGC后老年代内存使用(老年代总的内存大小)] FullGC前JVM堆内存使用的大小-&gt;FullGC后JVM堆内存使用的大小(堆的可用内存大小) [永久代: 2484K-&gt;2483K(21504K)], 0.0178331 secs] [Times: user&#x3D;0.01 sys&#x3D;0.00, real&#x3D;0.02 secs][GC [PSYoungGen: 501K->64K(28672K)] 980K->542K(92672K), 0.0005080 secs] [Times: user=0.00 sys=0.00, real=0.00 secs][Full GC [PSYoungGen: 64K->0K(28672K)] [ParOldGen: 478K->479K(64000K)] 542K->479K(92672K) [PSPermGen: 2483K->2483K(21504K)], 0.0133836 secs] [Times: user=0.05 sys=0.00, real=0.01 secs] Heap PSYoungGen total 28672K, used 1505K [0x00000000e0a00000, 0x00000000e2980000, 0x0000000100000000) eden space 25088K, 6% used [0x00000000e0a00000,0x00000000e0b78690,0x00000000e2280000) from space 3584K, 0% used [0x00000000e2600000,0x00000000e2600000,0x00000000e2980000) to space 3584K, 0% used [0x00000000e2280000,0x00000000e2280000,0x00000000e2600000) ParOldGen total 64000K, used 479K [0x00000000a1e00000, 0x00000000a5c80000, 0x00000000e0a00000) object space 64000K, 0% used [0x00000000a1e00000,0x00000000a1e77d18,0x00000000a5c80000) PSPermGen total 21504K, used 2492K [0x000000009cc00000, 0x000000009e100000, 0x00000000a1e00000) object space 21504K, 11% used [0x000000009cc00000,0x000000009ce6f2d0,0x000000009e100000)注：你可以用JConsole或者Runtime.getRuntime().maxMemory(),Runtime.getRuntime().totalMemory(), Runtime.getRuntime().freeMemory()来查看Java中堆内存的大小。 再看一个例子：public class PrintGCDetails2 &#123; /** ** -Xms60m -Xmx60m -Xmn20m -XX:NewRatio=2 ( 若 Xms = Xmx, 并且设定了 Xmn, ** 那么该项配置就不需要配置了 ) -XX:SurvivorRatio=8 -XX:PermSize=30m -XX:MaxPermSize=30m ** -XX:+PrintGCDetails **/ public static void main(String[] args) &#123; new PrintGCDetails2().doTest(); &#125; public void doTest() &#123; Integer M = new Integer(1024 * 1024 * 1); // 单位, 兆(M) byte[] bytes = new byte[1 * M]; // 申请 1M 大小的内存空间 bytes = null; // 断开引用链 System.gc(); // 通知 GC 收集垃圾 System.out.println(); bytes = new byte[1 * M]; // 重新申请 1M 大小的内存空间 bytes = new byte[1 * M]; // 再次申请 1M 大小的内存空间 System.gc(); System.out.println(); &#125; &#125;运行结果：[GC [PSYoungGen: 2007K->568K(18432K)] 2007K->568K(59392K), 0.0059377 secs] [Times: user=0.02 sys=0.00, real=0.01 secs][Full GC [PSYoungGen: 568K->0K(18432K)] [ParOldGen: 0K->479K(40960K)] 568K->479K(59392K) [PSPermGen: 2484K->2483K(30720K)], 0.0223249 secs] [Times: user=0.01 sys=0.00, real=0.02 secs] [GC [PSYoungGen: 3031K->1056K(18432K)] 3510K->1535K(59392K), 0.0140169 secs] [Times: user=0.05 sys=0.00, real=0.01 secs][Full GC [PSYoungGen: 1056K->0K(18432K)] [ParOldGen: 479K->1503K(40960K)] 1535K->1503K(59392K) [PSPermGen: 2486K->2486K(30720K)], 0.0119497 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] Heap PSYoungGen total 18432K, used 163K [0x00000000fec00000, 0x0000000100000000, 0x0000000100000000) eden space 16384K, 1% used [0x00000000fec00000,0x00000000fec28ff0,0x00000000ffc00000) from space 2048K, 0% used [0x00000000ffe00000,0x00000000ffe00000,0x0000000100000000) to space 2048K, 0% used [0x00000000ffc00000,0x00000000ffc00000,0x00000000ffe00000) ParOldGen total 40960K, used 1503K [0x00000000fc400000, 0x00000000fec00000, 0x00000000fec00000) object space 40960K, 3% used [0x00000000fc400000,0x00000000fc577e10,0x00000000fec00000) PSPermGen total 30720K, used 2493K [0x00000000fa600000, 0x00000000fc400000, 0x00000000fc400000) object space 30720K, 8% used [0x00000000fa600000,0x00000000fa86f4f0,0x00000000fc400000)从打印结果可以看出，堆中新生代的内存空间为 18432K ( 约 18M )，eden 的内存空间为 16384K ( 约 16M)，from &#x2F; to survivor 的内存空间为 2048K ( 约 2M)。 这里所配置的 Xmn 为 20M，也就是指定了新生代的内存空间为 20M，可是从打印的堆信息来看，新生代怎么就只有 18M 呢? 另外的 2M 哪里去了? 别急，是这样的。新生代 &#x3D; eden + from + to &#x3D; 16 + 2 + 2 &#x3D; 20M，可见新生代的内存空间确实是按 Xmn 参数分配得到的。 而且这里指定了 SurvivorRatio &#x3D; 8，因此，eden &#x3D; 8&#x2F;10 的新生代空间 &#x3D; 8&#x2F;10 * 20 &#x3D; 16M。from &#x3D; to &#x3D; 1&#x2F;10 的新生代空间 &#x3D; 1&#x2F;10 * 20 &#x3D; 2M。 堆信息中新生代的 total 18432K 是这样来的： eden + 1 个 survivor &#x3D; 16384K + 2048K &#x3D; 18432K，即约为 18M。 因为 jvm 每次只是用新生代中的 eden 和 一个 survivor，因此新生代实际的可用内存空间大小为所指定的 90%。 因此可以知道，这里新生代的内存空间指的是新生代可用的总的内存空间，而不是指整个新生代的空间大小。 另外，可以看出老年代的内存空间为 40960K ( 约 40M )，堆大小 &#x3D; 新生代 + 老年代。因此在这里，老年代 &#x3D; 堆大小 - 新生代 &#x3D; 60 - 20 &#x3D; 40M。 最后，这里还指定了 PermSize &#x3D; 30m，PermGen 即永久代 ( 方法区 )，它还有一个名字，叫非堆，主要用来存储由 jvm 加载的类文件信息、常量、静态变量等。 垃圾收集器参数总结","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://wuzguo.com/blog/tags/JVM/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://wuzguo.com/blog/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"author":"Zak"},{"title":"Flyway的基本使用","slug":"db/flyway","date":"2016-05-26T01:48:25.000Z","updated":"2022-08-01T06:35:23.650Z","comments":true,"path":"2016/05/26/db/flyway.html","link":"","permalink":"http://wuzguo.com/blog/2016/05/26/db/flyway.html","excerpt":"","text":"Flyway 是独立于数据库的应用、管理并跟踪数据库变更的数据库版本管理工具。 Flyway 的项目主页是 http://flywaydb.org/ Flyway配置成Maven插件，如下图： 以下是我测试时配置的POM.xml .... &lt;build&gt; &lt;finalName&gt;flyway&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.flywaydb&lt;/groupId&gt; &lt;artifactId&gt;flyway-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;flywaydb.version&#125;&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.jdbc.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;configuration&gt; &lt;driver&gt;com.mysql.jdbc.Driver&lt;/driver&gt; &lt;url&gt;jdbc:mysql://localhost:3306/flyway?useUnicode=true&amp;amp;characterEncoding=utf-8&lt;/url&gt; &lt;user&gt;root&lt;/user&gt; &lt;password&gt;root&lt;/password&gt; &lt;!-- 设置接受flyway进行版本管理的数据库，多个数据库以逗号分隔 --&gt; &lt;schemas&gt;flyway&lt;/schemas&gt; &lt;!-- 设置存放flyway metadata数据的表名 --&gt; &lt;table&gt;schema_version&lt;/table&gt; &lt;!-- 设置flyway扫描sql升级脚本、java升级脚本的目录路径或包路径 --&gt; &lt;locations&gt; &lt;location&gt;db/migration&lt;/location&gt; &lt;/locations&gt; &lt;!-- 设置sql脚本文件的编码 --&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; .... Flyway常用的命令含义 命令含义migrate升级数据库clean删除所有配置的schemas的对象info打印所有升级的明细和状态信息validate验证指定路径下（ClassPath）升级配置的正确性baseline基于当前数据库版本，跳过所有早于当前版本的升级repair修复数据库中的所有表","categories":[{"name":"数据库","slug":"db","permalink":"http://wuzguo.com/blog/categories/db/"}],"tags":[{"name":"Flyway","slug":"Flyway","permalink":"http://wuzguo.com/blog/tags/Flyway/"}],"author":"Zak"},{"title":"JMS基本概念及ActiveMQ使用","slug":"server/jms","date":"2016-05-22T04:57:22.000Z","updated":"2022-08-01T06:35:23.627Z","comments":true,"path":"2016/05/22/server/jms.html","link":"","permalink":"http://wuzguo.com/blog/2016/05/22/server/jms.html","excerpt":"","text":"1.JMS介绍JMS（Java Messaging Service）即Java消息服务，是Java平台上有关面向消息中间件(MOM)的技术规范，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。它便于消息系统中的Java应用程序进行消息交换,并且通过提供标准的产生、发送、接收消息的接口简化企业应用的开发。JMS是一种与厂商无关的 API，用来访问消息收发系统消息，它类似于JDBC(Java Database Connectivity)。这里，JDBC 是可以用来访问许多不同关系数据库的 API，而 JMS 则提供同样与厂商无关的访问方法，以访问消息收发服务。许多厂商都支持 JMS，包括 IBM 的 MQSeries、BEA的 Weblogic JMS service和 Progress 的 SonicMQ。 JMS 使您能够通过消息收发服务（有时称为消息中介程序或路由器）从一个 JMS 客户机向另一个 JMS客户机发送消息。消息是 JMS 中的一种类型对象，由三部分组成： 报头（head）每条JMS 消息都必须具有消息头。头由路由信息以及有关该消息的元数据组成。可以通过多种方式来设置消息头的值： 由JMS 提供者在生成或传送消息的过程中自动设置 由生产者客户机通过在创建消息生产者时指定的设置进行设置 由生产者客户机逐一对各条消息进行设置 属性（property）消息可以包含称作属性的可选头字段。他们是以属性名和属性值对的形式制定的。可以将属性归类为消息头得扩展，其中可以包括如下信息： 创建数据的进程 数据的创建时间 每条数据的结构JMS提供者也可以添加影响消息处理的属性，如是否应压缩消息或如何在消息生命周期结束时废弃消息。 主体（body）包含要发送给接收应用程序的内容。每个消息接口特定于它所支持的内容类型。JMS为不同类型的内容提供了他们各自的消息类型，但是所有消息都派生自Message接口。 StreamMessage 一种主体中包含Java基元值流的消息。其填充和读取均按顺序进行 MapMessage 一种主体中包含一组键–值对的消息。没有定义条目顺序 TextMessage 一种主体中包含Java字符串的消息（例如，XML消息） ObjectMessage 一种主体中包含序列化Java对象的消息 BytesMessage 一种主体中包含连续字节流的消息 例如:MapMessage 消息格式 MapMessage=&#123; Header=&#123; ... standard headers ... CorrelationID=&#123;123-00001&#125; &#125; Properties=&#123; AccountID=&#123;Integer:1234&#125; &#125; Fields=&#123; Name=&#123;String:Mark&#125; Age=&#123;Integer:47&#125; &#125; &#125; 2.JMS传递模型JMS 的编程过程很简单，概括为：应用程序A 发送一条消息到消息服务器（也就是JMS Provider）的某个目得地(Destination)，然后消息服务器把消息转发给应用程序B。因为应用程序A 和应用程序B 没有直接的代码关连，所以两者实现了解偶。如下图： JMS支持两种消息传递模型：点对点（point-to-point，简称PTP）和发布&#x2F;订阅（publish&#x2F;subscribe,简称pub&#x2F;sub）。这两种消息传递模型非常相似，但有以下区别: PTP消息传递模型规定了一条消息之恩能够传递费一个接收方 Pub&#x2F;sub消息传递模型允许一条消息传递给多个接收方 每个模型都通过扩展公用基类来实现。例如：javax.jms.Queue和Javax.jms.Topic都扩展自javax.jms.Destination类。 点对点消息传递通过点对点的消息传递模型，一个应用程序可以向另外一个应用程序发送消息。在此传递模型中，目标类型时队列。消息首先被传送至队列目标，然后从改对垒将消息传送至对此队列进行监听的某个消费者，如下图: 一个队列可以关联多个队列发送方和接收方，但一条消息仅传递给一个接收方。如果多个接收方正在监听队列上的消息，JMS Provider将根据“先来者优先”的原则确定由哪个价售房接受下一条消息。如果没有接收方在监听队列，消息将保留在队列中，直至接收方连接到队列为止。这种消息传递模型是传统意义上的拉模型或轮询模型。在此列模型中，消息不时自动推动给客户端的，而是要由客户端从队列中请求获得。 发布&#x2F;订阅消息传递通过发布&#x2F;订阅消息传递模型，应用程序能够将一条消息发送到多个接收方。在此传送模型中，目标类型是主题。消息首先被传送至主题目标，然后传送至所有已订阅此主题的或送消费者。如下图： 主题目标也支持长期订阅。长期订阅表示消费者已注册了主题目标，但在消息到达目标时改消费者可以处于非活动状态。当消费者再次处于活动状态时，将会接收该消息。如果消费者均没有注册某个主题目标，该主题只保留注册了长期订阅的非活动消费者的消息。与PTP消息传递模型不同，pub&#x2F;sub消息传递模型允许多个主题订阅者接收同一条消息。JMS一直保留消息，直至所有主题订阅者都接收到消息为止。pub&#x2F;sub消息传递模型基本上时一个推模型。在该模型中，消息会自动广播，消费者无须通过主动请求或轮询主题的方法来获得新的消息。 上面两种消息传递模型里，我们都需要定义消息生产者和消费者，生产者吧消息发送到JMS Provider的某个目标地址（Destination），消息从该目标地址传送至消费者。消费者可以同步或异步接收消息，一般而言，异步消息消费者的执行和伸缩性都优于同步消息接收者，体现在： 异步消息接收者创建的网络流量比较小。单向对东消息，并使之通过管道进入消息监听器。管道操作支持将多条消息聚合为一个网络调用。 异步消息接收者使用线程比较少。异步消息接收者在不活动期间不使用线程。同步消息接收者在接收调用期间内使用线程，结果线程可能会长时间保持空闲，尤其是如果该调用中指定了阻塞超时。 对于服务器上运行的应用程序代码，使用异步消息接收者几乎总是最佳选择，尤其是通过消息驱动Bean。使用异步消息接收者可以防止应用程序代码在服务器上执行阻塞操作。而阻塞操作会是服务器端线程空闲，甚至会导致死锁。阻塞操作使用所有线程时则发生死锁。如果没有空余的线程可以处理阻塞操作自身解锁所需的操作，这该操作永远无法停止阻塞。 3.JMS Provider（ActiveMQ）上面介绍了JMS的基本概念，下面介绍一款开源的JMS具体实现ActiveMQ，ActiveMQ是由Apache出品的，一款最流行的，能力强劲的开源消息总线（消息中间件）。ActiveMQ是一个完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现，它非常快速，支持多种语言的客户端和协议，而且可以非常容易的嵌入到企业的应用环境中，并有许多高级功能。 MQ首先简单的介绍一下MQ，MQ英文名MessageQueue，中文名也就是大家用的消息队列，干嘛用的呢，说白了就是一个消息的接受和转发的容器，可用于消息推送。 消息中间件我们简单的介绍一下消息中间件，对它有一个基本认识就好，消息中间件（MOM：Message Orient middleware）。 消息中间件有很多的用途和优点： 将数据从一个应用程序传送到另一个应用程序，或者从软件的一个模块传送到另外一个模块 负责建立网络通信的通道，进行数据的可靠传送 保证数据不重发，不丢失 能够实现跨平台操作，能够为不同操作系统上的软件集成技工数据传送服务 4.ActiveMQ使用 下载ActiveMQ去官方网站下载：http://activemq.apache.org/ 运行ActiveMQ解压缩apache-activemq-5.5.1-bin.zip，然后双击apache-activemq-5.5.1\\bin\\activemq.bat运行ActiveMQ程序,启动ActiveMQ以后，登陆：http://localhost:8161/admin/,如下图所示： 创建 intellij IDEA 项目并运行 点对点模式 订阅发布模式 5.本文测试源码：https://github.com/wuzguo/jms.git 6.参考博文：1. 深入浅出JMS2.JMS","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"JMS","slug":"JMS","permalink":"http://wuzguo.com/blog/tags/JMS/"},{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"http://wuzguo.com/blog/tags/ActiveMQ/"},{"name":"MQ","slug":"MQ","permalink":"http://wuzguo.com/blog/tags/MQ/"}],"author":"Zak"},{"title":"CAS单点登录","slug":"server/sso1","date":"2016-05-13T03:11:14.000Z","updated":"2022-08-01T06:35:23.741Z","comments":true,"path":"2016/05/13/server/sso1.html","link":"","permalink":"http://wuzguo.com/blog/2016/05/13/server/sso1.html","excerpt":"","text":"1.创建证书需要安装JDK1.4以上版本并配置JAVA_HOME和PATH环境变量。 切换到命令窗口，并切换到某个目录下（比如C:\\） 生成密钥 keytool -genkey -alias tomcat -keyalg RSA -keypass changeit -storepass changeit -keystore server.keystore -validity 3600 在交互命令行中，第一项“您的名字与姓氏是什么？”需要填写服务器域名（本机用localhost） 导出证书 keytool -export -trustcacerts -alias tomcat -file server.cer -keystore server.keystore -storepass changeit 加入JDK受信任库 keytool -import -trustcacerts -alias tomcat -file server.cer -keystore %JAVA_HOME%&#x2F;jre&#x2F;lib&#x2F;security&#x2F;cacerts -storepass changeit 还有两个辅助命令，如果需要的时候再使用： 查看受信任库内容 keytool -list -keystore %JAVA_HOME%&#x2F;jre&#x2F;lib&#x2F;security&#x2F;cacerts 从受信任库中删除 keytool -delete -trustcacerts -alias tomcat -keystore %JAVA_HOME%&#x2F;jre&#x2F;lib&#x2F;security&#x2F;cacerts -storepass changeit 2.修改Tomcat配置在%TOMCAT_HOME%&#x2F;conf&#x2F;server.xml中找到Connector配置群，增加一项 &lt;Connector protocol=&quot;org.apache.coyote.http11.Http11NioProtocol&quot; port=&quot;8443&quot; minSpareThreads=&quot;5&quot; maxSpareThreads=&quot;75&quot; enableLookups=&quot;true&quot; disableUploadTimeout=&quot;true&quot; acceptCount=&quot;100&quot; maxThreads=&quot;200&quot; scheme=&quot;https&quot; secure=&quot;true&quot; SSLEnabled=&quot;true&quot; clientAuth=&quot;false&quot; sslProtocol=&quot;TLS&quot; keystoreFile=&quot;C:/server.keystore&quot; keystorePass=&quot;changeit&quot;/&gt; 其中keystoreFile配置为密钥库所在路径，keystorePass配置为密钥库文件保护密码。 3.验证访问重启Tomcat，在浏览器中访问配置好的Web应用，如果弹出“证书错误”页面，选“继续浏览”即可，不影响使用。 —后续将连载单点登录，敬请期待…","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"SSO","slug":"SSO","permalink":"http://wuzguo.com/blog/tags/SSO/"},{"name":"CAS","slug":"CAS","permalink":"http://wuzguo.com/blog/tags/CAS/"},{"name":"SSL协议","slug":"SSL协议","permalink":"http://wuzguo.com/blog/tags/SSL%E5%8D%8F%E8%AE%AE/"}],"author":"Zak"},{"title":"用AXIS2发布WebService的方法","slug":"server/axis2","date":"2016-04-30T02:04:10.000Z","updated":"2022-08-01T06:35:23.903Z","comments":true,"path":"2016/04/30/server/axis2.html","link":"","permalink":"http://wuzguo.com/blog/2016/04/30/server/axis2.html","excerpt":"","text":"##用AXIS2发布WebService的方法Axis2+tomcat7.0 实现webService 服务端发布与客户端的调用. 第一步：首先要下载开发所需要的jar包 下载：axis2-1.7.1-war.zip http://www.apache.org/dyn/closer.lua/axis/axis2/java/core/1.7.1/axis2-1.7.1-war.zip 下载完后解压至tomcat安装目录下的webapps文件夹下，启动tomcat后，在webapps目录下会生成axis2文件夹。 访问http://localhost:8080/axis2/能看到以下页面表示axis2运行成功。 1.在IntelliJ IDEA下新建Web Project，工程名：HelloWorld，如下： 2.新建类HelloService，然后创建写两个方法。 3.将axis2 WEB-INF目录下的conf、lib、modules、services的文件夹拷贝到HelloWorld的 WEB-INF目录下。 4.在HelloWorld中services的文件夹创建 HelloWorld&#x2F;META-INF目录,在该目录下创建services.xml文件，配置信息如下： 5.在web.xml中妹子一个axis的servlet，拦截客户端请求。 6.特别注意：要将WEB-INF目录下的lib包引入到工程中，不然的话将不能运行。 7.配置WEB容器，这里使用Tomcat 7.0.64版本。 8.配置工程访问路径。 9.启动tomcat，然后用上图配置的路径在浏览器中访问我们在services.xml中配置的Service。 10.如果返回上图浏览器所示的XML文件信息，说明webservice服务就发布成功了。 —到此Axis2的WebService服务已成功发布 11.客户端调用 12.参考文章 http://www.cnblogs.com/javawebsoa/archive/2013/05/19/3087234.html","categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"}],"tags":[{"name":"WebService","slug":"WebService","permalink":"http://wuzguo.com/blog/tags/WebService/"},{"name":"Axis2","slug":"Axis2","permalink":"http://wuzguo.com/blog/tags/Axis2/"}],"author":"Zak"},{"title":"JQuery笔记","slug":"front/jquery","date":"2016-04-24T21:58:33.000Z","updated":"2022-08-01T06:33:30.341Z","comments":true,"path":"2016/04/25/front/jquery.html","link":"","permalink":"http://wuzguo.com/blog/2016/04/25/front/jquery.html","excerpt":"","text":"##JQuery笔记 本文来自w3school –2016-04-25 05:58:33 如何在页面上同时使用jQuery 和其他框架？jQuery 使用 $ 符号作为 jQuery 的简写。如果其他 JavaScript 框架（如：MooTools、Backbone、Sammy、Cappuccino、Knockout、JavaScript MVC、Google Web Toolkit、Google Closure、Ember、Batman 以及 Ext JS）也使用 $ 符号作为简写怎么办？ 其中某些框架也使用 $ 符号作为简写（就像 jQuery），如果您在用的两种不同的框架正在使用相同的简写符号，有可能导致脚本停止运行。 jQuery 的团队考虑到了这个问题，并实现了**noConflict()**方法。 jQuery noConflict() 方法noConflict() 方法会释放会 $ 标识符的控制，这样其他脚本就可以使用它了。 当然，您仍然可以通过全名替代简写的方式来使用 jQuery： $.noConflict(); jQuery(document).ready(function()&#123; jQuery(\"button\").click(function()&#123; jQuery(\"p\").text(\"jQuery 仍在运行！\"); &#125;); &#125;); 您也可以创建自己的简写。noConflict() 可返回对 jQuery 的引用，您可以把它存入变量，以供稍后使用。请看这个例子： var jq = $.noConflict(); jq(document).ready(function()&#123; jq(\"button\").click(function()&#123; jq(\"p\").text(\"jQuery 仍在运行！\"); &#125;); &#125;); 如果你的 jQuery 代码块使用 $ 简写，并且您不愿意改变这个快捷方式，那么您可以把 $ 符号作为变量传递给 ready 方法。这样就可以在函数内使用 $ 符号了 - 而在函数外，依旧不得不使用 “jQuery”： $.noConflict(); jQuery(document).ready(function($)&#123; $(\"button\").click(function()&#123; $(\"p\").text(\"jQuery 仍在运行！\"); &#125;); &#125;);","categories":[{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"}],"tags":[{"name":"JQuery","slug":"JQuery","permalink":"http://wuzguo.com/blog/tags/JQuery/"}],"author":"Zak"},{"title":"Git常用命令","slug":"tools/git_command_cheat_sheet","date":"2016-04-24T14:43:42.000Z","updated":"2022-08-01T06:33:30.257Z","comments":true,"path":"2016/04/24/tools/git_command_cheat_sheet.html","link":"","permalink":"http://wuzguo.com/blog/2016/04/24/tools/git_command_cheat_sheet.html","excerpt":"","text":"##Git常用命令速查表 创建版本库git clone ＜url&gt; #克隆远程的版本库git init #初始化本地版本库git checkout -b 本地分支名 远程分支名 #获取远程分支的代码 修改和提交git status #查看状态git diff #查看变更内容git add #跟踪所有改动过的文件git add ＜file&gt; #跟踪指定的文件git mv ＜old&gt; ＜new&gt; #文件改名git rm ＜file&gt; #删除文件git rm –cached ＜file&gt; #停止跟踪文件但不删除git commit -m “commit message” #提交所有更新过的文件git commit –amend #修改最后一次提交 查看提交历史git log #查看提交的历史git log -p ＜file&gt; #查看指定文件的提交历史git blame ＜file&gt; #以列表方式查看指定文件的提交历史 撤消git reset –hard HEAD #撤消工作目录中所有未提交文件的修改内容git checkout HEAD ＜file&gt; #撤消指定的未提交文件的修改内容git revert ＜commit&gt; #撤消指定的提交git reset –hard HEAD #放弃暂存区B和工作目录C的所有修改，恢复最近一次提交状态 （A -&gt;C 所有文件)git checkout – ＜file_name&gt; #恢复某文件到最近一次提交状态，放弃checkout后的修改 (A -&gt; C 指定文件)git revert HEAD #撤消最近的一个提交 （针对已经的commit，跟B和C无关）git revert HEAD^ # 撤消上次”(next-to-last)的提交 （^ 个数可以递增） 分支与标签git branch #显示所有本地分支git branch ＜branchName&gt; #创建新分支git checkout ＜branchName&#x2F;tagName&gt; #切换到指定分支或标签git checkout -b ＜branchName&gt; #创建新分支并切换到该分支git branch -d ＜branchName&gt; #删除本地分支git branch -D ＜branchName&gt; #强制删除本地分支git tag #列出所有本地标签git tag ＜tagname&gt; #基于最新提交创建标签git tag -d ＜tagname&gt; #删除标签git checkout ［options] [＜branchName&gt;] – ＜file&gt;… #复制文件到指定分支git diff ＜branch&gt; –stat #比较branch分支与当前分支差异 合并与衍合git merge ＜branch&gt; #合并指定分支到当前分支git rebase ＜branch&gt; #衍合指定分支到当前分支 远程操作git remote -v #查看远程版本库信息git remote show ＜remote&gt; #查看指定远程版本库信息git remote add ＜remote&gt; ＜url&gt; #添加远程版本库git fetch ＜remote&gt; #从远程库获取代码git pull ＜remote&gt; ＜branchName&gt; #下载代码及快速合并git push ＜remote&gt; ＜branchName&gt; #上传代码及快速合并git push ＜remote&gt; : ＜branchName&#x2F;tag-name&gt; #删除远程分支或标签git push –tag #上传所有标签git branch -r #查看远程分支 删除操作git rm –cached readme.txt #文件放弃跟踪，但是还是保留在工作目录中。 没有–cached 选项，就完全删除文件 git rm filename #直接删除文件 通过“git rm –cached README.txt”命令，可以将文件状态还原为未暂存状态，即回到“Untracked files”文件状态。通过“git add README.txt”命令将已修改文件更新到暂存区域中，如果想撤销修改。 可以使用“git checkout – README.txt”命令。 通过git reset HEAD ＜file&gt;取消暂存 总结先git add你修改过的文件，再git diff并git status查看确认，然后git commit提交，然后输入你的开发日志，最后git log再次确认。","categories":[{"name":"工具","slug":"tools","permalink":"http://wuzguo.com/blog/categories/tools/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://wuzguo.com/blog/tags/Git/"}],"author":"Zak"},{"title":"Hello World","slug":"others/HelloWorld","date":"2016-04-24T04:07:47.000Z","updated":"2022-08-01T06:33:30.129Z","comments":true,"path":"2016/04/24/others/HelloWorld.html","link":"","permalink":"http://wuzguo.com/blog/2016/04/24/others/HelloWorld.html","excerpt":"","text":"Welcome to MarkdownPad 2MarkdownPad is a full-featured Markdown editor for Windows. Built exclusively for MarkdownEnjoy first-class Markdown support with easy access to Markdown syntax and convenient keyboard shortcuts. Give them a try: Bold (Ctrl+B) and Italic (Ctrl+I) Quotes (Ctrl+Q) Code blocks (Ctrl+K) Headings 1, 2, 3 (Ctrl+1, Ctrl+2, Ctrl+3) Lists (Ctrl+U and Ctrl+Shift+O) See your changes instantly with LivePreviewDon’t guess if your hyperlink syntax is correct; LivePreview will show you exactly what your document looks like every time you press a key. Make it your ownFonts, color schemes, layouts and stylesheets are all 100% customizable so you can turn MarkdownPad into your perfect editor. A robust editor for advanced Markdown usersMarkdownPad supports multiple Markdown processing engines, including standard Markdown, Markdown Extra (with Table support) and GitHub Flavored Markdown. With a tabbed document interface, PDF export, a built-in image uploader, session management, spell check, auto-save, syntax highlighting and a built-in CSS management interface, there’s no limit to what you can do with MarkdownPad.","categories":[{"name":"其他","slug":"others","permalink":"http://wuzguo.com/blog/categories/others/"}],"tags":[{"name":"Hello World","slug":"Hello-World","permalink":"http://wuzguo.com/blog/tags/Hello-World/"},{"name":"Markdown","slug":"Markdown","permalink":"http://wuzguo.com/blog/tags/Markdown/"}],"author":"Zak"}],"categories":[{"name":"后台","slug":"server","permalink":"http://wuzguo.com/blog/categories/server/"},{"name":"其他","slug":"others","permalink":"http://wuzguo.com/blog/categories/others/"},{"name":"操作系统","slug":"os","permalink":"http://wuzguo.com/blog/categories/os/"},{"name":"IoT","slug":"iot","permalink":"http://wuzguo.com/blog/categories/iot/"},{"name":"工具","slug":"tools","permalink":"http://wuzguo.com/blog/categories/tools/"},{"name":"前端","slug":"front","permalink":"http://wuzguo.com/blog/categories/front/"},{"name":"大数据","slug":"bigdata","permalink":"http://wuzguo.com/blog/categories/bigdata/"},{"name":"数据库","slug":"db","permalink":"http://wuzguo.com/blog/categories/db/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://wuzguo.com/blog/tags/Zookeeper/"},{"name":"引用","slug":"引用","permalink":"http://wuzguo.com/blog/tags/%E5%BC%95%E7%94%A8/"},{"name":"强引用","slug":"强引用","permalink":"http://wuzguo.com/blog/tags/%E5%BC%BA%E5%BC%95%E7%94%A8/"},{"name":"软引用","slug":"软引用","permalink":"http://wuzguo.com/blog/tags/%E8%BD%AF%E5%BC%95%E7%94%A8/"},{"name":"弱引用","slug":"弱引用","permalink":"http://wuzguo.com/blog/tags/%E5%BC%B1%E5%BC%95%E7%94%A8/"},{"name":"虚引用","slug":"虚引用","permalink":"http://wuzguo.com/blog/tags/%E8%99%9A%E5%BC%95%E7%94%A8/"},{"name":"Drools","slug":"Drools","permalink":"http://wuzguo.com/blog/tags/Drools/"},{"name":"规则引擎","slug":"规则引擎","permalink":"http://wuzguo.com/blog/tags/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E/"},{"name":"linux","slug":"linux","permalink":"http://wuzguo.com/blog/tags/linux/"},{"name":"vi","slug":"vi","permalink":"http://wuzguo.com/blog/tags/vi/"},{"name":"vim","slug":"vim","permalink":"http://wuzguo.com/blog/tags/vim/"},{"name":"shell","slug":"shell","permalink":"http://wuzguo.com/blog/tags/shell/"},{"name":"bash","slug":"bash","permalink":"http://wuzguo.com/blog/tags/bash/"},{"name":"centos","slug":"centos","permalink":"http://wuzguo.com/blog/tags/centos/"},{"name":"IoT","slug":"IoT","permalink":"http://wuzguo.com/blog/tags/IoT/"},{"name":"MQTT","slug":"MQTT","permalink":"http://wuzguo.com/blog/tags/MQTT/"},{"name":"QoS","slug":"QoS","permalink":"http://wuzguo.com/blog/tags/QoS/"},{"name":"Broker","slug":"Broker","permalink":"http://wuzguo.com/blog/tags/Broker/"},{"name":"发布订阅","slug":"发布订阅","permalink":"http://wuzguo.com/blog/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"name":"协议","slug":"协议","permalink":"http://wuzguo.com/blog/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"Git","slug":"Git","permalink":"http://wuzguo.com/blog/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"http://wuzguo.com/blog/tags/GitHub/"},{"name":"彩蛋","slug":"彩蛋","permalink":"http://wuzguo.com/blog/tags/%E5%BD%A9%E8%9B%8B/"},{"name":"Selenium","slug":"Selenium","permalink":"http://wuzguo.com/blog/tags/Selenium/"},{"name":"WebDriver","slug":"WebDriver","permalink":"http://wuzguo.com/blog/tags/WebDriver/"},{"name":"Python","slug":"Python","permalink":"http://wuzguo.com/blog/tags/Python/"},{"name":"Intellij IDEA","slug":"Intellij-IDEA","permalink":"http://wuzguo.com/blog/tags/Intellij-IDEA/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://wuzguo.com/blog/tags/SpringBoot/"},{"name":"Angular","slug":"Angular","permalink":"http://wuzguo.com/blog/tags/Angular/"},{"name":"Angular5","slug":"Angular5","permalink":"http://wuzguo.com/blog/tags/Angular5/"},{"name":"heap out of memory","slug":"heap-out-of-memory","permalink":"http://wuzguo.com/blog/tags/heap-out-of-memory/"},{"name":"undefined","slug":"undefined","permalink":"http://wuzguo.com/blog/tags/undefined/"},{"name":"AngularJS","slug":"AngularJS","permalink":"http://wuzguo.com/blog/tags/AngularJS/"},{"name":"$http","slug":"http","permalink":"http://wuzguo.com/blog/tags/http/"},{"name":"$resource","slug":"resource","permalink":"http://wuzguo.com/blog/tags/resource/"},{"name":"Restangular","slug":"Restangular","permalink":"http://wuzguo.com/blog/tags/Restangular/"},{"name":"控制器","slug":"控制器","permalink":"http://wuzguo.com/blog/tags/%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"name":"Controller","slug":"Controller","permalink":"http://wuzguo.com/blog/tags/Controller/"},{"name":"AngularJS - 服务","slug":"AngularJS-服务","permalink":"http://wuzguo.com/blog/tags/AngularJS-%E6%9C%8D%E5%8A%A1/"},{"name":"Service","slug":"Service","permalink":"http://wuzguo.com/blog/tags/Service/"},{"name":"AngularJS - 过滤器","slug":"AngularJS-过滤器","permalink":"http://wuzguo.com/blog/tags/AngularJS-%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"name":"Filter","slug":"Filter","permalink":"http://wuzguo.com/blog/tags/Filter/"},{"name":"Upsource","slug":"Upsource","permalink":"http://wuzguo.com/blog/tags/Upsource/"},{"name":"Code Review","slug":"Code-Review","permalink":"http://wuzguo.com/blog/tags/Code-Review/"},{"name":"代码审查","slug":"代码审查","permalink":"http://wuzguo.com/blog/tags/%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5/"},{"name":"指令","slug":"指令","permalink":"http://wuzguo.com/blog/tags/%E6%8C%87%E4%BB%A4/"},{"name":"directive","slug":"directive","permalink":"http://wuzguo.com/blog/tags/directive/"},{"name":"ECharts","slug":"ECharts","permalink":"http://wuzguo.com/blog/tags/ECharts/"},{"name":"ExtJs","slug":"ExtJs","permalink":"http://wuzguo.com/blog/tags/ExtJs/"},{"name":"Spark","slug":"Spark","permalink":"http://wuzguo.com/blog/tags/Spark/"},{"name":"日志分析","slug":"日志分析","permalink":"http://wuzguo.com/blog/tags/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"name":"Spark Streaming","slug":"Spark-Streaming","permalink":"http://wuzguo.com/blog/tags/Spark-Streaming/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"http://wuzguo.com/blog/tags/SparkSQL/"},{"name":"RDD","slug":"RDD","permalink":"http://wuzguo.com/blog/tags/RDD/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://wuzguo.com/blog/tags/Elasticsearch/"},{"name":"Elasticsearch-head","slug":"Elasticsearch-head","permalink":"http://wuzguo.com/blog/tags/Elasticsearch-head/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://wuzguo.com/blog/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"},{"name":"ELK","slug":"ELK","permalink":"http://wuzguo.com/blog/tags/ELK/"},{"name":"Thread","slug":"Thread","permalink":"http://wuzguo.com/blog/tags/Thread/"},{"name":"多线程","slug":"多线程","permalink":"http://wuzguo.com/blog/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"Runable","slug":"Runable","permalink":"http://wuzguo.com/blog/tags/Runable/"},{"name":"JVM","slug":"JVM","permalink":"http://wuzguo.com/blog/tags/JVM/"},{"name":"JMM","slug":"JMM","permalink":"http://wuzguo.com/blog/tags/JMM/"},{"name":"内存模型","slug":"内存模型","permalink":"http://wuzguo.com/blog/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://wuzguo.com/blog/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"执行引擎","slug":"执行引擎","permalink":"http://wuzguo.com/blog/tags/%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E/"},{"name":"MongoDB","slug":"MongoDB","permalink":"http://wuzguo.com/blog/tags/MongoDB/"},{"name":"shiro","slug":"shiro","permalink":"http://wuzguo.com/blog/tags/shiro/"},{"name":"Flyway","slug":"Flyway","permalink":"http://wuzguo.com/blog/tags/Flyway/"},{"name":"JMS","slug":"JMS","permalink":"http://wuzguo.com/blog/tags/JMS/"},{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"http://wuzguo.com/blog/tags/ActiveMQ/"},{"name":"MQ","slug":"MQ","permalink":"http://wuzguo.com/blog/tags/MQ/"},{"name":"SSO","slug":"SSO","permalink":"http://wuzguo.com/blog/tags/SSO/"},{"name":"CAS","slug":"CAS","permalink":"http://wuzguo.com/blog/tags/CAS/"},{"name":"SSL协议","slug":"SSL协议","permalink":"http://wuzguo.com/blog/tags/SSL%E5%8D%8F%E8%AE%AE/"},{"name":"WebService","slug":"WebService","permalink":"http://wuzguo.com/blog/tags/WebService/"},{"name":"Axis2","slug":"Axis2","permalink":"http://wuzguo.com/blog/tags/Axis2/"},{"name":"JQuery","slug":"JQuery","permalink":"http://wuzguo.com/blog/tags/JQuery/"},{"name":"Hello World","slug":"Hello-World","permalink":"http://wuzguo.com/blog/tags/Hello-World/"},{"name":"Markdown","slug":"Markdown","permalink":"http://wuzguo.com/blog/tags/Markdown/"}]}